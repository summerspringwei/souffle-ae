{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Artifact Evaluation\n",
    "\n",
    "> #### Optimizing Deep Learning Inference via Global Analysis and Tensor Expressions\n",
    "\n",
    "Optimizing deep neural network (DNN) execution is impor- tant but becomes increasingly difficult as DNN complexity grows. Existing DNN compilers cannot effectively exploit op- timization opportunities across operator boundaries, leaving room for improvement. To address this challenge, we present Souffle, an open-source compiler that optimizes DNN in- ference across operator boundaries. Souffle creates a global tensor dependency graph using tensor expressions, traces data flow and tensor information, and partitions the compu- tation graph into subprograms based on dataflow analysis and resource constraints. Within a subprogram, Souffle per- forms local optimization via semantic-preserving transfor- mations, finds an optimized program schedule, and improves instruction-level parallelism and data reuse. We evaluated Souffle using six representative DNN models on an NVIDIA A100 GPU. Experimental results show that Souffle consis- tently outperforms six state-of-the-art DNN optimizers by \n",
    "delivering a geometric mean speedup of up to $3.7\\times$ over TensorRT and $7.8\\times$ over Tensorflow XLA.\n",
    "\n",
    "## Preliminaries\n",
    "This repository showcases the performance evaluation and comparison between `Souffle`(Our work) and the existing state-of-the-art compilers/frameworks(including `XLA`, `TensorRT`, `Rammer`, `Apollo` and `IREE`).\n",
    "\n",
    "- **XLA(Tensorflow v2.10)**: The TensorFlow XLA compiler\n",
    "can fuse DNN operators like point-wise and reduction op-\n",
    "erators and performs optimizations on the fused operator.\n",
    "Unlike Souffle that performs analysis and optimizations on\n",
    "TEs, XLA performs analysis on its high-level operators(HLO)\n",
    "\n",
    "- **TensorRT(v8.2)**:This GPU-vendor-specific framework optimizes the inference of DNNs on NVIDIA GPUs\n",
    "\n",
    "- **Rammer(v0.4)**:This DNN compiler is also known as NNFu-\n",
    "sion. It generates a spatial-temporal schedule at compile\n",
    "time to minimize scheduling overhead and exploit hardware\n",
    "parallelism through inter- and intra-operator co-scheduling\n",
    "\n",
    "- **Apollo**:This represents the state-of-the-art fusion framework for inference optimization. Apollo considers both\n",
    "memory- and compute-bound tensor operators for kernel\n",
    "fusions and uses hand-crafted rules to exploit parallelism\n",
    "between independent tensor operators\n",
    "\n",
    "- **IREE**:The intermediate resentation execution environment (IREE) builds upon the\n",
    "LLVM MLIR project. IREE is designed to lower DNN\n",
    "models to MLIR dialects to optimize model inference. IREE\n",
    "utilizes the linalg dialect to perform the operator fusion,\n",
    "which supports loop affine fusion optimization and global\n",
    "analysis.\n",
    "\n",
    "The metric assesed in this notebook mainly include end-to-end latency, global memory access and number of kernels.\n",
    "\n",
    "## Important Notes\n",
    "**A few bash scripts take more than half an hour to complete; Please wait for the results before executing the next one.**\n",
    "**I will also provide a online jupyter notebook if you are inconvenient or having troubles  to build the environment. The jupyter notebook is run on our server and you can directly access the notebook and run the experiments from the internet**\n",
    "### Links to The Paper\n",
    "\n",
    "**For each step, we highlight that the current evaluation is corresponding to which Section or Figure in the submitted paper.**\n",
    "The main restuls presented in this repository correspond to the submitted paper's Table 3, 4, 5 and Figure 6.\n",
    "\n",
    "\n",
    "## 1. Experimental Environments Setup\n",
    "\n",
    "We have setup the experiment environments on our machine.\n",
    "For more info about how to setup on you local machine, please refer to `README.md`\n",
    "\n",
    "### Check the status of docker containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see containers with the following image names:\n",
    "* souffle-tvm-0.8:latest\n",
    "* souffle-iree:latest\n",
    "* souffle-tensorrt8.4.1-ubuntu18.04:latest\n",
    "\n",
    "### Check the hardware of the server (nvidia A100 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation\n",
    "Next, we use four cases to test the end-to-end performance for the baseline compilers and our work.\n",
    "Each case matches a table or figure in the submitted paper.\n",
    "\n",
    "we recommend you to run these cases one by one, which reduces the total execution time at most **two hours**.\n",
    "We also provided a fast mode which re-use the existing profiling data \n",
    "to directly print the outputs.\n",
    "\n",
    "- 2.1 CASE - End-to-end model runtime (Table 3 in Section 8) - around 30 minutes.\n",
    "- 2.2 CASE - Execution time with Souffle individual optimization (Table 4 in Section 8) - around 30 minutes.\n",
    "- 2.3 CASE - The number of GPU kernel calls and global memory\n",
    "data transfer size of the resulting code (Table 5 in Section 8) - around 50 minutes.\n",
    "- 2.4 CASE - EfficientNet sub-module latency breakdown (Figure 6 in Section 8) - around 10 minutes.\n",
    "\n",
    "**Log files**\n",
    "\n",
    "PS: some cases would consume over a half-hour because we have to execute all baselines. Please have a coffee and wait for the output before the subsequent execution.\n",
    "\n",
    "### 2.1 CASE - End-to-end model runtime (Table 3)\n",
    "In this case, we compare souffle with five representative state-of-the-art baselines to exploit the end-to-end latency.\n",
    "We omit Ansor in this case.\n",
    "\n",
    "| Model       | XLA  | TRT   | Rammer | Apollo | IREE  | Ours |\n",
    "| ----        | ---- | ----  | ----   | ----   | ----  | ---- |\n",
    "| BERT        | 2.55 | 1.30  | 2.19   | 3.29   | 2.22  | 1.22 |\n",
    "| ResNeXt     | 8.91 | 24.82 | 11.69  | 22.80  | 314.8 | 4.43 |\n",
    "| LSTM        | 10.57| 6.30  | 1.72   | Failed | 16.0  | 0.80 |\n",
    "| EfficientNet| 2.96 | 1.21  | Falied | 2.3    | 12.33 | 0.66 |\n",
    "| SwinTrans.  | 6.43 | 1.74  | Falied | 10.78  | 18.1  | 1.55 |\n",
    "| MMoE        | 0.29 | 0.07  | Falied | 0.049  | 0.088 | 0.014|\n",
    "\n",
    "**The following commonds reproduce the results of Table 3 in the submiited paper. Please refer to Section8(Page 10) for more details**\n",
    "Run the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!bash scripts/run_table3.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!cat results/table3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results in `table3.csv` is a transposed matrix of table 3 in the submmited paper.\n",
    "### 2.2 CASE - Execution time with Souffle individual optimization (Table 4)\n",
    "In this case, we present an ablation study by enable the individual optimization one-by-one\n",
    "to demonstrate the effectiveness of each optmization techniques.\n",
    "Run the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!bash scripts/run_table4.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cat results/table4.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 CASE - The number of GPU kernel calls and global memory data transfer size of the resulting code (Table 5)\n",
    "In this case, We compare Souffle with STOA works about the number of GPU kernel calls and global memory data transfer size (M bytes).\n",
    "Less number of GPU kernel calls and global memory data transfer size leads to better\n",
    "fusion results.\n",
    "\n",
    "Run the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!bash scripts/run_table5.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!cat results/table5.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 CASE - EfficientNet sub-module latency breakdown (Figure 6)\n",
    "Run the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!bash scripts/run_figure6.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the results:\n",
    "The image will be saved in `results/efficientnet-se-module-latency-ours.pdf`.\n",
    "Please compare it with `Figure6`(on page 11) in the submitted paper.\n",
    "\n",
    "## Contact\n",
    "If there are any questions or suggestion, please feel free to drop me an email (scscx@leeds.ac.uk). Many thanks for your feedback!\n",
    "\n",
    "\n",
    "-----\n",
    "# The end of this Artifact Evaluation\n",
    "-----\n",
    "\n",
    "#### Many thanks for your review, time and efforts on this artifact evaluation.  <br> Many thanks for your understanding and bearing with some inconveniences on this notebook. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
