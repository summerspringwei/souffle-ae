{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifact Evaluation \n",
    "> #### STRONGHOLD: Fast and Affordable Billion-scale Deep Learning Model Training (pap381s2)\n",
    "\n",
    "Deep neural networks (DNNs) with billion-scale parameters have demonstrated impressive performance in solving many tasks. Unfortunately, training a billion-scale DNN requires high-performance GPU servers that are too expensive to purchase and maintain. Existing solutions for enabling larger DNN training with limited resources are inadequate because they suffer from high training time overhead. \n",
    "\n",
    "We present STRONGHOLD, a better approach for enabling large DNN model training by dynamically offloading data to the CPU RAM and using the secondary storage (e.g., an SSD drive). It maintains a working window to overlap the GPU computation with CPU-GPU data movement carefully and exploits the multi-core CPU for optimizer update. Compared to the state-of-the-art offloading-based solutions, STRONGHOLD improves the trainable model size by 1.9x∼6.5x on a 32GB V100 GPU, with 1.2x∼3.7x improvement on the training throughput.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This interactive Jupyter notebook showcases the performance evaluation and comparison between STRONGHOLD and the existing state-of-the-art methods (e.g., Megatron-LM, L2L, ZeRO-Offload and ZeRO-Infinity). \n",
    "\n",
    "\n",
    "<!-- The main results of our works are to compare the performance of our approach (i.e. STRONGHOLD) with the existing state-of-the-art methods. The evaluation criteria include throughput, model sizes, scalability and inference time.\n",
    "\n",
    "STRONGHOLD we proposed aims to increase the trainable model size through dynamic offloading. In this artifact, we show the performance of our apporach (i.e. STRONGHOLD) against the existing state-of-the-art methods:  Megatron-LM, L2L, Zero-offload and Zero-Infinity. The evaluation criteria includes throughput, model sizes, scalability and inference time. We will go through all the evalutions on the largest trainable deep neural network (DNN) size on a single GPU (NVIDIA 32GB V100). -->\n",
    "\n",
    "- **Megatron-LM [1]**: the library supporting Transformer-based models, released by NVIDIA, optimized for tensor parallelism. We choose the `tags/v2.6` version as a reference for the testing throughput and trainable model size.\n",
    "\n",
    "- **L2L [2]**: an offloading strategy, keeping only one Transformer layer in the GPU memory at a time and offloading model parameters between the GPU memory and CPU RAM sequentially. Since L2L stores the optimizer states on the GPU memory, it is limited mainly by the capacity of GPU memory.\n",
    "\n",
    "- **ZeRO-Offload [3]**: a training method, statically storing the model states in the GPU memory and optimizer states in the CPU RAM. It utilizes the CPU computation cycle to update the model parameters through a CPU-version Adam optimizer.\n",
    "\n",
    "- **ZeRO-Infinity [4]**: a training method based on ZeRO-3 [5], utilizing GPU, CPU RAM and/or NVMe secondary storage. In this notebook, we test STRONGHOLD on CPU and GPU against ZeRO-Infinity, since keeping high NVMe I/O for a long time causes a mistake issue of overload disk in the cloud platform, which triggers the process to be killed by the underlying hypervisor.\n",
    "\n",
    "\n",
    "The metric assessed in this notebook includes throughput, model size, scalability, etc., shown as:\n",
    "\n",
    "- The **largest trainable model size**, corresponding to **Fig.6a** in paper.\n",
    "- The **throughput comparison on respective maximum trainable model size** of each baseline, corresponding to **Fig.7a** in the paper.\n",
    "- The **throughput comparison on the maximum trainable model size** of Megatron-LM, corresponding to **Fig.8a** in the paper.\n",
    "- The **nearly linear scaling** on iteration time of STRONGHOLD, corresponding to **Fig.8b** in the paper.\n",
    "- The **impact of GPU working window size** on the performance of STRONGHOLD, corresponding to **Fig.9** in the paper.\n",
    "\n",
    "<!-- - The inference time of running the different model sizes of STRONGHOLD.\n",
    "- Changing two key settings of STONGHOLD (i.e **window size** and **multi-stream**) and evaluate the changes of the throughput. -->\n",
    "\n",
    "PS: The present notebook runs on a rented ECS (virtual machine) with one 32GB-V100 GPU, 90GB CPU RAM and 12 CPU Cores. The hardware configuration differs from that in our paper. Thus, we reproduce the equivalent cases, and results reported in the paper but keep the relevant ratio value the same.\n",
    "\n",
    "<!-- The absolute values might be distinct, but the relevant ratio keeps the same. -->\n",
    "\n",
    "\n",
    "### Reference\n",
    "> [1] Megatron-LM. https://github.com/NVIDIA/Megatron-LM. <br>\n",
    "> [2] B. Pudipeddi et al., “Training large neural networks with constant memory using a new execution algorithm,” arXiv, 2020. <br>\n",
    "> [3] J. Ren et al., “Zero-offload: Democratizing billion-scale model training,” in OSDI, 2021. <br>\n",
    "> [4] S. Rajbhandari, O. Ruwase et al., “Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning,” in SC, 2021, pp. 1–14. <br>\n",
    "> [5] S. Rajbhandari, J. Rasley, O. Ruwase et al., “Zero: Memory optimiza- tions toward training trillion parameter models,” in SC, 2020, pp. 1–16.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Important Notes\n",
    "\n",
    "**A few bash scripts take more than half an hour to complete; Please wait for the results before executing the next one.**\n",
    "\n",
    "Overload might lead to a longer wait for results. This issue may occur if multiple reviewers simultaneously run the scripts to generate results. One possible way is to check the running process by `ps aux` before executing any other script.\n",
    "\n",
    "The experiments are customisable as reviewers can edit the Jupyter notebook on the spot. Type your changes with different docker scripts we provided and re-run using **Cell > Run Cells** from the menu.\n",
    "\n",
    "<!-- All experiments run on a single 32GB V100 GPU in order to briefly show that all of the evaultion results in our paper can be reproduced. Since multi-GPU running is not used, some of the results are not included in this artifact, but researchers can surely follow the similar ways to reproduce those results. -->\n",
    "\n",
    "\n",
    "### Links to The Paper\n",
    "\n",
    "**For each step, we highlight that the current evaluation is corresponding to which Section or Figure in the submitted paper.**\n",
    "\n",
    "The main results presented in this notebook correspond to the submitted paper's Figures 6, 7, 8, and 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Setup\n",
    "\n",
    "### 1.1 Let's ensure the docker container (NAME: aetesting) is launched. \n",
    "\n",
    "`docker ps` command shows the current running docker containers. The next cell in this notebook should output the following information. If not produce similar output, please run `!docker stop aetesting` and `!docker start aetesting` commands in a cell to restart it.\n",
    "\n",
    "```\n",
    "CONTAINER ID   IMAGE                    COMMAND       CREATED         STATUS         PORTS     NAMES\n",
    "d67abb7f151c   strongh/sc22-ae:latest   \"/bin/bash\"   3 minutes ago   Up 3 minutes             aetesting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                    COMMAND       CREATED         STATUS         PORTS     NAMES\r\n",
      "d67abb7f151c   strongh/sc22-ae:latest   \"/bin/bash\"   3 minutes ago   Up 3 minutes             aetesting\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Let's check the runtime environment in the docker container works well. \n",
    "\n",
    "`docker exec` supports executing a bash script in one running container. The following cell executes a command that should output the information, shown as the following:\n",
    "\n",
    "```\n",
    "root\n",
    "/home/sys/STRONGHOLD\n",
    "torch                         1.10.0a0+git71f889c /root/.pyenv/......3.9.10/lib/python3.9/site-packages\n",
    "torchvision                   0.11.0a0+05eae32\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "/home/sys/STRONGHOLD\n",
      "torch                         1.10.0a0+git71f889c /root/.pyenv/versions/3.9.10/envs/py3.9.10/lib/python3.9/site-packages\n",
      "torchvision                   0.11.0a0+05eae32\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "whoami && pwd && pip list | grep torch && \\\n",
    "\\\n",
    "pyenv deactivate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 How to run customized commands in the docker container?\n",
    "\n",
    "If executing the customized commands in the docker container, please `copy` the whole of the above commands and `change` the second to last line - `whoami && pwd && pip list | grep torch && \\` as yours. \n",
    "\n",
    "PS: Please do not remove the other commands that initialize the environment variables of the current interactive shell session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation\n",
    "\n",
    "Next, we use five cases to test performance separately on the largest trainable model size, throughput and scalability. Each case matches a figure in the submitted paper. \n",
    "\n",
    "To reuse the existing log files produced in the previous cases, we recommend you to run these cases one by one, which reduces the total execution time to **about 5 hours**. \n",
    "\n",
    "- 2.1 CASE - The largest trainable model size (Figure 6a in Section VI.A) - around 130 mins\n",
    "- 2.2 CASE - Throughput  on the largest trainable model size supported by each baseline (Figure 7a in Section VI.B) - around 40 mins\n",
    "- 2.3 CASE - Throughput on the largest trainable model size of Megatron-LM (Figure 8a in Section VI.B) - around 45 mins\n",
    "- 2.4 CASE - Nearly linear scaling as model size increases (Figure 8b in Section VI.B) - around 60 mins\n",
    "- 2.5 CASE - Impact of working window size (Figure 9 in Section VI.C) - around 50 mins\n",
    "\n",
    "**All log files will be stored in `/home/sys/STRONGHOLD/results`** as a format of `log_[method]_l-[layers]_h-[hidden size]_bs-[BATCH_SIZE]_ws-[WINDOW_SIZE]_[date].txt`. We print the core content in the log files via `grep` and `awk` for you at the end of each execution.\n",
    "\n",
    "**Launch script** `./examples/run.sh -m [method] -l [layers] -h [hidden size] -b [batch size] -w [window size]` accepts five arguements, where `[method]` takes the values of `megatron-lm`, `l2l`, `zero-offload`, `zero-infinity`, `stronghold` and `all`. Using all to automatically evaluate all approaches. Default values for `[layers]`, `[hidden size]`, `[batch size]`, `[window size]` are 16, 2048, 4 and 4, respectively.\n",
    "\n",
    "PS: some cases would consume over a half-hour because we have to execute all baselines. Please have a coffee and wait for the output before the subsequent execution.\n",
    "\n",
    "<!-- > #### Description of the output log\n",
    ">\n",
    "> `------------------------ arguments ------------------------` and the below information shows the running parameters in details.\n",
    "> \n",
    "> `>>> done with compiling and loading fused kernels. Compilation time: X seconds` means that all the compiling settings including CUDA linking and loading CUDA module have been successfully done. And it also gives the compilation time..\n",
    "> \n",
    "> `>>> done with compiling and loading strongh utils. Compilation time: X seconds` means all the optimizer modules have been successfully linked and compiled.\n",
    "> \n",
    "> `> building train, validation, and test datasets ...` shows that we are now building the training, validation and test dataloaders. And the following information will show the dataset in details.\n",
    "> \n",
    "> `[before the start of training step] datetime: 2022-06-22 22:22:59` <br>\n",
    "> `done with setup ...` shows everything works well before starting the actual training. \n",
    "> \n",
    "> `time (ms) | model-and-optimizer-setup: X | train/valid/test-data-iterators-setup: X` shows each of the setup cost (ms).\n",
    "> \n",
    "> `training ...` and the below log information shows the traing breakdowns. \n",
    "> \n",
    "> **The below log shows the actual model running results, thus should be the one we pay attention for. And those results correspond to Figure 7(a) and Figure 8(a) in our paper. Please refer to our paper for more details.**\n",
    "> \n",
    "> `iteration       X/      50` shows the iteration level, where `X` is the number represents `X` out of the total iteration (50 in this artifact) .\n",
    "> \n",
    "> `[Rank X] (after 10 iterations) memory (MB) | ...` shows the memory footprint.\n",
    ">\n",
    "> `time (ms) | ...` shows the training time breakdowns.\n",
    ">\n",
    "> **All the logs will be stored in `/home/sys/STRONGHOLD/results`** -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty the previous log files in `results` folder\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "rm -rf /home/sys/STRONGHOLD/results/*.txt && \\\n",
    "\\\n",
    "pyenv deactivate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 CASE - The largest trainable model size (Figure 6a in Section VI.A)\n",
    "\n",
    "In this case, we use GPT-like models to exploit each method's largest trainable model size. Model size changes via increasing/decreasing the number of transformer layers.\n",
    "\n",
    "Here, we evaluate Megatron-LM, L2L, ZeRO-Offload, ZeRO-Infinity and STRONGHOLD on a virtual machine with one 32GB V100, 90GB CPU RAM and 12 CPU Cores to exploit their largest trainable model size and bottleneck. During this process, we configure the `Heads=16, Sequence Length=1024, Batch Size=4` in all GPT-like models and training setups.\n",
    "\n",
    "The largest model sizes have been tested in this notebook, shown in the following table. Please run the following cells to reproduce it. Thanks.\n",
    "\n",
    "| Methods | Largest Trainable Size | Layers | Hidden Size | Heads | Sequence Length | Batch Size |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| Megatron-LM | **1.717 B**| **32** | 2048 | 16 | 1024 | 4 |\n",
    "| L2L | **4.033 B**| **78** | 2048 | 16 | 1024 | 4 |\n",
    "| ZeRO-Offload | **2.522 B**| **48** | 2048 | 16 | 1024 | 4 |\n",
    "| ZeRO-Infinity | **2.522 B**| **48** | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **5.141 B**| **100** | 2048 | 16 | 1024 | 4 |\n",
    "\n",
    "PS: `Errors about GPU/CPU OOM` might be represented as other information, such as 'can not create XXX'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.0  0.0   2520    72 pts/0    Ss+  Jun28   0:00 sleep infinit\r\n",
      "root           7  0.0  0.0   4348  2760 pts/1    Ss+  Jun28   0:00 /bin/bash\r\n",
      "root         159  0.0  0.0   4348  2900 pts/2    Ss+  Jun28   0:00 /bin/bash\r\n",
      "root       34863  0.0  0.0   3976  3144 pts/3    Ss+  00:55   0:00 /bin/bash -c \r\n",
      "root       35021  0.0  0.0   5892  2844 pts/3    R+   00:55   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "# To check if there exists other running processes launched by other reviwers in case of GPU overlead.\n",
    "# Just run it and no need to change anything in this cell.\n",
    "#\n",
    "# `ps aux` in docker container. \n",
    "######\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c 'export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && export PYENV_ROOT=\"/root/.pyenv\" && export PATH=\"$PYENV_ROOT/bin:$PATH\" && eval \"$(pyenv init -)\" && eval \"$(pyenv virtualenv-init -)\" && pyenv activate py3.9.10 && ps aux && pyenv deactivate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following results correspond to Figure 6a in the submitted paper. Please refers to Section VI.A on page 8 for more details. Run around 130 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " !!! The training model size in megatron-lm might be much smaller than others, such as zero-offload, stronghold, etc. !!! \n",
      "\n",
      " \n",
      "cd /home/sys/STRONGHOLD/examples/../Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../Megatron-LM/examples/sc22-gpt-megatron.sh 32 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_megatron-lm_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656464109.txt && cd -\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ checkpoints/gpt2\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.153 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.588 seconds\n",
      "time to initialize megatron (seconds): 2.914\n",
      "[after megatron is initialized] datetime: 2022-06-29 00:55:15 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.20\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 00:55:15 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.001011 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 00:55:16 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 135.68 | train/valid/test-data-iterators-setup: 421.91\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 00:55:16 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 5377.6 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.090563E+01 | loss scale: 1.0 | grad norm: 36.791 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.46 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.377599501609803;  SamplesPerSecond: 0.7438263111268488\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 26195.06494140625 | max allocated: 29775.16845703125 | reserved: 29928.0 | max reserved: 29928.0\n",
      "time (ms) | forward-compute: 1370.47 | backward-compute: 3876.85 | backward-params-all-reduce: 16.84 | backward-embedding-all-reduce: 0.04 | optimizer: 102.60 | batch-generator: 1.96\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 5332.0 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 9.688580E+00 | loss scale: 1.0 | grad norm: 17.054 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.55 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.33199405670166;  SamplesPerSecond: 0.7501883830820277\n",
      "time (ms) | forward-compute: 1311.28 | backward-compute: 3896.57 | backward-params-all-reduce: 16.83 | backward-embedding-all-reduce: 0.04 | optimizer: 96.51 | batch-generator: 1.22\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 5335.2 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.169696E+00 | loss scale: 1.0 | grad norm: 11.516 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.54 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.335219955444336;  SamplesPerSecond: 0.7497347875823174\n",
      "time (ms) | forward-compute: 1310.82 | backward-compute: 3900.24 | backward-params-all-reduce: 16.83 | backward-embedding-all-reduce: 0.04 | optimizer: 96.60 | batch-generator: 1.20\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 5336.1 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 8.941277E+00 | loss scale: 1.0 | grad norm: 15.119 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.54 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.336108684539795;  SamplesPerSecond: 0.7496099192261813\n",
      "time (ms) | forward-compute: 1310.50 | backward-compute: 3901.42 | backward-params-all-reduce: 16.84 | backward-embedding-all-reduce: 0.04 | optimizer: 96.60 | batch-generator: 1.22\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 5334.5 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 8.780692E+00 | loss scale: 1.0 | grad norm: 7.671 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.54 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.334467363357544;  SamplesPerSecond: 0.7498405609294753\n",
      "time (ms) | forward-compute: 1310.38 | backward-compute: 3899.86 | backward-params-all-reduce: 16.84 | backward-embedding-all-reduce: 0.04 | optimizer: 96.59 | batch-generator: 1.22\n",
      "saving checkpoint at iteration      50 to checkpoints/gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  successfully saved checkpoint at iteration      50 to checkpoints/gpt2\n",
      "time (ms) | save-checkpoint: 66891.40\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 01:00:50 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../L2L-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../L2L-Megatron-LM/examples/sc22-gpt-l2l.sh 78 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_l2l_l-78_hs-2048_bs-4_ws-4_2022-06-29.1656464452.txt && cd -\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_l2l ...................................... True\n",
      "  enbale_strongh .................................. None\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ checkpoints/gpt2_345m_ds\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 78\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ checkpoints/gpt2_345m_ds\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.153 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.598 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module ds_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module ds_cpu_adam...\n",
      ">>> done with compiling and loading strongh utils. Compilation time: 0.751 seconds\n",
      "time to initialize megatron (seconds): 3.744\n",
      "[after megatron is initialized] datetime: 2022-06-29 01:01:01 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 4033069056\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_345m_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.23\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 01:01:18 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000628 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 01:01:19 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 16258.79 | train/valid/test-data-iterators-setup: 1049.52\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 01:01:19 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 68261.8 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.126409E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 1.94 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 68.26175181865692;  SamplesPerSecond: 0.05859796875161271\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 15979.15771484375 | max allocated: 21094.26123046875 | reserved: 21240.0 | max reserved: 31392.0\n",
      "time (ms) | forward-compute: 16371.91 | backward-compute: 40024.83 | backward-params-all-reduce: 39.45 | backward-embedding-all-reduce: 0.04 | optimizer: 11571.00 | batch-generator: 2.01\n",
      "/home/sys/STRONGHOLD/examples/../L2L-Megatron-LM/examples/sc22-gpt-l2l.sh: line 56: 35558 Killed                  PYTHONGIL=1 python pretrain_gpt.py --num-layers ${NLAYERS} --hidden-size ${NHIDDEN} --num-attention-heads ${HEADS} --micro-batch-size ${BATCHSIZE} --global-batch-size ${BATCHSIZE} --seq-length ${SEQ} --max-position-embeddings ${SEQ} --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save $CHECKPOINT_PATH --load $CHECKPOINT_PATH --data-path $DATA_PATH --vocab-file ${VOCAB_PATH} --merge-file ${MERGE_PATH} --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --lr-warmup-fraction .01 --activations-checkpoint-method uniform --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --enable-l2l\n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/sc22-gpt-zero-offloading.sh 48 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_zero-offload_l-48_hs-2048_bs-4_ws-4_2022-06-29.1656465726.txt && cd -\n",
      "deepspeed --num_nodes 1 --num_gpus 1 pretrain_gpt2.py --model-parallel-size 1 --num-layers 48 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json --zero-stage 2 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-06-29 01:22:08,747] [WARNING] [runner.py:132:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2022-06-29 01:22:11,147] [INFO] [runner.py:398:main] cmd = /root/.pyenv/versions/3.9.10/envs/py3.9.10/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 pretrain_gpt2.py --model-parallel-size 1 --num-layers 48 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json --zero-stage 2 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda11.4\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda11.4\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:100:main] dist_world_size=1\n",
      "[2022-06-29 01:22:12,002] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... 4\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... True\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ True\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... mmap\n",
      "  data_path ....................... /home/sys/STRONGHOLD/data/my-gpt2-en_text_document\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... True\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 1000\n",
      "  exit_interval ................... 50\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 2048\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ checkpoints/gpt2_ds\n",
      "  local_rank ...................... 0\n",
      "  log_interval .................... 10\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. 0.00015\n",
      "  lr_decay_iters .................. 320000\n",
      "  lr_decay_style .................. cosine\n",
      "  lr_decay_tokens ................. None\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 1024\n",
      "  memory_centric_tiled_linear ..... False\n",
      "  merge_file ...................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  min_lr .......................... 1e-05\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 16\n",
      "  num_layers ...................... 48\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... True\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  remote_device ................... none\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ checkpoints/gpt2_ds\n",
      "  save_interval ................... 10000\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  scattered_embeddings ............ False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... 1024\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 949,50,1\n",
      "  split_transformers .............. False\n",
      "  synchronize_each_layer .......... True\n",
      "  tensorboard_dir ................. None\n",
      "  tile_factor ..................... 1\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  tokens .......................... 0\n",
      "  train_iters ..................... 50\n",
      "  train_tokens .................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  use_pin_memory .................. False\n",
      "  vocab_file ...................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  warmup .......................... 0.01\n",
      "  warmup_iters .................... None\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 5000000000\n",
      "  zero_contigious_gradients ....... True\n",
      "  zero_reduce_bucket_size ......... 50000000\n",
      "  zero_reduce_scatter ............. True\n",
      "  zero_stage ...................... 2\n",
      "---------------- end of arguments ----------------\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "building GPT2 model ...\n",
      "[2022-06-29 01:22:16,070] [INFO] [utils.py:822:see_memory_usage] Before Building Model\n",
      "[2022-06-29 01:22:16,071] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2022-06-29 01:22:16,071] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.08 GB, percent = 3.4%\n",
      "[2022-06-29 01:22:16,071] [WARNING] [partition_parameters.py:457:__init__] zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f4dcfda71f0>\n",
      "train_batch = 4, micro_batch=None\n",
      "[2022-06-29 01:22:16,072] [WARNING] [config.py:1100:_do_warning_check] DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM (1.0) > 0, setting to zero\n",
      "[2022-06-29 01:22:16,254] [INFO] [utils.py:822:see_memory_usage] After Building Model\n",
      "[2022-06-29 01:22:16,255] [INFO] [utils.py:823:see_memory_usage] MA 9.39 GB         Max_MA 9.39 GB         CA 9.39 GB         Max_CA 9 GB \n",
      "[2022-06-29 01:22:16,255] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.09 GB, percent = 3.4%\n",
      " > number of parameters on model parallel rank 0            2.522 Billion\n",
      "> learning rate decay style: cosine\n",
      "DeepSpeed is enabled.\n",
      "[2022-06-29 01:22:16,261] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.7, git-hash=unknown, git-branch=unknown\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f4dcfda71f0>\n",
      "train_batch = 4, micro_batch=None\n",
      "[2022-06-29 01:22:16,266] [WARNING] [config.py:1100:_do_warning_check] DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM (1.0) > 0, setting to zero\n",
      "[2022-06-29 01:22:16,291] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups using mpu\n",
      "[2022-06-29 01:22:16,291] [INFO] [logging.py:69:log_dist] [Rank 0] Initializing deepspeed groups with model parallel size 1, expert parallel size 1, and data parallel size 1\n",
      "[2022-06-29 01:22:16,291] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
      "[2022-06-29 01:22:16,291] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
      "[2022-06-29 01:22:16,367] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "[2022-06-29 01:22:16,367] [INFO] [engine.py:1084:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2022-06-29 01:22:16,368] [INFO] [engine.py:1090:_configure_optimizer] Using client Optimizer as basic optimizer\n",
      "[2022-06-29 01:22:16,417] [INFO] [engine.py:1106:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW\n",
      "[2022-06-29 01:22:16,417] [INFO] [utils.py:43:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2022-06-29 01:22:16,417] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
      "[2022-06-29 01:22:16,417] [INFO] [stage2.py:113:__init__] Reduce bucket size 50000000\n",
      "[2022-06-29 01:22:16,417] [INFO] [stage2.py:114:__init__] Allgather bucket size 50000000\n",
      "[2022-06-29 01:22:16,417] [INFO] [stage2.py:115:__init__] CPU Offload: True\n",
      "[2022-06-29 01:22:16,417] [INFO] [stage2.py:116:__init__] Round robin gradient partitioning: False\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu114/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.23016595840454102 seconds\n",
      "Rank: 0 partition count [1, 1] and sizes[(2521038848, False), (1282048, False)] \n",
      "[2022-06-29 01:22:41,436] [INFO] [utils.py:822:see_memory_usage] Before initializing optimizer states\n",
      "[2022-06-29 01:22:41,437] [INFO] [utils.py:823:see_memory_usage] MA 9.78 GB         Max_MA 9.78 GB         CA 18.8 GB         Max_CA 19 GB \n",
      "[2022-06-29 01:22:41,437] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 13.73 GB, percent = 15.2%\n",
      "[2022-06-29 01:22:56,834] [INFO] [utils.py:822:see_memory_usage] After initializing optimizer states\n",
      "[2022-06-29 01:22:56,835] [INFO] [utils.py:823:see_memory_usage] MA 9.78 GB         Max_MA 9.78 GB         CA 18.8 GB         Max_CA 19 GB \n",
      "[2022-06-29 01:22:56,835] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 42.14 GB, percent = 46.6%\n",
      "[2022-06-29 01:22:56,835] [INFO] [stage2.py:483:__init__] optimizer state initialized\n",
      "[2022-06-29 01:22:56,867] [INFO] [utils.py:822:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2022-06-29 01:22:56,867] [INFO] [utils.py:823:see_memory_usage] MA 9.78 GB         Max_MA 9.78 GB         CA 18.8 GB         Max_CA 19 GB \n",
      "[2022-06-29 01:22:56,868] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 42.14 GB, percent = 46.6%\n",
      "[2022-06-29 01:22:56,868] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2022-06-29 01:22:56,868] [INFO] [engine.py:798:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2022-06-29 01:22:56,868] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f4dcfa74f40>\n",
      "[2022-06-29 01:22:56,868] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:22:56,869] [INFO] [config.py:1044:print] DeepSpeedEngine configuration:\n",
      "[2022-06-29 01:22:56,870] [INFO] [config.py:1048:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2022-06-29 01:22:56,870] [INFO] [config.py:1048:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2022-06-29 01:22:56,870] [INFO] [config.py:1048:print]   allreduce_always_fp32 ........ False\n",
      "[2022-06-29 01:22:56,870] [INFO] [config.py:1048:print]   amp_enabled .................. False\n",
      "[2022-06-29 01:22:56,870] [INFO] [config.py:1048:print]   amp_params ................... False\n",
      "[2022-06-29 01:22:56,870] [INFO] [config.py:1048:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2022-06-29 01:22:56,870] [INFO] [config.py:1048:print]   bfloat16_enabled ............. False\n",
      "[2022-06-29 01:22:56,870] [INFO] [config.py:1048:print]   checkpoint_tag_validation_enabled  True\n",
      "[2022-06-29 01:22:56,870] [INFO] [config.py:1048:print]   checkpoint_tag_validation_fail  False\n",
      "[2022-06-29 01:22:56,870] [INFO] [config.py:1048:print]   curriculum_enabled ........... False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   curriculum_params ............ False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   dataloader_drop_last ......... False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   disable_allgather ............ False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   dump_state ................... False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   dynamic_loss_scale_args ...... None\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   eigenvalue_enabled ........... False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   eigenvalue_layer_num ......... 0\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   eigenvalue_max_iter .......... 100\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   eigenvalue_stability ......... 1e-06\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   eigenvalue_tol ............... 0.01\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   eigenvalue_verbose ........... False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   elasticity_enabled ........... False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   fp16_enabled ................. False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   fp16_master_weights_and_gradients  False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   fp16_mixed_quantize .......... False\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   global_rank .................. 0\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   gradient_accumulation_steps .. 1\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   gradient_clipping ............ 1.0\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   gradient_predivide_factor .... 1.0\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2022-06-29 01:22:56,871] [INFO] [config.py:1048:print]   loss_scale ................... 0\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   memory_breakdown ............. False\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   optimizer_legacy_fusion ...... False\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   optimizer_name ............... adam\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   optimizer_params ............. {'lr': 0.00015, 'max_grad_norm': 0.0, 'betas': [0.9, 0.95]}\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   pld_enabled .................. False\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   pld_params ................... False\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   prescale_gradients ........... False\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   quantize_change_rate ......... 0.001\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   quantize_groups .............. 1\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   quantize_offset .............. 1000\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   quantize_period .............. 1000\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   quantize_rounding ............ 0\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   quantize_start_bits .......... 16\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   quantize_target_bits ......... 8\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   quantize_training_enabled .... False\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   quantize_type ................ 0\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   quantize_verbose ............. False\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   scheduler_name ............... None\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   scheduler_params ............. None\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   sparse_attention ............. None\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   sparse_gradients_enabled ..... False\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   steps_per_print .............. 10\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   tensorboard_enabled .......... False\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2022-06-29 01:22:56,872] [INFO] [config.py:1048:print]   tensorboard_output_path ...... \n",
      "[2022-06-29 01:22:56,873] [INFO] [config.py:1048:print]   train_batch_size ............. 4\n",
      "[2022-06-29 01:22:56,873] [INFO] [config.py:1048:print]   train_micro_batch_size_per_gpu  4\n",
      "[2022-06-29 01:22:56,873] [INFO] [config.py:1048:print]   use_quantizer_kernel ......... False\n",
      "[2022-06-29 01:22:56,873] [INFO] [config.py:1048:print]   wall_clock_breakdown ......... True\n",
      "[2022-06-29 01:22:56,873] [INFO] [config.py:1048:print]   world_size ................... 1\n",
      "[2022-06-29 01:22:56,873] [INFO] [config.py:1048:print]   zero_allow_untested_optimizer  False\n",
      "[2022-06-29 01:22:56,873] [INFO] [config.py:1048:print]   zero_config .................. {\n",
      "    \"stage\": 2, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+07, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+07, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": true, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": {\n",
      "        \"device\": null, \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 4, \n",
      "        \"pin_memory\": false, \n",
      "        \"pipeline_read\": false, \n",
      "        \"pipeline_write\": false, \n",
      "        \"fast_init\": false\n",
      "    }, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_fp16_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2022-06-29 01:22:56,873] [INFO] [config.py:1048:print]   zero_enabled ................. True\n",
      "[2022-06-29 01:22:56,873] [INFO] [config.py:1048:print]   zero_optimization_stage ...... 2\n",
      "[2022-06-29 01:22:56,873] [INFO] [config.py:1050:print]   json = {\n",
      "    \"train_batch_size\": 4, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+07, \n",
      "        \"reduce_bucket_size\": 5.000000e+07, \n",
      "        \"cpu_offload\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.00015, \n",
      "            \"max_grad_norm\": 0.0, \n",
      "            \"betas\": [0.9, 0.95]\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": true, \n",
      "    \"zero_allow_untested_optimizer\": false\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005216598510742188 seconds\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT2 ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000878 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT2 datasets ...\n",
      "setting training data start iteration to 0\n",
      "setting validation data start iteration to 0\n",
      "done with setups ...\n",
      "time (ms) | model and optimizer: 40848.95 | train/valid/test data iterators: 1877.20\n",
      "training ...\n",
      "[2022-06-29 01:23:32,355] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.42 | optimizer_gradients: 336.22 | optimizer_step: 23051.78\n",
      "[2022-06-29 01:23:32,356] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 3767.33 | backward_microstep: 6359.18 | backward_inner_microstep: 6315.27 | backward_allreduce_microstep: 43.77 | step_microstep: 23447.13\n",
      "[2022-06-29 01:23:32,356] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 3767.43 | backward: 6359.17 | backward_inner: 6315.27 | backward_allreduce: 43.78 | step: 23447.13\n",
      "[2022-06-29 01:23:48,209] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.38 | optimizer_gradients: 269.12 | optimizer_step: 7418.75\n",
      "[2022-06-29 01:23:48,210] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1751.05 | backward_microstep: 6353.24 | backward_inner_microstep: 6314.72 | backward_allreduce_microstep: 38.43 | step_microstep: 7746.27\n",
      "[2022-06-29 01:23:48,210] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1751.15 | backward: 6353.24 | backward_inner: 6314.73 | backward_allreduce: 38.44 | step: 7746.30\n",
      "[2022-06-29 01:24:04,033] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.38 | optimizer_gradients: 268.24 | optimizer_step: 7380.80\n",
      "[2022-06-29 01:24:04,034] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.06 | backward_microstep: 6358.26 | backward_inner_microstep: 6319.61 | backward_allreduce_microstep: 38.57 | step_microstep: 7707.59\n",
      "[2022-06-29 01:24:04,034] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.14 | backward: 6358.26 | backward_inner: 6319.62 | backward_allreduce: 38.57 | step: 7707.59\n",
      "[2022-06-29 01:24:19,904] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 271.99 | optimizer_step: 7411.41\n",
      "[2022-06-29 01:24:19,904] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1760.82 | backward_microstep: 6364.84 | backward_inner_microstep: 6326.14 | backward_allreduce_microstep: 38.61 | step_microstep: 7741.95\n",
      "[2022-06-29 01:24:19,904] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1760.91 | backward: 6364.84 | backward_inner: 6326.15 | backward_allreduce: 38.61 | step: 7741.95\n",
      "[2022-06-29 01:24:35,782] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 283.69 | optimizer_step: 7402.45\n",
      "[2022-06-29 01:24:35,783] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.98 | backward_microstep: 6372.42 | backward_inner_microstep: 6333.15 | backward_allreduce_microstep: 39.18 | step_microstep: 7744.78\n",
      "[2022-06-29 01:24:35,783] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.07 | backward: 6372.42 | backward_inner: 6333.16 | backward_allreduce: 39.18 | step: 7744.79\n",
      "[2022-06-29 01:24:51,626] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 270.33 | optimizer_step: 7386.10\n",
      "[2022-06-29 01:24:51,627] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.52 | backward_microstep: 6368.01 | backward_inner_microstep: 6329.49 | backward_allreduce_microstep: 38.43 | step_microstep: 7715.00\n",
      "[2022-06-29 01:24:51,627] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.60 | backward: 6368.01 | backward_inner: 6329.50 | backward_allreduce: 38.43 | step: 7715.00\n",
      "[2022-06-29 01:25:07,527] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 270.95 | optimizer_step: 7434.89\n",
      "[2022-06-29 01:25:07,528] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.62 | backward_microstep: 6375.57 | backward_inner_microstep: 6337.02 | backward_allreduce_microstep: 38.46 | step_microstep: 7764.54\n",
      "[2022-06-29 01:25:07,528] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.71 | backward: 6375.57 | backward_inner: 6337.03 | backward_allreduce: 38.47 | step: 7764.54\n",
      "[2022-06-29 01:25:23,370] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 265.95 | optimizer_step: 7384.21\n",
      "[2022-06-29 01:25:23,370] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.68 | backward_microstep: 6373.24 | backward_inner_microstep: 6334.72 | backward_allreduce_microstep: 38.44 | step_microstep: 7708.68\n",
      "[2022-06-29 01:25:23,371] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.77 | backward: 6373.24 | backward_inner: 6334.72 | backward_allreduce: 38.44 | step: 7708.69\n",
      "[2022-06-29 01:25:39,281] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 314.73 | optimizer_step: 7393.52\n",
      "[2022-06-29 01:25:39,282] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1761.02 | backward_microstep: 6380.61 | backward_inner_microstep: 6342.09 | backward_allreduce_microstep: 38.43 | step_microstep: 7766.96\n",
      "[2022-06-29 01:25:39,282] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1761.09 | backward: 6380.61 | backward_inner: 6342.10 | backward_allreduce: 38.44 | step: 7766.96\n",
      "[2022-06-29 01:25:55,216] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.32 | optimizer_gradients: 276.20 | optimizer_step: 7457.55\n",
      "[2022-06-29 01:25:55,216] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.6874999999999996e-07, 4.6874999999999996e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:25:55,217] [INFO] [timer.py:181:stop] 0/10, SamplesPerSec=0.2519909447468096\n",
      "[2022-06-29 01:25:55,217] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1760.69 | backward_microstep: 6378.19 | backward_inner_microstep: 6339.72 | backward_allreduce_microstep: 38.39 | step_microstep: 7792.53\n",
      "[2022-06-29 01:25:55,217] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1760.78 | backward: 6378.19 | backward_inner: 6339.72 | backward_allreduce: 38.39 | step: 7792.53\n",
      " iteration       10/      50 | elapsed time per iteration (ms): 17646.1 | learning rate: 4.687E-07 | lm loss: 1.082407E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "after 10 iterations memory (MB) | allocated: 10016.89111328125 | max allocated: 13945.46484375 | reserved: 29582.0 | max reserved: 29582.0\n",
      "time (ms) | forward: 1963.27 | backward: 6368.46 | backward-backward: 6368.42 | backward-allreduce: 0.00 | optimizer: 9313.82 | batch generator: 3.21\n",
      "Effective Tera Flops per GPU: 0.47 and total parameters 2.522 B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 01:26:11,085] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 270.08 | optimizer_step: 7393.54\n",
      "[2022-06-29 01:26:11,086] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1763.23 | backward_microstep: 6377.99 | backward_inner_microstep: 6338.63 | backward_allreduce_microstep: 39.27 | step_microstep: 7722.09\n",
      "[2022-06-29 01:26:11,086] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1763.31 | backward: 6377.99 | backward_inner: 6338.64 | backward_allreduce: 39.27 | step: 7722.10\n",
      "[2022-06-29 01:26:26,962] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 269.20 | optimizer_step: 7408.53\n",
      "[2022-06-29 01:26:26,963] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1760.92 | backward_microstep: 6376.31 | backward_inner_microstep: 6337.84 | backward_allreduce_microstep: 38.37 | step_microstep: 7736.25\n",
      "[2022-06-29 01:26:26,963] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1761.00 | backward: 6376.31 | backward_inner: 6337.85 | backward_allreduce: 38.37 | step: 7736.26\n",
      "[2022-06-29 01:26:42,815] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 267.99 | optimizer_step: 7401.95\n",
      "[2022-06-29 01:26:42,816] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1759.26 | backward_microstep: 6362.53 | backward_inner_microstep: 6324.05 | backward_allreduce_microstep: 38.39 | step_microstep: 7728.52\n",
      "[2022-06-29 01:26:42,816] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.34 | backward: 6362.53 | backward_inner: 6324.06 | backward_allreduce: 38.39 | step: 7728.52\n",
      "[2022-06-29 01:26:58,680] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 266.95 | optimizer_step: 7408.42\n",
      "[2022-06-29 01:26:58,681] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1761.84 | backward_microstep: 6366.04 | backward_inner_microstep: 6327.46 | backward_allreduce_microstep: 38.49 | step_microstep: 7733.89\n",
      "[2022-06-29 01:26:58,681] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1761.93 | backward: 6366.04 | backward_inner: 6327.47 | backward_allreduce: 38.50 | step: 7733.90\n",
      "[2022-06-29 01:27:14,560] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 266.75 | optimizer_step: 7420.49\n",
      "[2022-06-29 01:27:14,561] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.79 | backward_microstep: 6373.39 | backward_inner_microstep: 6334.76 | backward_allreduce_microstep: 38.54 | step_microstep: 7745.72\n",
      "[2022-06-29 01:27:14,561] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.86 | backward: 6373.39 | backward_inner: 6334.77 | backward_allreduce: 38.55 | step: 7745.72\n",
      "[2022-06-29 01:27:30,493] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 267.51 | optimizer_step: 7454.40\n",
      "[2022-06-29 01:27:30,494] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1759.08 | backward_microstep: 6390.27 | backward_inner_microstep: 6351.68 | backward_allreduce_microstep: 38.50 | step_microstep: 7780.45\n",
      "[2022-06-29 01:27:30,494] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.15 | backward: 6390.27 | backward_inner: 6351.68 | backward_allreduce: 38.50 | step: 7780.45\n",
      "[2022-06-29 01:27:46,430] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 293.11 | optimizer_step: 7436.78\n",
      "[2022-06-29 01:27:46,431] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1760.29 | backward_microstep: 6384.84 | backward_inner_microstep: 6345.62 | backward_allreduce_microstep: 39.09 | step_microstep: 7788.72\n",
      "[2022-06-29 01:27:46,431] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1760.39 | backward: 6384.85 | backward_inner: 6345.64 | backward_allreduce: 39.10 | step: 7788.72\n",
      "[2022-06-29 01:28:02,357] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 266.48 | optimizer_step: 7461.55\n",
      "[2022-06-29 01:28:02,357] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.34 | backward_microstep: 6378.56 | backward_inner_microstep: 6340.06 | backward_allreduce_microstep: 38.41 | step_microstep: 7786.62\n",
      "[2022-06-29 01:28:02,358] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.42 | backward: 6378.56 | backward_inner: 6340.06 | backward_allreduce: 38.41 | step: 7786.62\n",
      "[2022-06-29 01:28:18,248] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 309.50 | optimizer_step: 7383.27\n",
      "[2022-06-29 01:28:18,249] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.12 | backward_microstep: 6379.16 | backward_inner_microstep: 6340.62 | backward_allreduce_microstep: 38.45 | step_microstep: 7751.16\n",
      "[2022-06-29 01:28:18,249] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.22 | backward: 6379.16 | backward_inner: 6340.63 | backward_allreduce: 38.46 | step: 7751.16\n",
      "[2022-06-29 01:28:34,105] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 266.27 | optimizer_step: 7391.30\n",
      "[2022-06-29 01:28:34,106] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.374999999999999e-07, 9.374999999999999e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:28:34,106] [INFO] [timer.py:181:stop] 0/20, SamplesPerSec=0.2518778712067455\n",
      "[2022-06-29 01:28:34,106] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.39 | backward_microstep: 6382.14 | backward_inner_microstep: 6343.45 | backward_allreduce_microstep: 38.60 | step_microstep: 7716.44\n",
      "[2022-06-29 01:28:34,106] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.48 | backward: 6382.14 | backward_inner: 6343.46 | backward_allreduce: 38.61 | step: 7716.45\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 15888.9 | learning rate: 9.375E-07 | lm loss: 9.506392E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1761.55 | backward: 6377.23 | backward-backward: 6377.19 | backward-allreduce: 0.00 | optimizer: 7749.25 | batch generator: 1.25\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 2.522 B\n",
      "[2022-06-29 01:28:49,967] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 267.55 | optimizer_step: 7394.37\n",
      "[2022-06-29 01:28:49,968] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1759.17 | backward_microstep: 6376.65 | backward_inner_microstep: 6337.12 | backward_allreduce_microstep: 39.45 | step_microstep: 7720.53\n",
      "[2022-06-29 01:28:49,968] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.27 | backward: 6376.65 | backward_inner: 6337.12 | backward_allreduce: 39.45 | step: 7720.53\n",
      "[2022-06-29 01:29:05,823] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.40 | optimizer_gradients: 266.29 | optimizer_step: 7388.87\n",
      "[2022-06-29 01:29:05,824] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.13 | backward_microstep: 6381.12 | backward_inner_microstep: 6342.38 | backward_allreduce_microstep: 38.61 | step_microstep: 7713.80\n",
      "[2022-06-29 01:29:05,824] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.21 | backward: 6381.11 | backward_inner: 6342.39 | backward_allreduce: 38.61 | step: 7713.80\n",
      "[2022-06-29 01:29:21,661] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 268.26 | optimizer_step: 7375.02\n",
      "[2022-06-29 01:29:21,661] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.54 | backward_microstep: 6376.06 | backward_inner_microstep: 6337.58 | backward_allreduce_microstep: 38.39 | step_microstep: 7701.76\n",
      "[2022-06-29 01:29:21,662] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.63 | backward: 6376.06 | backward_inner: 6337.59 | backward_allreduce: 38.39 | step: 7701.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 01:29:37,519] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 264.97 | optimizer_step: 7388.26\n",
      "[2022-06-29 01:29:37,519] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1759.43 | backward_microstep: 6383.61 | backward_inner_microstep: 6345.15 | backward_allreduce_microstep: 38.37 | step_microstep: 7711.75\n",
      "[2022-06-29 01:29:37,519] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.52 | backward: 6383.61 | backward_inner: 6345.16 | backward_allreduce: 38.37 | step: 7711.75\n",
      "[2022-06-29 01:29:53,365] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 264.47 | optimizer_step: 7392.00\n",
      "[2022-06-29 01:29:53,366] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.86 | backward_microstep: 6372.57 | backward_inner_microstep: 6334.04 | backward_allreduce_microstep: 38.44 | step_microstep: 7715.13\n",
      "[2022-06-29 01:29:53,366] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.96 | backward: 6372.57 | backward_inner: 6334.05 | backward_allreduce: 38.44 | step: 7715.14\n",
      "[2022-06-29 01:30:09,318] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 286.31 | optimizer_step: 7465.79\n",
      "[2022-06-29 01:30:09,319] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.40 | backward_microstep: 6381.79 | backward_inner_microstep: 6343.13 | backward_allreduce_microstep: 38.56 | step_microstep: 7810.82\n",
      "[2022-06-29 01:30:09,319] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.49 | backward: 6381.79 | backward_inner: 6343.14 | backward_allreduce: 38.57 | step: 7810.82\n",
      "[2022-06-29 01:30:25,131] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 266.49 | optimizer_step: 7356.94\n",
      "[2022-06-29 01:30:25,131] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1751.93 | backward_microstep: 6375.18 | backward_inner_microstep: 6336.07 | backward_allreduce_microstep: 39.02 | step_microstep: 7682.09\n",
      "[2022-06-29 01:30:25,131] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1752.03 | backward: 6375.18 | backward_inner: 6336.08 | backward_allreduce: 39.02 | step: 7682.10\n",
      "[2022-06-29 01:30:40,965] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 266.86 | optimizer_step: 7371.05\n",
      "[2022-06-29 01:30:40,965] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.94 | backward_microstep: 6377.25 | backward_inner_microstep: 6338.84 | backward_allreduce_microstep: 38.32 | step_microstep: 7696.53\n",
      "[2022-06-29 01:30:40,965] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.03 | backward: 6377.25 | backward_inner: 6338.85 | backward_allreduce: 38.33 | step: 7696.53\n",
      "[2022-06-29 01:30:57,009] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 370.75 | optimizer_step: 7475.26\n",
      "[2022-06-29 01:30:57,010] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.87 | backward_microstep: 6378.00 | backward_inner_microstep: 6339.40 | backward_allreduce_microstep: 38.52 | step_microstep: 7904.64\n",
      "[2022-06-29 01:30:57,010] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.94 | backward: 6378.00 | backward_inner: 6339.40 | backward_allreduce: 38.52 | step: 7904.64\n",
      "[2022-06-29 01:31:12,873] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.38 | optimizer_gradients: 266.70 | optimizer_step: 7400.33\n",
      "[2022-06-29 01:31:12,873] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.4062499999999999e-06, 1.4062499999999999e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:31:12,874] [INFO] [timer.py:181:stop] 0/30, SamplesPerSec=0.2519142251140072\n",
      "[2022-06-29 01:31:12,874] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.91 | backward_microstep: 6377.79 | backward_inner_microstep: 6339.33 | backward_allreduce_microstep: 38.37 | step_microstep: 7725.79\n",
      "[2022-06-29 01:31:12,874] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.99 | backward: 6377.79 | backward_inner: 6339.34 | backward_allreduce: 38.37 | step: 7725.79\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 15876.8 | learning rate: 1.406E-06 | lm loss: 9.063718E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1759.30 | backward: 6378.11 | backward-backward: 6378.07 | backward-allreduce: 0.00 | optimizer: 7738.55 | batch generator: 1.29\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 2.522 B\n",
      "[2022-06-29 01:31:28,750] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 265.76 | optimizer_step: 7401.88\n",
      "[2022-06-29 01:31:28,751] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1760.49 | backward_microstep: 6385.01 | backward_inner_microstep: 6346.54 | backward_allreduce_microstep: 38.38 | step_microstep: 7726.26\n",
      "[2022-06-29 01:31:28,751] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1760.57 | backward: 6385.01 | backward_inner: 6346.55 | backward_allreduce: 38.38 | step: 7726.26\n",
      "[2022-06-29 01:31:44,612] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 268.36 | optimizer_step: 7413.57\n",
      "[2022-06-29 01:31:44,613] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1753.94 | backward_microstep: 6364.55 | backward_inner_microstep: 6326.09 | backward_allreduce_microstep: 38.37 | step_microstep: 7740.48\n",
      "[2022-06-29 01:31:44,613] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.02 | backward: 6364.55 | backward_inner: 6326.10 | backward_allreduce: 38.37 | step: 7740.48\n",
      "[2022-06-29 01:32:00,477] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 266.12 | optimizer_step: 7403.76\n",
      "[2022-06-29 01:32:00,478] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1754.75 | backward_microstep: 6378.34 | backward_inner_microstep: 6339.83 | backward_allreduce_microstep: 38.41 | step_microstep: 7728.50\n",
      "[2022-06-29 01:32:00,478] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.83 | backward: 6378.34 | backward_inner: 6339.84 | backward_allreduce: 38.41 | step: 7728.50\n",
      "[2022-06-29 01:32:16,349] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 268.46 | optimizer_step: 7405.53\n",
      "[2022-06-29 01:32:16,350] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1754.80 | backward_microstep: 6381.31 | backward_inner_microstep: 6342.83 | backward_allreduce_microstep: 38.39 | step_microstep: 7732.60\n",
      "[2022-06-29 01:32:16,350] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.88 | backward: 6381.31 | backward_inner: 6342.84 | backward_allreduce: 38.39 | step: 7732.61\n",
      "[2022-06-29 01:32:32,218] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 273.90 | optimizer_step: 7412.64\n",
      "[2022-06-29 01:32:32,218] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1753.61 | backward_microstep: 6366.66 | backward_inner_microstep: 6327.17 | backward_allreduce_microstep: 39.40 | step_microstep: 7745.16\n",
      "[2022-06-29 01:32:32,219] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1753.68 | backward: 6366.66 | backward_inner: 6327.18 | backward_allreduce: 39.40 | step: 7745.17\n",
      "[2022-06-29 01:32:48,101] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 265.76 | optimizer_step: 7435.91\n",
      "[2022-06-29 01:32:48,102] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.16 | backward_microstep: 6364.18 | backward_inner_microstep: 6325.73 | backward_allreduce_microstep: 38.36 | step_microstep: 7760.29\n",
      "[2022-06-29 01:32:48,102] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.24 | backward: 6364.18 | backward_inner: 6325.74 | backward_allreduce: 38.36 | step: 7760.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 01:33:03,946] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 265.89 | optimizer_step: 7384.33\n",
      "[2022-06-29 01:33:03,947] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.36 | backward_microstep: 6377.65 | backward_inner_microstep: 6339.19 | backward_allreduce_microstep: 38.36 | step_microstep: 7708.76\n",
      "[2022-06-29 01:33:03,947] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.42 | backward: 6377.65 | backward_inner: 6339.20 | backward_allreduce: 38.37 | step: 7708.76\n",
      "[2022-06-29 01:33:19,823] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 269.27 | optimizer_step: 7400.83\n",
      "[2022-06-29 01:33:19,823] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1754.49 | backward_microstep: 6390.42 | backward_inner_microstep: 6351.86 | backward_allreduce_microstep: 38.47 | step_microstep: 7728.76\n",
      "[2022-06-29 01:33:19,823] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.58 | backward: 6390.42 | backward_inner: 6351.87 | backward_allreduce: 38.48 | step: 7728.76\n",
      "[2022-06-29 01:33:35,699] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 277.77 | optimizer_step: 7398.80\n",
      "[2022-06-29 01:33:35,700] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.60 | backward_microstep: 6381.48 | backward_inner_microstep: 6342.92 | backward_allreduce_microstep: 38.48 | step_microstep: 7735.25\n",
      "[2022-06-29 01:33:35,700] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.68 | backward: 6381.48 | backward_inner: 6342.92 | backward_allreduce: 38.48 | step: 7735.25\n",
      "[2022-06-29 01:33:51,569] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 268.22 | optimizer_step: 7412.69\n",
      "[2022-06-29 01:33:51,570] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.8749999999999998e-06, 1.8749999999999998e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:33:51,570] [INFO] [timer.py:181:stop] 0/40, SamplesPerSec=0.25196149273020724\n",
      "[2022-06-29 01:33:51,570] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1754.72 | backward_microstep: 6373.25 | backward_inner_microstep: 6334.73 | backward_allreduce_microstep: 38.43 | step_microstep: 7739.66\n",
      "[2022-06-29 01:33:51,570] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.81 | backward: 6373.25 | backward_inner: 6334.74 | backward_allreduce: 38.43 | step: 7739.67\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 15869.7 | learning rate: 1.875E-06 | lm loss: 8.885904E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1757.55 | backward: 6376.39 | backward-backward: 6376.35 | backward-allreduce: 0.00 | optimizer: 7734.88 | batch generator: 1.29\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 2.522 B\n",
      "[2022-06-29 01:34:07,479] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.39 | optimizer_gradients: 306.14 | optimizer_step: 7413.16\n",
      "[2022-06-29 01:34:07,479] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1752.92 | backward_microstep: 6372.71 | backward_inner_microstep: 6334.03 | backward_allreduce_microstep: 38.58 | step_microstep: 7777.95\n",
      "[2022-06-29 01:34:07,479] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1753.00 | backward: 6372.72 | backward_inner: 6334.04 | backward_allreduce: 38.59 | step: 7777.94\n",
      "[2022-06-29 01:34:23,317] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 267.66 | optimizer_step: 7388.19\n",
      "[2022-06-29 01:34:23,318] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1751.43 | backward_microstep: 6369.55 | backward_inner_microstep: 6330.33 | backward_allreduce_microstep: 39.13 | step_microstep: 7714.43\n",
      "[2022-06-29 01:34:23,318] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1751.53 | backward: 6369.55 | backward_inner: 6330.34 | backward_allreduce: 39.14 | step: 7714.44\n",
      "[2022-06-29 01:34:39,145] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 266.43 | optimizer_step: 7374.00\n",
      "[2022-06-29 01:34:39,145] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1752.91 | backward_microstep: 6372.35 | backward_inner_microstep: 6333.10 | backward_allreduce_microstep: 39.16 | step_microstep: 7698.92\n",
      "[2022-06-29 01:34:39,145] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1753.01 | backward: 6372.35 | backward_inner: 6333.10 | backward_allreduce: 39.16 | step: 7698.92\n",
      "[2022-06-29 01:34:55,006] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.40 | optimizer_gradients: 269.26 | optimizer_step: 7400.40\n",
      "[2022-06-29 01:34:55,007] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1752.79 | backward_microstep: 6377.69 | backward_inner_microstep: 6338.72 | backward_allreduce_microstep: 38.89 | step_microstep: 7728.19\n",
      "[2022-06-29 01:34:55,007] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1752.88 | backward: 6377.69 | backward_inner: 6338.72 | backward_allreduce: 38.89 | step: 7728.19\n",
      "[2022-06-29 01:35:10,911] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 269.99 | optimizer_step: 7430.41\n",
      "[2022-06-29 01:35:10,912] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.26 | backward_microstep: 6386.40 | backward_inner_microstep: 6347.89 | backward_allreduce_microstep: 38.42 | step_microstep: 7759.03\n",
      "[2022-06-29 01:35:10,912] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.35 | backward: 6386.40 | backward_inner: 6347.90 | backward_allreduce: 38.43 | step: 7759.03\n",
      "[2022-06-29 01:35:26,764] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.38 | optimizer_gradients: 269.21 | optimizer_step: 7398.17\n",
      "[2022-06-29 01:35:26,765] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1759.78 | backward_microstep: 6364.54 | backward_inner_microstep: 6326.01 | backward_allreduce_microstep: 38.45 | step_microstep: 7725.89\n",
      "[2022-06-29 01:35:26,765] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.86 | backward: 6364.54 | backward_inner: 6326.01 | backward_allreduce: 38.45 | step: 7725.89\n",
      "[2022-06-29 01:35:42,640] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.40 | optimizer_gradients: 268.53 | optimizer_step: 7406.53\n",
      "[2022-06-29 01:35:42,641] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.98 | backward_microstep: 6383.21 | backward_inner_microstep: 6344.72 | backward_allreduce_microstep: 38.41 | step_microstep: 7733.61\n",
      "[2022-06-29 01:35:42,641] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.05 | backward: 6383.21 | backward_inner: 6344.72 | backward_allreduce: 38.41 | step: 7733.61\n",
      "[2022-06-29 01:35:58,496] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 273.38 | optimizer_step: 7403.21\n",
      "[2022-06-29 01:35:58,497] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.21 | backward_microstep: 6362.31 | backward_inner_microstep: 6323.75 | backward_allreduce_microstep: 38.44 | step_microstep: 7735.24\n",
      "[2022-06-29 01:35:58,497] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.30 | backward: 6362.31 | backward_inner: 6323.77 | backward_allreduce: 38.44 | step: 7735.25\n",
      "[2022-06-29 01:36:14,368] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 267.86 | optimizer_step: 7405.49\n",
      "[2022-06-29 01:36:14,369] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1752.61 | backward_microstep: 6384.28 | backward_inner_microstep: 6345.54 | backward_allreduce_microstep: 38.64 | step_microstep: 7731.88\n",
      "[2022-06-29 01:36:14,369] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1752.69 | backward: 6384.28 | backward_inner: 6345.55 | backward_allreduce: 38.65 | step: 7731.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 01:36:30,237] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 271.23 | optimizer_step: 7408.36\n",
      "[2022-06-29 01:36:30,237] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.34375e-06, 2.34375e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:36:30,237] [INFO] [timer.py:181:stop] 0/50, SamplesPerSec=0.25199842464335126\n",
      "[2022-06-29 01:36:30,238] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1754.44 | backward_microstep: 6372.73 | backward_inner_microstep: 6334.18 | backward_allreduce_microstep: 38.46 | step_microstep: 7738.42\n",
      "[2022-06-29 01:36:30,238] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.53 | backward: 6372.73 | backward_inner: 6334.18 | backward_allreduce: 38.47 | step: 7738.42\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 15866.7 | learning rate: 2.344E-06 | lm loss: 8.758082E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1756.59 | backward: 6374.68 | backward-backward: 6374.65 | backward-allreduce: 0.00 | optimizer: 7734.63 | batch generator: 1.25\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 2.522 B\n",
      "rank: 0 | time: 2022-06-29 01:36:30 | exiting the program at iteration 50\n",
      "[2022-06-29 01:36:36,806] [INFO] [launch.py:159:main] Process 35951 exits successfully.\n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/sc22-gpt-zero-infinity-cpu.sh 48 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_zero-infinity_l-48_hs-2048_bs-4_ws-4_2022-06-29.1656466598.txt && cd -\n",
      "deepspeed --num_nodes 1 --num_gpus 1 pretrain_gpt2.py --model-parallel-size 1 --num-layers 48 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json --zero-stage 3 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-06-29 01:36:39,685] [WARNING] [runner.py:132:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2022-06-29 01:36:42,289] [INFO] [runner.py:398:main] cmd = /root/.pyenv/versions/3.9.10/envs/py3.9.10/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 pretrain_gpt2.py --model-parallel-size 1 --num-layers 48 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json --zero-stage 3 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda11.4\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda11.4\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:100:main] dist_world_size=1\n",
      "[2022-06-29 01:36:43,144] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... 4\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... True\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ True\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... mmap\n",
      "  data_path ....................... /home/sys/STRONGHOLD/data/my-gpt2-en_text_document\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... True\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 1000\n",
      "  exit_interval ................... 50\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 2048\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ checkpoints/gpt2_ds\n",
      "  local_rank ...................... 0\n",
      "  log_interval .................... 10\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. 0.00015\n",
      "  lr_decay_iters .................. 320000\n",
      "  lr_decay_style .................. cosine\n",
      "  lr_decay_tokens ................. None\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 1024\n",
      "  memory_centric_tiled_linear ..... False\n",
      "  merge_file ...................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  min_lr .......................... 1e-05\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 16\n",
      "  num_layers ...................... 48\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... True\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  remote_device ................... none\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ checkpoints/gpt2_ds\n",
      "  save_interval ................... 10000\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  scattered_embeddings ............ False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... 1024\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 949,50,1\n",
      "  split_transformers .............. False\n",
      "  synchronize_each_layer .......... True\n",
      "  tensorboard_dir ................. None\n",
      "  tile_factor ..................... 1\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  tokens .......................... 0\n",
      "  train_iters ..................... 50\n",
      "  train_tokens .................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  use_pin_memory .................. False\n",
      "  vocab_file ...................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  warmup .......................... 0.01\n",
      "  warmup_iters .................... None\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 5000000000\n",
      "  zero_contigious_gradients ....... True\n",
      "  zero_reduce_bucket_size ......... 50000000\n",
      "  zero_reduce_scatter ............. True\n",
      "  zero_stage ...................... 3\n",
      "---------------- end of arguments ----------------\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "building GPT2 model ...\n",
      "[2022-06-29 01:36:46,842] [INFO] [utils.py:822:see_memory_usage] Before Building Model\n",
      "[2022-06-29 01:36:46,843] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2022-06-29 01:36:46,843] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.08 GB, percent = 3.4%\n",
      "[2022-06-29 01:36:46,843] [WARNING] [partition_parameters.py:457:__init__] zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7fc2986461f0>\n",
      "train_batch = None, micro_batch=4\n",
      "[2022-06-29 01:36:54,466] [INFO] [utils.py:822:see_memory_usage] After Building Model\n",
      "[2022-06-29 01:36:54,467] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.38 GB         CA 0.39 GB         Max_CA 0 GB \n",
      "[2022-06-29 01:36:54,467] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 12.75 GB, percent = 14.1%\n",
      " > number of parameters on model parallel rank 0            2.522 Billion\n",
      "> learning rate decay style: cosine\n",
      "DeepSpeed is enabled.\n",
      "[2022-06-29 01:36:54,472] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.7, git-hash=unknown, git-branch=unknown\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7fc2986461f0>\n",
      "train_batch = None, micro_batch=4\n",
      "[2022-06-29 01:36:54,485] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups using mpu\n",
      "[2022-06-29 01:36:54,485] [INFO] [logging.py:69:log_dist] [Rank 0] Initializing deepspeed groups with model parallel size 1, expert parallel size 1, and data parallel size 1\n",
      "[2022-06-29 01:36:54,486] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
      "[2022-06-29 01:36:54,486] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
      "[2022-06-29 01:36:54,488] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "[2022-06-29 01:36:54,489] [INFO] [engine.py:1084:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2022-06-29 01:36:54,489] [INFO] [engine.py:1090:_configure_optimizer] Using client Optimizer as basic optimizer\n",
      "[2022-06-29 01:36:54,539] [INFO] [engine.py:1106:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW\n",
      "[2022-06-29 01:36:54,539] [INFO] [utils.py:43:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2022-06-29 01:36:54,539] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
      "Initializing ZeRO Stage 3\n",
      "[2022-06-29 01:36:54,545] [INFO] [stage3.py:639:__init__] Reduce bucket size 90000000\n",
      "[2022-06-29 01:36:54,545] [INFO] [stage3.py:640:__init__] Allgather bucket size 50000000.0\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu114/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.22570323944091797 seconds\n",
      "[2022-06-29 01:37:17,445] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 14672.86\n",
      "[2022-06-29 01:37:17,460] [INFO] [stage3.py:811:__init__] optimizer state initialized\n",
      "[2022-06-29 01:37:18,560] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2022-06-29 01:37:18,560] [INFO] [engine.py:798:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2022-06-29 01:37:18,560] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7fc29751a130>\n",
      "[2022-06-29 01:37:18,561] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:37:18,562] [INFO] [config.py:1044:print] DeepSpeedEngine configuration:\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 16, 'thread_count': 2, 'single_submit': False, 'overlap_events': True}\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   allreduce_always_fp32 ........ False\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   amp_enabled .................. False\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   amp_params ................... False\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   bfloat16_enabled ............. False\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   checkpoint_tag_validation_enabled  True\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   checkpoint_tag_validation_fail  False\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   curriculum_enabled ........... False\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   curriculum_params ............ False\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   dataloader_drop_last ......... False\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   disable_allgather ............ False\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   dump_state ................... False\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   dynamic_loss_scale_args ...... None\n",
      "[2022-06-29 01:37:18,563] [INFO] [config.py:1048:print]   eigenvalue_enabled ........... False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   eigenvalue_layer_num ......... 0\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   eigenvalue_max_iter .......... 100\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   eigenvalue_stability ......... 1e-06\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   eigenvalue_tol ............... 0.01\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   eigenvalue_verbose ........... False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   elasticity_enabled ........... False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   fp16_enabled ................. False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   fp16_master_weights_and_gradients  False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   fp16_mixed_quantize .......... False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   global_rank .................. 0\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   gradient_accumulation_steps .. 1\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   gradient_clipping ............ 1.0\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   gradient_predivide_factor .... 1.0\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   loss_scale ................... 0\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   memory_breakdown ............. False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   optimizer_legacy_fusion ...... False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   optimizer_name ............... None\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   optimizer_params ............. None\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   pld_enabled .................. False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   pld_params ................... False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   prescale_gradients ........... False\n",
      "[2022-06-29 01:37:18,564] [INFO] [config.py:1048:print]   quantize_change_rate ......... 0.001\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   quantize_groups .............. 1\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   quantize_offset .............. 1000\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   quantize_period .............. 1000\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   quantize_rounding ............ 0\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   quantize_start_bits .......... 16\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   quantize_target_bits ......... 8\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   quantize_training_enabled .... False\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   quantize_type ................ 0\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   quantize_verbose ............. False\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   scheduler_name ............... None\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   scheduler_params ............. None\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   sparse_attention ............. None\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   sparse_gradients_enabled ..... False\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   steps_per_print .............. 10\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   tensorboard_enabled .......... False\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   tensorboard_output_path ...... \n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   train_batch_size ............. 4\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   train_micro_batch_size_per_gpu  4\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   use_quantizer_kernel ......... False\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   wall_clock_breakdown ......... True\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   world_size ................... 1\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   zero_allow_untested_optimizer  False\n",
      "[2022-06-29 01:37:18,565] [INFO] [config.py:1048:print]   zero_config .................. {\n",
      "    \"stage\": 3, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 9.000000e+07, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": true, \n",
      "    \"offload_param\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 5, \n",
      "        \"buffer_size\": 1.000000e+08, \n",
      "        \"max_in_cpu\": 1, \n",
      "        \"pin_memory\": true\n",
      "    }, \n",
      "    \"offload_optimizer\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 4, \n",
      "        \"pin_memory\": true, \n",
      "        \"pipeline_read\": false, \n",
      "        \"pipeline_write\": false, \n",
      "        \"fast_init\": false, \n",
      "        \"pipeline\": false\n",
      "    }, \n",
      "    \"sub_group_size\": 1.000000e+08, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_fp16_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2022-06-29 01:37:18,566] [INFO] [config.py:1048:print]   zero_enabled ................. True\n",
      "[2022-06-29 01:37:18,566] [INFO] [config.py:1048:print]   zero_optimization_stage ...... 3\n",
      "[2022-06-29 01:37:18,566] [INFO] [config.py:1050:print]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_param_persitence_threshold\": 1.000000e+05, \n",
      "        \"stage3_prefetch_bucket_size\": 5.000000e+07, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_bucket_size\": 9.000000e+07, \n",
      "        \"sub_group_size\": 1.000000e+08, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"buffer_count\": 4, \n",
      "            \"pipeline_read\": false, \n",
      "            \"pipeline_write\": false, \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"max_in_cpu\": 1, \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": true, \n",
      "    \"zero_allow_untested_optimizer\": false, \n",
      "    \"aio\": {\n",
      "        \"block_size\": 1.048576e+06, \n",
      "        \"queue_depth\": 16, \n",
      "        \"single_submit\": false, \n",
      "        \"overlap_events\": true, \n",
      "        \"thread_count\": 2\n",
      "    }\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005750656127929688 seconds\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT2 ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000456 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT2 datasets ...\n",
      "setting training data start iteration to 0\n",
      "setting validation data start iteration to 0\n",
      "done with setups ...\n",
      "time (ms) | model and optimizer: 31767.19 | train/valid/test data iterators: 1643.72\n",
      "training ...\n",
      "[2022-06-29 01:37:53,021] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22142.63\n",
      "[2022-06-29 01:37:53,022] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 3243.24 | backward_microstep: 7364.64 | backward_inner_microstep: 7287.22 | backward_allreduce_microstep: 77.27 | step_microstep: 22188.54\n",
      "[2022-06-29 01:37:53,022] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 3243.24 | backward: 7364.64 | backward_inner: 7287.24 | backward_allreduce: 77.30 | step: 22188.55\n",
      "[2022-06-29 01:38:09,245] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6205.35\n",
      "[2022-06-29 01:38:09,246] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2561.90 | backward_microstep: 7409.00 | backward_inner_microstep: 7331.93 | backward_allreduce_microstep: 76.95 | step_microstep: 6248.33\n",
      "[2022-06-29 01:38:09,246] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2561.93 | backward: 7408.99 | backward_inner: 7331.93 | backward_allreduce: 76.96 | step: 6248.34\n",
      "[2022-06-29 01:38:25,646] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6217.35\n",
      "[2022-06-29 01:38:25,647] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2606.35 | backward_microstep: 7528.59 | backward_inner_microstep: 7452.45 | backward_allreduce_microstep: 76.01 | step_microstep: 6261.41\n",
      "[2022-06-29 01:38:25,647] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2606.39 | backward: 7528.59 | backward_inner: 7452.46 | backward_allreduce: 76.04 | step: 6261.41\n",
      "[2022-06-29 01:38:42,068] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6229.15\n",
      "[2022-06-29 01:38:42,069] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2632.88 | backward_microstep: 7512.06 | backward_inner_microstep: 7436.57 | backward_allreduce_microstep: 75.38 | step_microstep: 6272.39\n",
      "[2022-06-29 01:38:42,069] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2632.91 | backward: 7512.06 | backward_inner: 7436.58 | backward_allreduce: 75.40 | step: 6272.39\n",
      "[2022-06-29 01:38:58,519] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6266.46\n",
      "[2022-06-29 01:38:58,520] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2607.20 | backward_microstep: 7528.07 | backward_inner_microstep: 7452.29 | backward_allreduce_microstep: 75.69 | step_microstep: 6310.53\n",
      "[2022-06-29 01:38:58,520] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2607.20 | backward: 7528.07 | backward_inner: 7452.30 | backward_allreduce: 75.69 | step: 6310.53\n",
      "[2022-06-29 01:39:14,955] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6243.27\n",
      "[2022-06-29 01:39:14,956] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2605.57 | backward_microstep: 7538.54 | backward_inner_microstep: 7463.09 | backward_allreduce_microstep: 75.35 | step_microstep: 6287.13\n",
      "[2022-06-29 01:39:14,956] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2605.59 | backward: 7538.54 | backward_inner: 7463.10 | backward_allreduce: 75.36 | step: 6287.13\n",
      "[2022-06-29 01:39:31,447] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6260.68\n",
      "[2022-06-29 01:39:31,448] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2646.80 | backward_microstep: 7536.04 | backward_inner_microstep: 7458.45 | backward_allreduce_microstep: 77.47 | step_microstep: 6305.02\n",
      "[2022-06-29 01:39:31,449] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2646.81 | backward: 7536.04 | backward_inner: 7458.47 | backward_allreduce: 77.49 | step: 6305.02\n",
      "[2022-06-29 01:39:47,870] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6246.22\n",
      "[2022-06-29 01:39:47,871] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2610.35 | backward_microstep: 7516.49 | backward_inner_microstep: 7440.72 | backward_allreduce_microstep: 75.67 | step_microstep: 6291.15\n",
      "[2022-06-29 01:39:47,871] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2610.38 | backward: 7516.49 | backward_inner: 7440.73 | backward_allreduce: 75.68 | step: 6291.15\n",
      "[2022-06-29 01:40:04,462] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6399.60\n",
      "[2022-06-29 01:40:04,463] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2607.87 | backward_microstep: 7535.16 | backward_inner_microstep: 7455.17 | backward_allreduce_microstep: 79.88 | step_microstep: 6444.29\n",
      "[2022-06-29 01:40:04,463] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2607.88 | backward: 7535.17 | backward_inner: 7455.19 | backward_allreduce: 79.90 | step: 6444.29\n",
      "[2022-06-29 01:40:20,899] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6254.72\n",
      "[2022-06-29 01:40:20,900] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.6874999999999996e-07, 4.6874999999999996e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:40:20,900] [INFO] [timer.py:181:stop] 0/10, SamplesPerSec=0.24312370111631262\n",
      "[2022-06-29 01:40:20,900] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2624.71 | backward_microstep: 7508.80 | backward_inner_microstep: 7431.60 | backward_allreduce_microstep: 77.08 | step_microstep: 6298.45\n",
      "[2022-06-29 01:40:20,900] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2624.72 | backward: 7508.80 | backward_inner: 7431.61 | backward_allreduce: 77.11 | step: 6298.46\n",
      " iteration       10/      50 | elapsed time per iteration (ms): 18068.5 | learning rate: 4.687E-07 | lm loss: 1.127124E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "after 10 iterations memory (MB) | allocated: 792.986328125 | max allocated: 5485.412109375 | reserved: 8252.0 | max reserved: 8252.0\n",
      "time (ms) | forward: 2678.61 | backward: 7497.84 | backward-backward: 7497.80 | backward-allreduce: 0.00 | optimizer: 7891.01 | batch generator: 1.90\n",
      "Effective Tera Flops per GPU: 0.46 and total parameters 2.522 B\n",
      "[2022-06-29 01:40:37,370] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6260.99\n",
      "[2022-06-29 01:40:37,372] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2613.01 | backward_microstep: 7544.40 | backward_inner_microstep: 7467.62 | backward_allreduce_microstep: 76.68 | step_microstep: 6306.37\n",
      "[2022-06-29 01:40:37,372] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2613.04 | backward: 7544.40 | backward_inner: 7467.63 | backward_allreduce: 76.69 | step: 6306.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 01:40:53,783] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6228.15\n",
      "[2022-06-29 01:40:53,784] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2611.08 | backward_microstep: 7522.57 | backward_inner_microstep: 7445.85 | backward_allreduce_microstep: 76.61 | step_microstep: 6273.97\n",
      "[2022-06-29 01:40:53,784] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2611.09 | backward: 7522.57 | backward_inner: 7445.86 | backward_allreduce: 76.62 | step: 6273.98\n",
      "[2022-06-29 01:41:10,264] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6256.10\n",
      "[2022-06-29 01:41:10,265] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2636.03 | backward_microstep: 7540.76 | backward_inner_microstep: 7464.15 | backward_allreduce_microstep: 76.48 | step_microstep: 6299.09\n",
      "[2022-06-29 01:41:10,265] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2636.06 | backward: 7540.76 | backward_inner: 7464.16 | backward_allreduce: 76.51 | step: 6299.09\n",
      "[2022-06-29 01:41:26,676] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6227.32\n",
      "[2022-06-29 01:41:26,677] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2614.95 | backward_microstep: 7520.94 | backward_inner_microstep: 7444.06 | backward_allreduce_microstep: 76.74 | step_microstep: 6271.12\n",
      "[2022-06-29 01:41:26,677] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2614.96 | backward: 7520.94 | backward_inner: 7444.07 | backward_allreduce: 76.78 | step: 6271.13\n",
      "[2022-06-29 01:41:43,191] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6322.48\n",
      "[2022-06-29 01:41:43,192] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2621.65 | backward_microstep: 7522.19 | backward_inner_microstep: 7445.81 | backward_allreduce_microstep: 76.24 | step_microstep: 6366.63\n",
      "[2022-06-29 01:41:43,192] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2621.67 | backward: 7522.19 | backward_inner: 7445.83 | backward_allreduce: 76.27 | step: 6366.63\n",
      "[2022-06-29 01:41:59,613] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6215.72\n",
      "[2022-06-29 01:41:59,614] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2625.67 | backward_microstep: 7532.75 | backward_inner_microstep: 7456.18 | backward_allreduce_microstep: 76.43 | step_microstep: 6259.67\n",
      "[2022-06-29 01:41:59,614] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2625.68 | backward: 7532.75 | backward_inner: 7456.21 | backward_allreduce: 76.46 | step: 6259.67\n",
      "[2022-06-29 01:42:16,049] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6244.63\n",
      "[2022-06-29 01:42:16,050] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2617.09 | backward_microstep: 7524.41 | backward_inner_microstep: 7447.85 | backward_allreduce_microstep: 76.45 | step_microstep: 6289.78\n",
      "[2022-06-29 01:42:16,050] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2617.10 | backward: 7524.41 | backward_inner: 7447.87 | backward_allreduce: 76.47 | step: 6289.78\n",
      "[2022-06-29 01:42:32,482] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6237.47\n",
      "[2022-06-29 01:42:32,483] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2615.29 | backward_microstep: 7529.61 | backward_inner_microstep: 7451.73 | backward_allreduce_microstep: 77.75 | step_microstep: 6282.74\n",
      "[2022-06-29 01:42:32,484] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2615.30 | backward: 7529.61 | backward_inner: 7451.74 | backward_allreduce: 77.78 | step: 6282.75\n",
      "[2022-06-29 01:42:49,002] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6299.70\n",
      "[2022-06-29 01:42:49,003] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2635.02 | backward_microstep: 7534.89 | backward_inner_microstep: 7459.38 | backward_allreduce_microstep: 75.40 | step_microstep: 6344.35\n",
      "[2022-06-29 01:42:49,003] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2635.04 | backward: 7534.89 | backward_inner: 7459.40 | backward_allreduce: 75.41 | step: 6344.35\n",
      "[2022-06-29 01:43:05,415] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6228.92\n",
      "[2022-06-29 01:43:05,416] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.374999999999999e-07, 9.374999999999999e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:43:05,416] [INFO] [timer.py:181:stop] 0/20, SamplesPerSec=0.2431688385299387\n",
      "[2022-06-29 01:43:05,416] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2619.63 | backward_microstep: 7516.32 | backward_inner_microstep: 7439.47 | backward_allreduce_microstep: 76.71 | step_microstep: 6273.03\n",
      "[2022-06-29 01:43:05,416] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2619.66 | backward: 7516.32 | backward_inner: 7439.49 | backward_allreduce: 76.74 | step: 6273.05\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 16451.6 | learning rate: 9.375E-07 | lm loss: 1.123667E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 2624.31 | backward: 7528.98 | backward-backward: 7528.94 | backward-allreduce: 0.00 | optimizer: 6296.97 | batch generator: 1.41\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 2.522 B\n",
      "[2022-06-29 01:43:21,840] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6228.01\n",
      "[2022-06-29 01:43:21,841] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2626.15 | backward_microstep: 7519.58 | backward_inner_microstep: 7443.19 | backward_allreduce_microstep: 76.26 | step_microstep: 6271.47\n",
      "[2022-06-29 01:43:21,841] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2626.21 | backward: 7519.57 | backward_inner: 7443.20 | backward_allreduce: 76.28 | step: 6271.47\n",
      "[2022-06-29 01:43:38,266] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6236.00\n",
      "[2022-06-29 01:43:38,268] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2633.81 | backward_microstep: 7508.90 | backward_inner_microstep: 7431.80 | backward_allreduce_microstep: 77.00 | step_microstep: 6279.50\n",
      "[2022-06-29 01:43:38,268] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2633.85 | backward: 7508.90 | backward_inner: 7431.81 | backward_allreduce: 77.02 | step: 6279.51\n",
      "[2022-06-29 01:43:54,677] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6205.86\n",
      "[2022-06-29 01:43:54,678] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2615.56 | backward_microstep: 7541.45 | backward_inner_microstep: 7465.02 | backward_allreduce_microstep: 76.32 | step_microstep: 6249.19\n",
      "[2022-06-29 01:43:54,678] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2615.59 | backward: 7541.45 | backward_inner: 7465.03 | backward_allreduce: 76.33 | step: 6249.19\n",
      "[2022-06-29 01:44:11,118] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6228.12\n",
      "[2022-06-29 01:44:11,119] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2617.38 | backward_microstep: 7546.46 | backward_inner_microstep: 7470.23 | backward_allreduce_microstep: 76.12 | step_microstep: 6272.51\n",
      "[2022-06-29 01:44:11,119] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2617.39 | backward: 7546.46 | backward_inner: 7470.24 | backward_allreduce: 76.14 | step: 6272.52\n",
      "[2022-06-29 01:44:27,620] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6297.29\n",
      "[2022-06-29 01:44:27,621] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2630.18 | backward_microstep: 7525.93 | backward_inner_microstep: 7449.48 | backward_allreduce_microstep: 76.35 | step_microstep: 6340.60\n",
      "[2022-06-29 01:44:27,621] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2630.19 | backward: 7525.93 | backward_inner: 7449.49 | backward_allreduce: 76.36 | step: 6340.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 01:44:44,081] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6243.18\n",
      "[2022-06-29 01:44:44,082] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2622.09 | backward_microstep: 7546.32 | backward_inner_microstep: 7465.72 | backward_allreduce_microstep: 80.50 | step_microstep: 6288.35\n",
      "[2022-06-29 01:44:44,082] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2622.11 | backward: 7546.32 | backward_inner: 7465.73 | backward_allreduce: 80.51 | step: 6288.35\n",
      "[2022-06-29 01:45:00,563] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6287.69\n",
      "[2022-06-29 01:45:00,564] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2617.51 | backward_microstep: 7527.39 | backward_inner_microstep: 7450.77 | backward_allreduce_microstep: 76.51 | step_microstep: 6332.27\n",
      "[2022-06-29 01:45:00,564] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2617.54 | backward: 7527.39 | backward_inner: 7450.78 | backward_allreduce: 76.53 | step: 6332.27\n",
      "[2022-06-29 01:45:17,003] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6236.78\n",
      "[2022-06-29 01:45:17,004] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2630.47 | backward_microstep: 7525.01 | backward_inner_microstep: 7447.80 | backward_allreduce_microstep: 77.04 | step_microstep: 6280.10\n",
      "[2022-06-29 01:45:17,004] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2630.48 | backward: 7525.01 | backward_inner: 7447.84 | backward_allreduce: 77.09 | step: 6280.10\n",
      "[2022-06-29 01:45:33,421] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6214.29\n",
      "[2022-06-29 01:45:33,423] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2612.70 | backward_microstep: 7543.76 | backward_inner_microstep: 7466.50 | backward_allreduce_microstep: 77.11 | step_microstep: 6257.78\n",
      "[2022-06-29 01:45:33,423] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2612.73 | backward: 7543.76 | backward_inner: 7466.53 | backward_allreduce: 77.13 | step: 6257.78\n",
      "[2022-06-29 01:45:49,842] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6232.87\n",
      "[2022-06-29 01:45:49,843] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.4062499999999999e-06, 1.4062499999999999e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:45:49,843] [INFO] [timer.py:181:stop] 0/30, SamplesPerSec=0.24322728044503683\n",
      "[2022-06-29 01:45:49,843] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2608.55 | backward_microstep: 7529.48 | backward_inner_microstep: 7453.18 | backward_allreduce_microstep: 76.18 | step_microstep: 6278.14\n",
      "[2022-06-29 01:45:49,843] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2608.59 | backward: 7529.48 | backward_inner: 7453.20 | backward_allreduce: 76.20 | step: 6278.15\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 16442.7 | learning rate: 1.406E-06 | lm loss: 1.108386E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 2624.70 | backward: 7531.52 | backward-backward: 7531.49 | backward-allreduce: 0.00 | optimizer: 6285.25 | batch generator: 1.31\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 2.522 B\n",
      "[2022-06-29 01:46:06,299] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6256.39\n",
      "[2022-06-29 01:46:06,300] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2633.11 | backward_microstep: 7515.65 | backward_inner_microstep: 7437.18 | backward_allreduce_microstep: 78.37 | step_microstep: 6301.14\n",
      "[2022-06-29 01:46:06,300] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2633.11 | backward: 7515.65 | backward_inner: 7437.19 | backward_allreduce: 78.39 | step: 6301.14\n",
      "[2022-06-29 01:46:22,686] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6225.19\n",
      "[2022-06-29 01:46:22,687] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2610.26 | backward_microstep: 7503.12 | backward_inner_microstep: 7426.86 | backward_allreduce_microstep: 76.11 | step_microstep: 6268.65\n",
      "[2022-06-29 01:46:22,687] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2610.28 | backward: 7503.12 | backward_inner: 7426.88 | backward_allreduce: 76.15 | step: 6268.66\n",
      "[2022-06-29 01:46:39,124] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6245.54\n",
      "[2022-06-29 01:46:39,125] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2606.45 | backward_microstep: 7537.71 | backward_inner_microstep: 7461.60 | backward_allreduce_microstep: 76.01 | step_microstep: 6289.19\n",
      "[2022-06-29 01:46:39,125] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2606.46 | backward: 7537.71 | backward_inner: 7461.61 | backward_allreduce: 76.02 | step: 6289.21\n",
      "[2022-06-29 01:46:55,584] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6253.22\n",
      "[2022-06-29 01:46:55,585] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2616.51 | backward_microstep: 7541.90 | backward_inner_microstep: 7465.37 | backward_allreduce_microstep: 76.43 | step_microstep: 6297.15\n",
      "[2022-06-29 01:46:55,585] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2616.52 | backward: 7541.90 | backward_inner: 7465.38 | backward_allreduce: 76.44 | step: 6297.16\n",
      "[2022-06-29 01:47:12,023] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6264.99\n",
      "[2022-06-29 01:47:12,025] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2609.24 | backward_microstep: 7515.72 | backward_inner_microstep: 7438.41 | backward_allreduce_microstep: 77.19 | step_microstep: 6308.65\n",
      "[2022-06-29 01:47:12,025] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2609.25 | backward: 7515.72 | backward_inner: 7438.42 | backward_allreduce: 77.21 | step: 6308.65\n",
      "[2022-06-29 01:47:28,488] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6256.13\n",
      "[2022-06-29 01:47:28,489] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2614.08 | backward_microstep: 7545.17 | backward_inner_microstep: 7468.57 | backward_allreduce_microstep: 76.48 | step_microstep: 6300.78\n",
      "[2022-06-29 01:47:28,489] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2614.10 | backward: 7545.17 | backward_inner: 7468.58 | backward_allreduce: 76.51 | step: 6300.78\n",
      "[2022-06-29 01:47:44,942] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6232.80\n",
      "[2022-06-29 01:47:44,943] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2634.90 | backward_microstep: 7536.55 | backward_inner_microstep: 7459.31 | backward_allreduce_microstep: 77.09 | step_microstep: 6277.69\n",
      "[2022-06-29 01:47:44,943] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2634.94 | backward: 7536.55 | backward_inner: 7459.32 | backward_allreduce: 77.14 | step: 6277.70\n",
      "[2022-06-29 01:48:01,361] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6220.00\n",
      "[2022-06-29 01:48:01,362] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2611.42 | backward_microstep: 7538.98 | backward_inner_microstep: 7462.16 | backward_allreduce_microstep: 76.70 | step_microstep: 6264.00\n",
      "[2022-06-29 01:48:01,362] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2611.45 | backward: 7538.98 | backward_inner: 7462.18 | backward_allreduce: 76.72 | step: 6264.00\n",
      "[2022-06-29 01:48:17,800] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6225.11\n",
      "[2022-06-29 01:48:17,801] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2630.61 | backward_microstep: 7533.07 | backward_inner_microstep: 7456.54 | backward_allreduce_microstep: 76.38 | step_microstep: 6269.75\n",
      "[2022-06-29 01:48:17,801] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2630.62 | backward: 7533.07 | backward_inner: 7456.57 | backward_allreduce: 76.41 | step: 6269.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 01:48:34,283] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6235.07\n",
      "[2022-06-29 01:48:34,284] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.8749999999999998e-06, 1.8749999999999998e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:48:34,284] [INFO] [timer.py:181:stop] 0/40, SamplesPerSec=0.2432504830307154\n",
      "[2022-06-29 01:48:34,284] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2640.63 | backward_microstep: 7558.83 | backward_inner_microstep: 7482.02 | backward_allreduce_microstep: 76.66 | step_microstep: 6279.17\n",
      "[2022-06-29 01:48:34,284] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2640.67 | backward: 7558.83 | backward_inner: 7482.05 | backward_allreduce: 76.70 | step: 6279.18\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 16444.1 | learning rate: 1.875E-06 | lm loss: 1.085358E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 2624.13 | backward: 7532.77 | backward-backward: 7532.73 | backward-allreduce: 0.00 | optimizer: 6285.89 | batch generator: 1.43\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 2.522 B\n",
      "[2022-06-29 01:48:50,698] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6227.85\n",
      "[2022-06-29 01:48:50,699] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2602.10 | backward_microstep: 7533.62 | backward_inner_microstep: 7460.63 | backward_allreduce_microstep: 72.83 | step_microstep: 6271.20\n",
      "[2022-06-29 01:48:50,699] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2602.13 | backward: 7533.62 | backward_inner: 7460.66 | backward_allreduce: 72.86 | step: 6271.20\n",
      "[2022-06-29 01:49:07,151] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6248.42\n",
      "[2022-06-29 01:49:07,152] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2602.43 | backward_microstep: 7553.41 | backward_inner_microstep: 7476.98 | backward_allreduce_microstep: 76.32 | step_microstep: 6292.63\n",
      "[2022-06-29 01:49:07,152] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2602.44 | backward: 7553.41 | backward_inner: 7476.99 | backward_allreduce: 76.34 | step: 6292.64\n",
      "[2022-06-29 01:49:23,635] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6280.56\n",
      "[2022-06-29 01:49:23,636] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2629.93 | backward_microstep: 7524.43 | backward_inner_microstep: 7447.77 | backward_allreduce_microstep: 76.55 | step_microstep: 6324.25\n",
      "[2022-06-29 01:49:23,636] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2629.95 | backward: 7524.43 | backward_inner: 7447.79 | backward_allreduce: 76.57 | step: 6324.25\n",
      "[2022-06-29 01:49:40,057] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6234.14\n",
      "[2022-06-29 01:49:40,058] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2602.07 | backward_microstep: 7536.21 | backward_inner_microstep: 7458.99 | backward_allreduce_microstep: 77.10 | step_microstep: 6278.82\n",
      "[2022-06-29 01:49:40,058] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2602.13 | backward: 7536.20 | backward_inner: 7458.99 | backward_allreduce: 77.12 | step: 6278.83\n",
      "[2022-06-29 01:49:56,501] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6239.99\n",
      "[2022-06-29 01:49:56,502] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2611.85 | backward_microstep: 7542.95 | backward_inner_microstep: 7466.25 | backward_allreduce_microstep: 76.55 | step_microstep: 6284.23\n",
      "[2022-06-29 01:49:56,502] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2611.87 | backward: 7542.95 | backward_inner: 7466.27 | backward_allreduce: 76.59 | step: 6284.24\n",
      "[2022-06-29 01:50:12,986] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6262.97\n",
      "[2022-06-29 01:50:12,987] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2624.32 | backward_microstep: 7548.97 | backward_inner_microstep: 7471.04 | backward_allreduce_microstep: 77.78 | step_microstep: 6306.88\n",
      "[2022-06-29 01:50:12,987] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2624.34 | backward: 7548.97 | backward_inner: 7471.05 | backward_allreduce: 77.83 | step: 6306.88\n",
      "[2022-06-29 01:50:29,411] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6238.58\n",
      "[2022-06-29 01:50:29,412] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2616.52 | backward_microstep: 7521.12 | backward_inner_microstep: 7444.30 | backward_allreduce_microstep: 76.72 | step_microstep: 6283.13\n",
      "[2022-06-29 01:50:29,413] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2616.54 | backward: 7521.12 | backward_inner: 7444.31 | backward_allreduce: 76.73 | step: 6283.14\n",
      "[2022-06-29 01:50:45,847] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6225.36\n",
      "[2022-06-29 01:50:45,849] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2611.94 | backward_microstep: 7550.35 | backward_inner_microstep: 7473.72 | backward_allreduce_microstep: 76.53 | step_microstep: 6268.71\n",
      "[2022-06-29 01:50:45,849] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2611.95 | backward: 7550.35 | backward_inner: 7473.73 | backward_allreduce: 76.55 | step: 6268.72\n",
      "[2022-06-29 01:51:02,263] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6218.77\n",
      "[2022-06-29 01:51:02,264] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2621.93 | backward_microstep: 7526.22 | backward_inner_microstep: 7449.94 | backward_allreduce_microstep: 76.18 | step_microstep: 6262.79\n",
      "[2022-06-29 01:51:02,264] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2621.94 | backward: 7526.22 | backward_inner: 7449.95 | backward_allreduce: 76.19 | step: 6262.80\n",
      "[2022-06-29 01:51:18,707] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6241.98\n",
      "[2022-06-29 01:51:18,708] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.34375e-06, 2.34375e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 01:51:18,708] [INFO] [timer.py:181:stop] 0/50, SamplesPerSec=0.24326923550408885\n",
      "[2022-06-29 01:51:18,708] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2610.30 | backward_microstep: 7544.25 | backward_inner_microstep: 7467.22 | backward_allreduce_microstep: 76.90 | step_microstep: 6285.12\n",
      "[2022-06-29 01:51:18,708] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2610.35 | backward: 7544.25 | backward_inner: 7467.23 | backward_allreduce: 76.94 | step: 6285.13\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 16442.4 | learning rate: 2.344E-06 | lm loss: 1.051464E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 2616.88 | backward: 7538.25 | backward-backward: 7538.22 | backward-allreduce: 0.00 | optimizer: 6286.04 | batch generator: 1.48\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 2.522 B\n",
      "rank: 0 | time: 2022-06-29 01:51:18 | exiting the program at iteration 50\n",
      "[2022-06-29 01:51:27,977] [INFO] [launch.py:159:main] Process 36134 exits successfully.\n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 100 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-100_hs-2048_bs-4_ws-4_2022-06-29.1656467489.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 100 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 4 --gl-ray-max-concurrency 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 4\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 100\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 4\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 100\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.167 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.579 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.706 seconds\n",
      "time to initialize megatron (seconds): 3.794\n",
      "[after megatron is initialized] datetime: 2022-06-29 01:51:36 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 5140951040\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             5.141 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.24\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 01:52:56 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000565 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 01:52:57 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 79991.12 | train/valid/test-data-iterators-setup: 620.98\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 01:52:57 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 28800.1 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 9.930238E+00 | loss scale: 1.0 | grad norm: 3.452 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 5.85 and total parameters 5.141 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 28.80014133453369;  SamplesPerSecond: 0.13888820730208284\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 6408.6064453125 | max allocated: 12178.0068359375 | reserved: 14174.0 | max reserved: 14174.0\n",
      "time (ms) | e2e-time: 28800.52 | forward-compute: 4160.81 | backward-compute: 24628.53 | backward-embedding-all-reduce: 0.02 | optimizer: 2.10 | batch-generator: 1.82 | offloading-func-call-overhead: 8427.97 | offloading-fwd-overhead: 2777.92 | offloading-bwd-overhead: 15692.56 | offloading-fwd-2gpu-overhead: 670.45 | offloading-fwd-2cpu-overhead: 2105.47 | offloading-bwd-2gpu-overhead: 376.59 | offloading-bwd-2cpu-overhead: 15313.37\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 25356.5 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 9.047261E+00 | loss scale: 1.0 | grad norm: 1.702 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.64 and total parameters 5.141 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 25.35652871131897;  SamplesPerSecond: 0.15775029955951458\n",
      "time (ms) | e2e-time: 25356.43 | forward-compute: 3744.86 | backward-compute: 21600.67 | backward-embedding-all-reduce: 0.02 | optimizer: 2.04 | batch-generator: 1.31 | offloading-func-call-overhead: 51.60 | offloading-fwd-overhead: 3408.10 | offloading-bwd-overhead: 19094.15 | offloading-fwd-2gpu-overhead: 958.09 | offloading-fwd-2cpu-overhead: 2448.09 | offloading-bwd-2gpu-overhead: 402.36 | offloading-bwd-2cpu-overhead: 18689.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 25268.9 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.315873E+00 | loss scale: 1.0 | grad norm: 397.991 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.67 and total parameters 5.141 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 25.268943524360658;  SamplesPerSecond: 0.1582970810055347\n",
      "time (ms) | e2e-time: 25269.06 | forward-compute: 3714.81 | backward-compute: 21543.24 | backward-embedding-all-reduce: 0.02 | optimizer: 2.02 | batch-generator: 1.38 | offloading-func-call-overhead: 51.58 | offloading-fwd-overhead: 3387.11 | offloading-bwd-overhead: 18918.47 | offloading-fwd-2gpu-overhead: 721.87 | offloading-fwd-2cpu-overhead: 2663.27 | offloading-bwd-2gpu-overhead: 589.25 | offloading-bwd-2cpu-overhead: 18326.52\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 25335.5 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.076915E+00 | loss scale: 1.0 | grad norm: 1.597 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.65 and total parameters 5.141 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 25.335460710525513;  SamplesPerSecond: 0.15788147867933644\n",
      "time (ms) | e2e-time: 25335.36 | forward-compute: 3820.71 | backward-compute: 21503.72 | backward-embedding-all-reduce: 0.02 | optimizer: 2.03 | batch-generator: 1.36 | offloading-func-call-overhead: 52.04 | offloading-fwd-overhead: 3488.40 | offloading-bwd-overhead: 19142.74 | offloading-fwd-2gpu-overhead: 974.14 | offloading-fwd-2cpu-overhead: 2512.29 | offloading-bwd-2gpu-overhead: 574.81 | offloading-bwd-2cpu-overhead: 18565.17\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 25212.9 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 8.798536E+00 | loss scale: 1.0 | grad norm: 1.699 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.68 and total parameters 5.141 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 25.212936091423035;  SamplesPerSecond: 0.15864871847911138\n",
      "time (ms) | e2e-time: 25212.93 | forward-compute: 3751.07 | backward-compute: 21450.96 | backward-embedding-all-reduce: 0.02 | optimizer: 1.99 | batch-generator: 1.30 | offloading-func-call-overhead: 51.51 | offloading-fwd-overhead: 3428.21 | offloading-bwd-overhead: 18517.90 | offloading-fwd-2gpu-overhead: 712.30 | offloading-fwd-2cpu-overhead: 2713.96 | offloading-bwd-2gpu-overhead: 430.29 | offloading-bwd-2cpu-overhead: 18084.92\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 02:14:36 \n",
      "[after training is done] datetime: 2022-06-29 02:14:36 \n",
      "/home/sys/STRONGHOLD\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "./examples/run.sh -m \"megatron-lm\" -l 32 -h 2048 && \\\n",
    "./examples/run.sh -m \"l2l\" -l 78 -h 2048 && \\\n",
    "./examples/run.sh -m \"zero-offload\" -l 48 -h 2048 && \\\n",
    "./examples/run.sh -m \"zero-infinity\" -l 48 -h 2048 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 100 -h 2048 && \\\n",
    "\\\n",
    "pyenv deactivate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:  grep -R 'total parameters' ./results/log_l2l_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656470788.txt ./results/log_l2l_l-78_hs-2048_bs-4_ws-4_2022-06-29.1656464452.txt ./results/log_megatron-lm_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656464109.txt ./results/log_stronghold_l-100_hs-2048_bs-4_ws-4_2022-06-29.1656467489.txt ./results/log_stronghold_l-16_hs-2048_bs-4_ws-15_2022-06-29.1656476653.txt ./results/log_stronghold_l-24_hs-2048_bs-4_ws-15_2022-06-29.1656476410.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-10_2022-06-29.1656478517.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-12_2022-06-29.1656478904.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-14_2022-06-29.1656479286.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-15_2022-06-29.1656468888.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-2_2022-06-29.1656476815.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656477260.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-6_2022-06-29.1656477696.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-8_2022-06-29.1656478111.txt ./results/log_stronghold_l-40_hs-2048_bs-4_ws-15_2022-06-29.1656475948.txt ./results/log_stronghold_l-48_hs-2048_bs-4_ws-15_2022-06-29.1656469235.txt ./results/log_stronghold_l-56_hs-2048_bs-4_ws-15_2022-06-29.1656475269.txt ./results/log_stronghold_l-64_hs-2048_bs-4_ws-15_2022-06-29.1656474476.txt ./results/log_stronghold_l-78_hs-2048_bs-4_ws-15_2022-06-29.1656469802.txt ./results/log_stronghold_l-92_hs-2048_bs-4_ws-15_2022-06-29.1656473293.txt ./results/log_zero-infinity_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472684.txt ./results/log_zero-infinity_l-48_hs-2048_bs-4_ws-4_2022-06-29.1656466598.txt ./results/log_zero-offload_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472088.txt ./results/log_zero-offload_l-48_hs-2048_bs-4_ws-4_2022-06-29.1656465726.txt | awk -v FS='[/,_: ]' '{print $4, $18, $19, $20, $21}' | uniq \r\n",
      "\r\n",
      "l2l total parameters 1.717 B\r\n",
      "l2l total parameters 4.033 B\r\n",
      "megatron-lm total parameters 1.717 B\r\n",
      "stronghold total parameters 5.141 B\r\n",
      "stronghold total parameters 0.911 B\r\n",
      "stronghold total parameters 1.314 B\r\n",
      "stronghold total parameters 1.717 B\r\n",
      "stronghold total parameters 2.119 B\r\n",
      "stronghold total parameters 2.522 B\r\n",
      "stronghold total parameters 2.925 B\r\n",
      "stronghold total parameters 3.328 B\r\n",
      "stronghold total parameters 4.033 B\r\n",
      "stronghold total parameters 4.738 B\r\n",
      "zero-infinity total parameters 1.717 B\r\n",
      "zero-infinity total parameters 2.522 B\r\n",
      "zero-offload total parameters 1.717 B\r\n",
      "zero-offload total parameters 2.522 B\r\n"
     ]
    }
   ],
   "source": [
    "# To print the relevant information from log files\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "./examples/case1.sh && \\\n",
    "\\\n",
    "pyenv deactivate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CASE - Throughput  on the largest trainable model size supported by each baseline (Figure 7a in Section VI.B)\n",
    "\n",
    "In this case, we use GPT-like models to exploit the largest trainable model size supported by each baseline and compare the performance against STRONGHOLD on each largest model size. Model size changes via increasing/decreasing the number of transformer layers.\n",
    "\n",
    "Here, we evaluate (Megatron-LM, L2L, ZeRO-Offload, ZeRO-Infinity) v.s. STRONGHOLD on a virtual machine with one 32GB V100, 90GB CPU RAM and 12 CPU Cores. During this process, we configure the `Heads=16, Sequence Length=1024, Batch Size=4` in all GPT-like models and training setups.\n",
    "\n",
    "The throughput has been tested in this notebook, shown in the following table. Please run the next cells to reproduce it. Thanks.\n",
    "\n",
    "| Methods | Throughput | Trainable Size | Layers | Hidden Size | Heads | Sequence Length | Batch Size |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| Megatron-LM | **0.7496** |1.717 B | 32 | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **0.6647** | 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "|\n",
    "| L2L | **0.0529** | 4.033 B| 78 | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **0.2271** | 4.033 B| 78 | 2048 | 16 | 1024 | 4 |\n",
    "|\n",
    "| ZeRO-Offload | **0.2523** |2.522 B | 48 | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **0.3999**| 2.522 B| 48 | 2048 | 16 | 1024 | 4 |\n",
    "|\n",
    "| ZeRO-Infinity | **0.2439** | 2.522 B| 48 | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **0.3999**| 2.522 B| 48 | 2048 | 16 | 1024 | 4 |\n",
    "\n",
    "PS: Limitations of CPU cores and bandwidth in the virtual machine hurts the performance of STRONGHOLD a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.0  0.0   2520    72 pts/0    Ss+  Jun28   0:00 sleep infinit\r\n",
      "root           7  0.0  0.0   4348   696 pts/1    Ss+  Jun28   0:00 /bin/bash\r\n",
      "root         159  0.0  0.0   4348   804 pts/2    Ss+  Jun28   0:00 /bin/bash\r\n",
      "root       35813  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35825  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35855  0.0  0.0      0     0 ?        Z    01:01   0:01 [python] <def\r\n",
      "root       35856  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35864  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35865  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       36874  0.0  0.0   3976  3200 pts/3    Ss+  02:14   0:00 /bin/bash -c \r\n",
      "root       37032  0.0  0.0   5892  2964 pts/3    R+   02:14   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "# To check if there exists other running processes launched by other reviwers in case of GPU overlead.\n",
    "# Just run it and no need to change anything in this cell.\n",
    "#\n",
    "# `ps aux` in docker container. \n",
    "######\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c 'export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && export PYENV_ROOT=\"/root/.pyenv\" && export PATH=\"$PYENV_ROOT/bin:$PATH\" && eval \"$(pyenv init -)\" && eval \"$(pyenv virtualenv-init -)\" && pyenv activate py3.9.10 && ps aux && pyenv deactivate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following results correspond to Figure 7a in the submitted paper. Please refers to Section VI.B on page 9 for more details. Run around 40 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-15_2022-06-29.1656468888.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.156 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.520 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.696 seconds\n",
      "time to initialize megatron (seconds): 3.704\n",
      "[after megatron is initialized] datetime: 2022-06-29 02:14:56 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.21\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 02:15:24 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000591 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 02:15:25 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 28883.37 | train/valid/test-data-iterators-setup: 425.83\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 02:15:25 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 6596.2 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.113258E+01 | loss scale: 1.0 | grad norm: 9820.539 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.53 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.596226096153259;  SamplesPerSecond: 0.6064073519755019\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 23609.392578125 | reserved: 29180.0 | max reserved: 29180.0\n",
      "time (ms) | e2e-time: 6596.25 | forward-compute: 859.93 | backward-compute: 5725.35 | backward-embedding-all-reduce: 0.02 | optimizer: 2.50 | batch-generator: 1.81 | offloading-func-call-overhead: 1511.85 | offloading-fwd-overhead: 568.51 | offloading-bwd-overhead: 2.23 | offloading-fwd-2gpu-overhead: 262.24 | offloading-fwd-2cpu-overhead: 305.66 | offloading-bwd-2gpu-overhead: 0.76 | offloading-bwd-2cpu-overhead: 0.56\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 5956.9 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.067282E+01 | loss scale: 1.0 | grad norm: 476491.962 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.44 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.956890559196472;  SamplesPerSecond: 0.671491268850768\n",
      "time (ms) | e2e-time: 5956.85 | forward-compute: 860.70 | backward-compute: 5085.41 | backward-embedding-all-reduce: 0.02 | optimizer: 2.45 | batch-generator: 1.28 | offloading-func-call-overhead: 14.32 | offloading-fwd-overhead: 758.84 | offloading-bwd-overhead: 2.27 | offloading-fwd-2gpu-overhead: 357.56 | offloading-fwd-2cpu-overhead: 400.57 | offloading-bwd-2gpu-overhead: 0.79 | offloading-bwd-2cpu-overhead: 0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 6051.8 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.052111E+01 | loss scale: 1.0 | grad norm: 12.352 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.29 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.051781535148621;  SamplesPerSecond: 0.6609623921101718\n",
      "time (ms) | e2e-time: 6051.73 | forward-compute: 852.50 | backward-compute: 5188.55 | backward-embedding-all-reduce: 0.02 | optimizer: 2.45 | batch-generator: 1.21 | offloading-func-call-overhead: 14.16 | offloading-fwd-overhead: 751.53 | offloading-bwd-overhead: 61.74 | offloading-fwd-2gpu-overhead: 348.26 | offloading-fwd-2cpu-overhead: 402.66 | offloading-bwd-2gpu-overhead: 0.76 | offloading-bwd-2cpu-overhead: 60.09\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 6017.3 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.036228E+01 | loss scale: 1.0 | grad norm: 8.427 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.35 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.01729519367218;  SamplesPerSecond: 0.6647505018876956\n",
      "time (ms) | e2e-time: 6017.29 | forward-compute: 833.95 | backward-compute: 5172.68 | backward-embedding-all-reduce: 0.02 | optimizer: 2.43 | batch-generator: 1.21 | offloading-func-call-overhead: 14.05 | offloading-fwd-overhead: 730.69 | offloading-bwd-overhead: 2.47 | offloading-fwd-2gpu-overhead: 321.13 | offloading-fwd-2cpu-overhead: 408.85 | offloading-bwd-2gpu-overhead: 0.77 | offloading-bwd-2cpu-overhead: 0.70\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 5976.5 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.016025E+01 | loss scale: 1.0 | grad norm: 23.103 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.41 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.976530981063843;  SamplesPerSecond: 0.6692845753956063\n",
      "time (ms) | e2e-time: 5976.54 | forward-compute: 855.41 | backward-compute: 5110.46 | backward-embedding-all-reduce: 0.02 | optimizer: 2.45 | batch-generator: 1.18 | offloading-func-call-overhead: 15.12 | offloading-fwd-overhead: 740.56 | offloading-bwd-overhead: 2.53 | offloading-fwd-2gpu-overhead: 355.34 | offloading-fwd-2cpu-overhead: 384.59 | offloading-bwd-2gpu-overhead: 0.84 | offloading-bwd-2cpu-overhead: 0.81\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 02:20:31 \n",
      "[after training is done] datetime: 2022-06-29 02:20:31 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 48 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-48_hs-2048_bs-4_ws-15_2022-06-29.1656469235.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 48 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 48\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 48\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.589 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.688 seconds\n",
      "time to initialize megatron (seconds): 3.768\n",
      "[after megatron is initialized] datetime: 2022-06-29 02:20:42 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2522320896\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             2.522 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.24\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 02:21:23 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000625 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 02:21:24 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 41415.44 | train/valid/test-data-iterators-setup: 482.03\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 02:21:24 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 11323.9 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.066761E+01 | loss scale: 1.0 | grad norm: 40.265 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.3 and total parameters 2.522 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 11.323884534835816;  SamplesPerSecond: 0.3532356752397772\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 24169.408203125 | reserved: 30396.0 | max reserved: 30396.0\n",
      "time (ms) | e2e-time: 11324.02 | forward-compute: 1445.53 | backward-compute: 9867.42 | backward-embedding-all-reduce: 0.02 | optimizer: 2.40 | batch-generator: 1.88 | offloading-func-call-overhead: 2905.87 | offloading-fwd-overhead: 925.33 | offloading-bwd-overhead: 3.72 | offloading-fwd-2gpu-overhead: 421.73 | offloading-fwd-2cpu-overhead: 502.70 | offloading-bwd-2gpu-overhead: 1.51 | offloading-bwd-2cpu-overhead: 0.81\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 10005.5 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.034449E+01 | loss scale: 1.0 | grad norm: 85303.633 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.26 and total parameters 2.522 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 10.005548882484437;  SamplesPerSecond: 0.3997781677927075\n",
      "time (ms) | e2e-time: 10005.47 | forward-compute: 1393.72 | backward-compute: 8601.01 | backward-embedding-all-reduce: 0.02 | optimizer: 2.34 | batch-generator: 1.25 | offloading-func-call-overhead: 26.27 | offloading-fwd-overhead: 1242.61 | offloading-bwd-overhead: 3.73 | offloading-fwd-2gpu-overhead: 542.33 | offloading-fwd-2cpu-overhead: 699.37 | offloading-bwd-2gpu-overhead: 1.60 | offloading-bwd-2cpu-overhead: 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 10053.0 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.951887E+00 | loss scale: 1.0 | grad norm: 29079963.862 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.22 and total parameters 2.522 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 10.053036069869995;  SamplesPerSecond: 0.3978897491463718\n",
      "time (ms) | e2e-time: 10053.03 | forward-compute: 1360.82 | backward-compute: 8681.45 | backward-embedding-all-reduce: 0.02 | optimizer: 2.33 | batch-generator: 1.31 | offloading-func-call-overhead: 23.37 | offloading-fwd-overhead: 1198.38 | offloading-bwd-overhead: 3.80 | offloading-fwd-2gpu-overhead: 533.43 | offloading-fwd-2cpu-overhead: 663.93 | offloading-bwd-2gpu-overhead: 1.71 | offloading-bwd-2cpu-overhead: 0.74\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 9953.1 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.822738E+00 | loss scale: 1.0 | grad norm: 3.506 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.3 and total parameters 2.522 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 9.953149557113647;  SamplesPerSecond: 0.40188283889908466\n",
      "time (ms) | e2e-time: 9953.16 | forward-compute: 1308.92 | backward-compute: 8633.45 | backward-embedding-all-reduce: 0.02 | optimizer: 2.35 | batch-generator: 1.27 | offloading-func-call-overhead: 34.77 | offloading-fwd-overhead: 1141.10 | offloading-bwd-overhead: 3.64 | offloading-fwd-2gpu-overhead: 475.71 | offloading-fwd-2cpu-overhead: 664.38 | offloading-bwd-2gpu-overhead: 1.59 | offloading-bwd-2cpu-overhead: 0.73\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 9984.5 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.967432E+00 | loss scale: 1.0 | grad norm: 34.961 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.28 and total parameters 2.522 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 9.984450507164002;  SamplesPerSecond: 0.4006229483665562\n",
      "time (ms) | e2e-time: 9984.44 | forward-compute: 1378.41 | backward-compute: 8595.26 | backward-embedding-all-reduce: 0.02 | optimizer: 2.36 | batch-generator: 1.25 | offloading-func-call-overhead: 24.65 | offloading-fwd-overhead: 1220.56 | offloading-bwd-overhead: 3.71 | offloading-fwd-2gpu-overhead: 536.64 | offloading-fwd-2cpu-overhead: 683.01 | offloading-bwd-2gpu-overhead: 1.65 | offloading-bwd-2cpu-overhead: 0.75\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 02:29:57 \n",
      "[after training is done] datetime: 2022-06-29 02:29:57 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 78 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-78_hs-2048_bs-4_ws-15_2022-06-29.1656469802.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 78 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 78\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 78\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.155 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.567 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.703 seconds\n",
      "time to initialize megatron (seconds): 3.758\n",
      "[after megatron is initialized] datetime: 2022-06-29 02:30:09 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 4033069056\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             4.033 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.23\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 02:31:14 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000579 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 02:31:14 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 64409.64 | train/valid/test-data-iterators-setup: 526.43\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 02:31:14 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 20163.3 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.069659E+01 | loss scale: 1.0 | grad norm: 13363.388 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.55 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 20.163342356681824;  SamplesPerSecond: 0.19837980872622843\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 25017.369140625 | reserved: 30668.0 | max reserved: 30668.0\n",
      "time (ms) | e2e-time: 20163.47 | forward-compute: 2853.22 | backward-compute: 17299.03 | backward-embedding-all-reduce: 0.02 | optimizer: 2.19 | batch-generator: 1.91 | offloading-func-call-overhead: 5814.76 | offloading-fwd-overhead: 1921.84 | offloading-bwd-overhead: 308.58 | offloading-fwd-2gpu-overhead: 926.48 | offloading-fwd-2cpu-overhead: 993.33 | offloading-bwd-2gpu-overhead: 3.38 | offloading-bwd-2cpu-overhead: 303.01\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 17553.8 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.058776E+01 | loss scale: 1.0 | grad norm: 9748.476 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.53 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 17.553775310516357;  SamplesPerSecond: 0.22787120885634357\n",
      "time (ms) | e2e-time: 17553.70 | forward-compute: 2554.77 | backward-compute: 14988.01 | backward-embedding-all-reduce: 0.02 | optimizer: 2.13 | batch-generator: 1.34 | offloading-func-call-overhead: 43.19 | offloading-fwd-overhead: 2287.75 | offloading-bwd-overhead: 278.20 | offloading-fwd-2gpu-overhead: 1121.71 | offloading-fwd-2cpu-overhead: 1164.49 | offloading-bwd-2gpu-overhead: 3.23 | offloading-bwd-2cpu-overhead: 272.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 17616.2 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.012751E+01 | loss scale: 1.0 | grad norm: 521.329 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.5 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 17.616170144081117;  SamplesPerSecond: 0.22706411026257975\n",
      "time (ms) | e2e-time: 17616.19 | forward-compute: 2594.27 | backward-compute: 15010.93 | backward-embedding-all-reduce: 0.02 | optimizer: 2.17 | batch-generator: 1.41 | offloading-func-call-overhead: 41.88 | offloading-fwd-overhead: 2329.38 | offloading-bwd-overhead: 221.90 | offloading-fwd-2gpu-overhead: 1134.03 | offloading-fwd-2cpu-overhead: 1193.82 | offloading-bwd-2gpu-overhead: 3.16 | offloading-bwd-2cpu-overhead: 216.40\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 17583.2 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.801062E+00 | loss scale: 1.0 | grad norm: 6.727 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.52 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 17.583243680000304;  SamplesPerSecond: 0.2274893115739343\n",
      "time (ms) | e2e-time: 17583.24 | forward-compute: 2685.79 | backward-compute: 14886.54 | backward-embedding-all-reduce: 0.02 | optimizer: 2.12 | batch-generator: 1.37 | offloading-func-call-overhead: 43.55 | offloading-fwd-overhead: 2310.85 | offloading-bwd-overhead: 7.27 | offloading-fwd-2gpu-overhead: 1090.61 | offloading-fwd-2cpu-overhead: 1218.72 | offloading-bwd-2gpu-overhead: 3.66 | offloading-bwd-2cpu-overhead: 1.23\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 17482.1 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.754141E+00 | loss scale: 1.0 | grad norm: 428.518 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.56 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 17.482054281234742;  SamplesPerSecond: 0.2288060622425595\n",
      "time (ms) | e2e-time: 17482.05 | forward-compute: 2498.12 | backward-compute: 14973.01 | backward-embedding-all-reduce: 0.02 | optimizer: 2.15 | batch-generator: 1.41 | offloading-func-call-overhead: 41.83 | offloading-fwd-overhead: 2233.45 | offloading-bwd-overhead: 6.54 | offloading-fwd-2gpu-overhead: 1109.10 | offloading-fwd-2cpu-overhead: 1122.86 | offloading-bwd-2gpu-overhead: 3.10 | offloading-bwd-2cpu-overhead: 1.30\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 02:46:18 \n",
      "[after training is done] datetime: 2022-06-29 02:46:18 \n",
      "/home/sys/STRONGHOLD\n"
     ]
    }
   ],
   "source": [
    "# Code Here\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 15 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 48 -h 2048 -w 15 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 78 -h 2048 -w 15 && \\\n",
    "\\\n",
    "pyenv deactivate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:  grep -R 'SamplesPerSec' ./results/log_zero-infinity_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472684.txt ./results/log_zero-infinity_l-48_hs-2048_bs-4_ws-4_2022-06-29.1656466598.txt ./results/log_zero-offload_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472088.txt ./results/log_zero-offload_l-48_hs-2048_bs-4_ws-4_2022-06-29.1656465726.txt | awk -v FS='[/,_: ]' '{print $5, $6, $4, $22}' | sort \r\n",
      "\r\n",
      "Running:  grep -R 'SamplesPerSec' ./results/log_l2l_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656470788.txt ./results/log_l2l_l-78_hs-2048_bs-4_ws-4_2022-06-29.1656464452.txt ./results/log_megatron-lm_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656464109.txt ./results/log_stronghold_l-100_hs-2048_bs-4_ws-4_2022-06-29.1656467489.txt ./results/log_stronghold_l-16_hs-2048_bs-4_ws-15_2022-06-29.1656476653.txt ./results/log_stronghold_l-24_hs-2048_bs-4_ws-15_2022-06-29.1656476410.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-10_2022-06-29.1656478517.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-12_2022-06-29.1656478904.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-14_2022-06-29.1656479286.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-15_2022-06-29.1656468888.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-2_2022-06-29.1656476815.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656477260.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-6_2022-06-29.1656477696.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-8_2022-06-29.1656478111.txt ./results/log_stronghold_l-40_hs-2048_bs-4_ws-15_2022-06-29.1656475948.txt ./results/log_stronghold_l-48_hs-2048_bs-4_ws-15_2022-06-29.1656469235.txt ./results/log_stronghold_l-56_hs-2048_bs-4_ws-15_2022-06-29.1656475269.txt ./results/log_stronghold_l-64_hs-2048_bs-4_ws-15_2022-06-29.1656474476.txt ./results/log_stronghold_l-78_hs-2048_bs-4_ws-15_2022-06-29.1656469802.txt ./results/log_stronghold_l-92_hs-2048_bs-4_ws-15_2022-06-29.1656473293.txt ./results/log_zero-infinity_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472684.txt ./results/log_zero-infinity_l-48_hs-2048_bs-4_ws-4_2022-06-29.1656466598.txt ./results/log_zero-offload_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472088.txt ./results/log_zero-offload_l-48_hs-2048_bs-4_ws-4_2022-06-29.1656465726.txt --exclude=*zero*.txt | awk -v FS='[/,_: ]' '{print $5, $6, $4, $20, $22}' | sort \r\n",
      "\r\n",
      "l-32 hs-2048 zero-infinity SamplesPerSec=0.3580743926343268\r\n",
      "l-32 hs-2048 zero-infinity SamplesPerSec=0.35807442201378215\r\n",
      "l-32 hs-2048 zero-infinity SamplesPerSec=0.3581642600833874\r\n",
      "l-32 hs-2048 zero-infinity SamplesPerSec=0.3581780523838733\r\n",
      "l-32 hs-2048 zero-infinity SamplesPerSec=0.35832065821292713\r\n",
      "l-32 hs-2048 zero-offload SamplesPerSec=0.37059720779517674\r\n",
      "l-32 hs-2048 zero-offload SamplesPerSec=0.3707088317743382\r\n",
      "l-32 hs-2048 zero-offload SamplesPerSec=0.370806853608525\r\n",
      "l-32 hs-2048 zero-offload SamplesPerSec=0.3708752180795792\r\n",
      "l-32 hs-2048 zero-offload SamplesPerSec=0.3709171697153578\r\n",
      "l-48 hs-2048 zero-infinity SamplesPerSec=0.24312370111631262\r\n",
      "l-48 hs-2048 zero-infinity SamplesPerSec=0.2431688385299387\r\n",
      "l-48 hs-2048 zero-infinity SamplesPerSec=0.24322728044503683\r\n",
      "l-48 hs-2048 zero-infinity SamplesPerSec=0.2432504830307154\r\n",
      "l-48 hs-2048 zero-infinity SamplesPerSec=0.24326923550408885\r\n",
      "l-48 hs-2048 zero-offload SamplesPerSec=0.2518778712067455\r\n",
      "l-48 hs-2048 zero-offload SamplesPerSec=0.2519142251140072\r\n",
      "l-48 hs-2048 zero-offload SamplesPerSec=0.25196149273020724\r\n",
      "l-48 hs-2048 zero-offload SamplesPerSec=0.2519909447468096\r\n",
      "l-48 hs-2048 zero-offload SamplesPerSec=0.25199842464335126\r\n",
      "l-100 hs-2048 stronghold SamplesPerSecond 0.13888820730208284\r\n",
      "l-100 hs-2048 stronghold SamplesPerSecond 0.15775029955951458\r\n",
      "l-100 hs-2048 stronghold SamplesPerSecond 0.15788147867933644\r\n",
      "l-100 hs-2048 stronghold SamplesPerSecond 0.1582970810055347\r\n",
      "l-100 hs-2048 stronghold SamplesPerSecond 0.15864871847911138\r\n",
      "l-16 hs-2048 stronghold SamplesPerSecond 1.4648562358202202\r\n",
      "l-16 hs-2048 stronghold SamplesPerSecond 1.4670263896960234\r\n",
      "l-16 hs-2048 stronghold SamplesPerSecond 1.46945501998462\r\n",
      "l-16 hs-2048 stronghold SamplesPerSecond 1.5029747399308013\r\n",
      "l-16 hs-2048 stronghold SamplesPerSecond 1.5222936660577842\r\n",
      "l-24 hs-2048 stronghold SamplesPerSecond 0.8908816371798837\r\n",
      "l-24 hs-2048 stronghold SamplesPerSecond 0.9545345302474427\r\n",
      "l-24 hs-2048 stronghold SamplesPerSecond 0.9700793720206667\r\n",
      "l-24 hs-2048 stronghold SamplesPerSecond 0.9701678078409888\r\n",
      "l-24 hs-2048 stronghold SamplesPerSecond 0.9831243317139151\r\n",
      "l-32 hs-2048 l2l SamplesPerSecond 0.13781014511285522\r\n",
      "l-32 hs-2048 l2l SamplesPerSecond 0.16228004126551213\r\n",
      "l-32 hs-2048 l2l SamplesPerSecond 0.1747173025930287\r\n",
      "l-32 hs-2048 l2l SamplesPerSecond 0.17803514419011424\r\n",
      "l-32 hs-2048 l2l SamplesPerSecond 0.17974635511317175\r\n",
      "l-32 hs-2048 megatron-lm SamplesPerSecond 0.7438263111268488\r\n",
      "l-32 hs-2048 megatron-lm SamplesPerSecond 0.7496099192261813\r\n",
      "l-32 hs-2048 megatron-lm SamplesPerSecond 0.7497347875823174\r\n",
      "l-32 hs-2048 megatron-lm SamplesPerSecond 0.7498405609294753\r\n",
      "l-32 hs-2048 megatron-lm SamplesPerSecond 0.7501883830820277\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.4443891688213538\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.4588837381719193\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.48206704653991833\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.4936834767174005\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.500718459212588\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5040417547536666\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5064041904239727\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5084029606168415\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5089427664506396\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5160017424807263\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5166440240183751\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5196351929775191\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5271565503740522\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5272408353652496\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.538802376275119\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5487643996098076\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5491873261544319\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5501055754982426\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5526876802820975\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5580439624480265\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5634126642091529\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5635024715766087\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5671335866691215\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5806314246860206\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5863023174163046\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5892956651298789\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5912245060047543\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5967951704471945\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6064073519755019\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6064338524896208\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6075235571405322\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6122348693502793\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.613892038809362\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6320721247511137\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6423642682670394\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6464636949129531\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6609623921101718\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6647505018876956\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6692845753956063\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.671491268850768\r\n",
      "l-40 hs-2048 stronghold SamplesPerSecond 0.4429790476776485\r\n",
      "l-40 hs-2048 stronghold SamplesPerSecond 0.48148481404193566\r\n",
      "l-40 hs-2048 stronghold SamplesPerSecond 0.4892740162965899\r\n",
      "l-40 hs-2048 stronghold SamplesPerSecond 0.49801725518371565\r\n",
      "l-40 hs-2048 stronghold SamplesPerSecond 0.5051487048674246\r\n",
      "l-48 hs-2048 stronghold SamplesPerSecond 0.3532356752397772\r\n",
      "l-48 hs-2048 stronghold SamplesPerSecond 0.3978897491463718\r\n",
      "l-48 hs-2048 stronghold SamplesPerSecond 0.3997781677927075\r\n",
      "l-48 hs-2048 stronghold SamplesPerSecond 0.4006229483665562\r\n",
      "l-48 hs-2048 stronghold SamplesPerSecond 0.40188283889908466\r\n",
      "l-56 hs-2048 stronghold SamplesPerSecond 0.29534723574275695\r\n",
      "l-56 hs-2048 stronghold SamplesPerSecond 0.33032718658867705\r\n",
      "l-56 hs-2048 stronghold SamplesPerSecond 0.33109231369133457\r\n",
      "l-56 hs-2048 stronghold SamplesPerSecond 0.3327932222630368\r\n",
      "l-56 hs-2048 stronghold SamplesPerSecond 0.33354825317947145\r\n",
      "l-64 hs-2048 stronghold SamplesPerSecond 0.2506517215682859\r\n",
      "l-64 hs-2048 stronghold SamplesPerSecond 0.27965468656548914\r\n",
      "l-64 hs-2048 stronghold SamplesPerSecond 0.28331722784202074\r\n",
      "l-64 hs-2048 stronghold SamplesPerSecond 0.284315574422947\r\n",
      "l-64 hs-2048 stronghold SamplesPerSecond 0.284621761701911\r\n",
      "l-78 hs-2048 l2l SamplesPerSecond 0.05859796875161271\r\n",
      "l-78 hs-2048 stronghold SamplesPerSecond 0.19837980872622843\r\n",
      "l-78 hs-2048 stronghold SamplesPerSecond 0.22706411026257975\r\n",
      "l-78 hs-2048 stronghold SamplesPerSecond 0.2274893115739343\r\n",
      "l-78 hs-2048 stronghold SamplesPerSecond 0.22787120885634357\r\n",
      "l-78 hs-2048 stronghold SamplesPerSecond 0.2288060622425595\r\n",
      "l-92 hs-2048 stronghold SamplesPerSecond 0.1666225348244212\r\n",
      "l-92 hs-2048 stronghold SamplesPerSecond 0.18734349671838763\r\n",
      "l-92 hs-2048 stronghold SamplesPerSecond 0.1878752986699505\r\n",
      "l-92 hs-2048 stronghold SamplesPerSecond 0.1880838836639666\r\n",
      "l-92 hs-2048 stronghold SamplesPerSecond 0.18858165832387072\r\n"
     ]
    }
   ],
   "source": [
    "# To print the relevant information from log files\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "./examples/case2.sh && \\\n",
    "\\\n",
    "pyenv deactivate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 CASE - Throughput on the largest trainable model size of Megatron-LM (Figure 8a in Section VI.B)\n",
    "\n",
    "This case shows the throughput performance of running Megatron-LM, L2L, ZeRO-Offload, ZeRO-Infinity and STRONGHOLD, respectively, on a 1.717 B model that is the largest trainable model size supported by Megatron-LM. The evaluation is conducted on a virtual machine with one 32GB V100, 90GB CPU RAM and 12 CPU Cores.\n",
    "\n",
    "The throughput results have been tested in this notebook, shown in the following table. Please run the following cells to reproduce it. Thanks.\n",
    "\n",
    "| Methods | Throughput | Trainable Size | Layers | Hidden Size | Heads | Sequence Length | Batch Size |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| Megatron-LM | **0.7496** | 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "| L2L | **0.1729**| 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "| ZeRO-Offload | **0.3711**| 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "| ZeRO-Infinity | **0.3587** | 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **0.6647** | 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "\n",
    "PS: Limitations of CPU cores and bandwidth in the virtual machine hurts the performance of STRONGHOLD a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.0  0.0   2520    72 pts/0    Ss+  Jun28   0:00 sleep infinit\r\n",
      "root           7  0.0  0.0   4348   696 pts/1    Ss+  Jun28   0:00 /bin/bash\r\n",
      "root         159  0.0  0.0   4348   804 pts/2    Ss+  Jun28   0:00 /bin/bash\r\n",
      "root       35813  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35825  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35855  0.0  0.0      0     0 ?        Z    01:01   0:01 [python] <def\r\n",
      "root       35856  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35864  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35865  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       38697  0.0  0.0   3976  3200 pts/3    Ss+  02:46   0:00 /bin/bash -c \r\n",
      "root       38855  0.0  0.0   5892  2836 pts/3    R+   02:46   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "# To check if there exists other running processes launched by other reviwers in case of GPU overlead.\n",
    "# Just run it and no need to change anything in this cell.\n",
    "#\n",
    "# `ps aux` in docker container. \n",
    "######\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c 'export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && export PYENV_ROOT=\"/root/.pyenv\" && export PATH=\"$PYENV_ROOT/bin:$PATH\" && eval \"$(pyenv init -)\" && eval \"$(pyenv virtualenv-init -)\" && pyenv activate py3.9.10 && ps aux && pyenv deactivate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following results correspond to Figure 8a in the submitted paper. Please refers to Section VI.B on page 9 for more details. Run around 45 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/sys/STRONGHOLD/examples/../L2L-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../L2L-Megatron-LM/examples/sc22-gpt-l2l.sh 32 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_l2l_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656470788.txt && cd -\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_l2l ...................................... True\n",
      "  enbale_strongh .................................. None\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ checkpoints/gpt2_345m_ds\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ checkpoints/gpt2_345m_ds\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.159 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.562 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module ds_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module ds_cpu_adam...\n",
      ">>> done with compiling and loading strongh utils. Compilation time: 0.554 seconds\n",
      "time to initialize megatron (seconds): 3.524\n",
      "[after megatron is initialized] datetime: 2022-06-29 02:46:35 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_345m_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.25\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 02:46:42 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000581 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 02:46:43 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 7440.01 | train/valid/test-data-iterators-setup: 731.33\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 02:46:43 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 29025.4 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.120928E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 1.94 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 29.025439286231993;  SamplesPerSecond: 0.13781014511285522\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 7142.48583984375 | max allocated: 10785.58935546875 | reserved: 15412.0 | max reserved: 15412.0\n",
      "time (ms) | forward-compute: 6972.37 | backward-compute: 16346.58 | backward-params-all-reduce: 16.85 | backward-embedding-all-reduce: 0.04 | optimizer: 5572.92 | batch-generator: 1.98\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 24648.7 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.120716E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 2.28 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 24.64874897003174;  SamplesPerSecond: 0.16228004126551213\n",
      "time (ms) | forward-compute: 5050.80 | backward-compute: 15976.19 | backward-params-all-reduce: 16.83 | backward-embedding-all-reduce: 0.04 | optimizer: 3550.74 | batch-generator: 1.37\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 22894.1 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.119732E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 2.46 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 22.894126343727113;  SamplesPerSecond: 0.1747173025930287\n",
      "time (ms) | forward-compute: 4371.26 | backward-compute: 15624.52 | backward-params-all-reduce: 16.83 | backward-embedding-all-reduce: 0.04 | optimizer: 2857.43 | batch-generator: 1.45\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 22253.6 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.121551E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 2.53 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 22.253580594062804;  SamplesPerSecond: 0.17974635511317175\n",
      "time (ms) | forward-compute: 4057.43 | backward-compute: 15604.67 | backward-params-all-reduce: 16.83 | backward-embedding-all-reduce: 0.04 | optimizer: 2563.47 | batch-generator: 1.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       50/      50 | elapsed time per iteration (ms): 22467.5 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.121118E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 2.5 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 22.467474150657655;  SamplesPerSecond: 0.17803514419011424\n",
      "time (ms) | forward-compute: 4155.95 | backward-compute: 15576.49 | backward-params-all-reduce: 16.84 | backward-embedding-all-reduce: 0.04 | optimizer: 2700.05 | batch-generator: 1.44\n",
      "saving checkpoint at iteration      50 to checkpoints/gpt2_345m_ds\n",
      "  successfully saved checkpoint at iteration      50 to checkpoints/gpt2_345m_ds\n",
      "time (ms) | save-checkpoint: 69348.96\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 03:08:05 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/sc22-gpt-zero-offloading.sh 32 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_zero-offload_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472088.txt && cd -\n",
      "deepspeed --num_nodes 1 --num_gpus 1 pretrain_gpt2.py --model-parallel-size 1 --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json --zero-stage 2 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-06-29 03:08:09,628] [WARNING] [runner.py:132:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2022-06-29 03:08:12,126] [INFO] [runner.py:398:main] cmd = /root/.pyenv/versions/3.9.10/envs/py3.9.10/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 pretrain_gpt2.py --model-parallel-size 1 --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json --zero-stage 2 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-06-29 03:08:12,992] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda11.4\n",
      "[2022-06-29 03:08:12,992] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-06-29 03:08:12,992] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1\n",
      "[2022-06-29 03:08:12,992] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2022-06-29 03:08:12,992] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda11.4\n",
      "[2022-06-29 03:08:12,993] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2022-06-29 03:08:12,993] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-06-29 03:08:12,993] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2022-06-29 03:08:12,993] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2022-06-29 03:08:12,993] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2022-06-29 03:08:12,993] [INFO] [launch.py:100:main] dist_world_size=1\n",
      "[2022-06-29 03:08:12,993] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... 4\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... True\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ True\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... mmap\n",
      "  data_path ....................... /home/sys/STRONGHOLD/data/my-gpt2-en_text_document\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... True\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 1000\n",
      "  exit_interval ................... 50\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 2048\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ checkpoints/gpt2_ds\n",
      "  local_rank ...................... 0\n",
      "  log_interval .................... 10\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. 0.00015\n",
      "  lr_decay_iters .................. 320000\n",
      "  lr_decay_style .................. cosine\n",
      "  lr_decay_tokens ................. None\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 1024\n",
      "  memory_centric_tiled_linear ..... False\n",
      "  merge_file ...................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  min_lr .......................... 1e-05\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 16\n",
      "  num_layers ...................... 32\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... True\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  remote_device ................... none\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ checkpoints/gpt2_ds\n",
      "  save_interval ................... 10000\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  scattered_embeddings ............ False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... 1024\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 949,50,1\n",
      "  split_transformers .............. False\n",
      "  synchronize_each_layer .......... True\n",
      "  tensorboard_dir ................. None\n",
      "  tile_factor ..................... 1\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  tokens .......................... 0\n",
      "  train_iters ..................... 50\n",
      "  train_tokens .................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  use_pin_memory .................. False\n",
      "  vocab_file ...................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  warmup .......................... 0.01\n",
      "  warmup_iters .................... None\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 5000000000\n",
      "  zero_contigious_gradients ....... True\n",
      "  zero_reduce_bucket_size ......... 50000000\n",
      "  zero_reduce_scatter ............. True\n",
      "  zero_stage ...................... 2\n",
      "---------------- end of arguments ----------------\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "building GPT2 model ...\n",
      "[2022-06-29 03:08:16,589] [INFO] [utils.py:822:see_memory_usage] Before Building Model\n",
      "[2022-06-29 03:08:16,590] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2022-06-29 03:08:16,590] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.2 GB, percent = 3.5%\n",
      "[2022-06-29 03:08:16,590] [WARNING] [partition_parameters.py:457:__init__] zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7febec5d41f0>\n",
      "train_batch = 4, micro_batch=None\n",
      "[2022-06-29 03:08:16,591] [WARNING] [config.py:1100:_do_warning_check] DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM (1.0) > 0, setting to zero\n",
      "[2022-06-29 03:08:16,729] [INFO] [utils.py:822:see_memory_usage] After Building Model\n",
      "[2022-06-29 03:08:16,730] [INFO] [utils.py:823:see_memory_usage] MA 6.39 GB         Max_MA 6.39 GB         CA 6.39 GB         Max_CA 6 GB \n",
      "[2022-06-29 03:08:16,731] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.2 GB, percent = 3.5%\n",
      " > number of parameters on model parallel rank 0            1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "DeepSpeed is enabled.\n",
      "[2022-06-29 03:08:16,734] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.7, git-hash=unknown, git-branch=unknown\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7febec5d41f0>\n",
      "train_batch = 4, micro_batch=None\n",
      "[2022-06-29 03:08:16,737] [WARNING] [config.py:1100:_do_warning_check] DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM (1.0) > 0, setting to zero\n",
      "[2022-06-29 03:08:16,748] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups using mpu\n",
      "[2022-06-29 03:08:16,748] [INFO] [logging.py:69:log_dist] [Rank 0] Initializing deepspeed groups with model parallel size 1, expert parallel size 1, and data parallel size 1\n",
      "[2022-06-29 03:08:16,748] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
      "[2022-06-29 03:08:16,749] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
      "[2022-06-29 03:08:16,809] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "[2022-06-29 03:08:16,810] [INFO] [engine.py:1084:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2022-06-29 03:08:16,810] [INFO] [engine.py:1090:_configure_optimizer] Using client Optimizer as basic optimizer\n",
      "[2022-06-29 03:08:16,834] [INFO] [engine.py:1106:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW\n",
      "[2022-06-29 03:08:16,835] [INFO] [utils.py:43:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2022-06-29 03:08:16,835] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
      "[2022-06-29 03:08:16,835] [INFO] [stage2.py:113:__init__] Reduce bucket size 50000000\n",
      "[2022-06-29 03:08:16,835] [INFO] [stage2.py:114:__init__] Allgather bucket size 50000000\n",
      "[2022-06-29 03:08:16,835] [INFO] [stage2.py:115:__init__] CPU Offload: True\n",
      "[2022-06-29 03:08:16,835] [INFO] [stage2.py:116:__init__] Round robin gradient partitioning: False\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu114/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.2525520324707031 seconds\n",
      "Rank: 0 partition count [1, 1] and sizes[(1715732480, False), (856064, False)] \n",
      "[2022-06-29 03:08:34,299] [INFO] [utils.py:822:see_memory_usage] Before initializing optimizer states\n",
      "[2022-06-29 03:08:34,300] [INFO] [utils.py:823:see_memory_usage] MA 6.78 GB         Max_MA 6.78 GB         CA 12.8 GB         Max_CA 13 GB \n",
      "[2022-06-29 03:08:34,300] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 10.59 GB, percent = 11.7%\n",
      "[2022-06-29 03:08:44,972] [INFO] [utils.py:822:see_memory_usage] After initializing optimizer states\n",
      "[2022-06-29 03:08:44,973] [INFO] [utils.py:823:see_memory_usage] MA 6.78 GB         Max_MA 6.78 GB         CA 12.8 GB         Max_CA 13 GB \n",
      "[2022-06-29 03:08:44,974] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 29.91 GB, percent = 33.1%\n",
      "[2022-06-29 03:08:44,974] [INFO] [stage2.py:483:__init__] optimizer state initialized\n",
      "[2022-06-29 03:08:45,005] [INFO] [utils.py:822:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2022-06-29 03:08:45,005] [INFO] [utils.py:823:see_memory_usage] MA 6.78 GB         Max_MA 6.78 GB         CA 12.8 GB         Max_CA 13 GB \n",
      "[2022-06-29 03:08:45,006] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 29.91 GB, percent = 33.1%\n",
      "[2022-06-29 03:08:45,006] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2022-06-29 03:08:45,006] [INFO] [engine.py:798:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2022-06-29 03:08:45,006] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7febec2a1f40>\n",
      "[2022-06-29 03:08:45,006] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:08:45,007] [INFO] [config.py:1044:print] DeepSpeedEngine configuration:\n",
      "[2022-06-29 03:08:45,007] [INFO] [config.py:1048:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2022-06-29 03:08:45,007] [INFO] [config.py:1048:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2022-06-29 03:08:45,007] [INFO] [config.py:1048:print]   allreduce_always_fp32 ........ False\n",
      "[2022-06-29 03:08:45,007] [INFO] [config.py:1048:print]   amp_enabled .................. False\n",
      "[2022-06-29 03:08:45,007] [INFO] [config.py:1048:print]   amp_params ................... False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   bfloat16_enabled ............. False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   checkpoint_tag_validation_enabled  True\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   checkpoint_tag_validation_fail  False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   curriculum_enabled ........... False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   curriculum_params ............ False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   dataloader_drop_last ......... False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   disable_allgather ............ False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   dump_state ................... False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   dynamic_loss_scale_args ...... None\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   eigenvalue_enabled ........... False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   eigenvalue_layer_num ......... 0\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   eigenvalue_max_iter .......... 100\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   eigenvalue_stability ......... 1e-06\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   eigenvalue_tol ............... 0.01\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   eigenvalue_verbose ........... False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   elasticity_enabled ........... False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   fp16_enabled ................. False\n",
      "[2022-06-29 03:08:45,008] [INFO] [config.py:1048:print]   fp16_master_weights_and_gradients  False\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   fp16_mixed_quantize .......... False\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   global_rank .................. 0\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   gradient_accumulation_steps .. 1\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   gradient_clipping ............ 1.0\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   gradient_predivide_factor .... 1.0\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   loss_scale ................... 0\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   memory_breakdown ............. False\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   optimizer_legacy_fusion ...... False\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   optimizer_name ............... adam\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   optimizer_params ............. {'lr': 0.00015, 'max_grad_norm': 0.0, 'betas': [0.9, 0.95]}\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   pld_enabled .................. False\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   pld_params ................... False\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   prescale_gradients ........... False\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   quantize_change_rate ......... 0.001\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   quantize_groups .............. 1\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   quantize_offset .............. 1000\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   quantize_period .............. 1000\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   quantize_rounding ............ 0\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   quantize_start_bits .......... 16\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   quantize_target_bits ......... 8\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   quantize_training_enabled .... False\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   quantize_type ................ 0\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   quantize_verbose ............. False\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   scheduler_name ............... None\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   scheduler_params ............. None\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   sparse_attention ............. None\n",
      "[2022-06-29 03:08:45,009] [INFO] [config.py:1048:print]   sparse_gradients_enabled ..... False\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   steps_per_print .............. 10\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   tensorboard_enabled .......... False\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   tensorboard_output_path ...... \n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   train_batch_size ............. 4\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   train_micro_batch_size_per_gpu  4\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   use_quantizer_kernel ......... False\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   wall_clock_breakdown ......... True\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   world_size ................... 1\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   zero_allow_untested_optimizer  False\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   zero_config .................. {\n",
      "    \"stage\": 2, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+07, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+07, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": true, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": {\n",
      "        \"device\": null, \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 4, \n",
      "        \"pin_memory\": false, \n",
      "        \"pipeline_read\": false, \n",
      "        \"pipeline_write\": false, \n",
      "        \"fast_init\": false\n",
      "    }, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_fp16_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   zero_enabled ................. True\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1048:print]   zero_optimization_stage ...... 2\n",
      "[2022-06-29 03:08:45,010] [INFO] [config.py:1050:print]   json = {\n",
      "    \"train_batch_size\": 4, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+07, \n",
      "        \"reduce_bucket_size\": 5.000000e+07, \n",
      "        \"cpu_offload\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.00015, \n",
      "            \"max_grad_norm\": 0.0, \n",
      "            \"betas\": [0.9, 0.95]\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": true, \n",
      "    \"zero_allow_untested_optimizer\": false\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00047516822814941406 seconds\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT2 ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000364 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT2 datasets ...\n",
      "setting training data start iteration to 0\n",
      "setting validation data start iteration to 0\n",
      "done with setups ...\n",
      "time (ms) | model and optimizer: 28467.48 | train/valid/test data iterators: 1413.49\n",
      "training ...\n",
      "[2022-06-29 03:09:09,013] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.85 | optimizer_gradients: 206.17 | optimizer_step: 15958.96\n",
      "[2022-06-29 03:09:09,014] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2082.50 | backward_microstep: 4288.41 | backward_inner_microstep: 4248.66 | backward_allreduce_microstep: 39.63 | step_microstep: 16206.06\n",
      "[2022-06-29 03:09:09,014] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2082.57 | backward: 4288.41 | backward_inner: 4248.67 | backward_allreduce: 39.64 | step: 16206.06\n",
      "[2022-06-29 03:09:19,774] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.79 | optimizer_gradients: 184.65 | optimizer_step: 5058.44\n",
      "[2022-06-29 03:09:19,775] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1186.53 | backward_microstep: 4287.17 | backward_inner_microstep: 4248.01 | backward_allreduce_microstep: 39.07 | step_microstep: 5283.31\n",
      "[2022-06-29 03:09:19,775] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1186.63 | backward: 4287.17 | backward_inner: 4248.02 | backward_allreduce: 39.07 | step: 5283.32\n",
      "[2022-06-29 03:09:30,545] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 182.94 | optimizer_step: 5054.34\n",
      "[2022-06-29 03:09:30,545] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1187.33 | backward_microstep: 4303.02 | backward_inner_microstep: 4264.01 | backward_allreduce_microstep: 38.92 | step_microstep: 5277.42\n",
      "[2022-06-29 03:09:30,545] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1187.43 | backward: 4303.02 | backward_inner: 4264.02 | backward_allreduce: 38.93 | step: 5277.42\n",
      "[2022-06-29 03:09:41,312] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.78 | optimizer_gradients: 182.44 | optimizer_step: 5053.11\n",
      "[2022-06-29 03:09:41,312] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1194.37 | backward_microstep: 4293.38 | backward_inner_microstep: 4254.11 | backward_allreduce_microstep: 39.17 | step_microstep: 5275.85\n",
      "[2022-06-29 03:09:41,312] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1194.44 | backward: 4293.38 | backward_inner: 4254.12 | backward_allreduce: 39.18 | step: 5275.85\n",
      "[2022-06-29 03:09:52,102] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 181.33 | optimizer_step: 5069.43\n",
      "[2022-06-29 03:09:52,103] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1195.06 | backward_microstep: 4301.45 | backward_inner_microstep: 4262.42 | backward_allreduce_microstep: 38.94 | step_microstep: 5291.04\n",
      "[2022-06-29 03:09:52,103] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1195.14 | backward: 4301.45 | backward_inner: 4262.43 | backward_allreduce: 38.94 | step: 5291.04\n",
      "[2022-06-29 03:10:02,891] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 181.94 | optimizer_step: 5057.88\n",
      "[2022-06-29 03:10:02,892] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.11 | backward_microstep: 4312.58 | backward_inner_microstep: 4273.49 | backward_allreduce_microstep: 39.01 | step_microstep: 5280.09\n",
      "[2022-06-29 03:10:02,892] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.17 | backward: 4312.59 | backward_inner: 4273.49 | backward_allreduce: 39.01 | step: 5280.09\n",
      "[2022-06-29 03:10:13,690] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.78 | optimizer_gradients: 201.86 | optimizer_step: 5057.28\n",
      "[2022-06-29 03:10:13,690] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.48 | backward_microstep: 4303.18 | backward_inner_microstep: 4264.03 | backward_allreduce_microstep: 39.06 | step_microstep: 5299.37\n",
      "[2022-06-29 03:10:13,690] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.55 | backward: 4303.18 | backward_inner: 4264.04 | backward_allreduce: 39.06 | step: 5299.37\n",
      "[2022-06-29 03:10:24,510] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.78 | optimizer_gradients: 185.42 | optimizer_step: 5080.01\n",
      "[2022-06-29 03:10:24,510] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.25 | backward_microstep: 4318.12 | backward_inner_microstep: 4279.08 | backward_allreduce_microstep: 38.95 | step_microstep: 5305.77\n",
      "[2022-06-29 03:10:24,510] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.33 | backward: 4318.12 | backward_inner: 4279.09 | backward_allreduce: 38.95 | step: 5305.77\n",
      "[2022-06-29 03:10:35,345] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 193.21 | optimizer_step: 5090.46\n",
      "[2022-06-29 03:10:35,345] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.07 | backward_microstep: 4314.87 | backward_inner_microstep: 4275.72 | backward_allreduce_microstep: 39.06 | step_microstep: 5324.02\n",
      "[2022-06-29 03:10:35,346] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.17 | backward: 4314.87 | backward_inner: 4275.72 | backward_allreduce: 39.06 | step: 5324.02\n",
      "[2022-06-29 03:10:46,139] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 186.50 | optimizer_step: 5063.32\n",
      "[2022-06-29 03:10:46,139] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.6874999999999996e-07, 4.6874999999999996e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:10:46,140] [INFO] [timer.py:181:stop] 0/10, SamplesPerSec=0.37059720779517674\n",
      "[2022-06-29 03:10:46,140] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.74 | backward_microstep: 4307.13 | backward_inner_microstep: 4267.79 | backward_allreduce_microstep: 39.25 | step_microstep: 5290.42\n",
      "[2022-06-29 03:10:46,140] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.83 | backward: 4307.13 | backward_inner: 4267.80 | backward_allreduce: 39.25 | step: 5290.42\n",
      " iteration       10/      50 | elapsed time per iteration (ms): 11971.2 | learning rate: 4.687E-07 | lm loss: 1.091905E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 10 iterations memory (MB) | allocated: 6943.26611328125 | max allocated: 10359.83984375 | reserved: 20720.0 | max reserved: 20720.0\n",
      "time (ms) | forward: 1284.07 | backward: 4303.04 | backward-backward: 4303.00 | backward-allreduce: 0.00 | optimizer: 6383.62 | batch generator: 1.98\n",
      "Effective Tera Flops per GPU: 0.47 and total parameters 1.717 B\n",
      "[2022-06-29 03:10:56,925] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.78 | optimizer_gradients: 184.14 | optimizer_step: 5058.33\n",
      "[2022-06-29 03:10:56,925] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.52 | backward_microstep: 4306.71 | backward_inner_microstep: 4267.75 | backward_allreduce_microstep: 38.86 | step_microstep: 5282.66\n",
      "[2022-06-29 03:10:56,925] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.60 | backward: 4306.71 | backward_inner: 4267.76 | backward_allreduce: 38.87 | step: 5282.66\n",
      "[2022-06-29 03:11:07,719] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 182.16 | optimizer_step: 5068.54\n",
      "[2022-06-29 03:11:07,719] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.43 | backward_microstep: 4307.82 | backward_inner_microstep: 4268.82 | backward_allreduce_microstep: 38.92 | step_microstep: 5290.89\n",
      "[2022-06-29 03:11:07,719] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.52 | backward: 4307.82 | backward_inner: 4268.83 | backward_allreduce: 38.92 | step: 5290.89\n",
      "[2022-06-29 03:11:18,505] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.78 | optimizer_gradients: 183.93 | optimizer_step: 5069.88\n",
      "[2022-06-29 03:11:18,506] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1189.89 | backward_microstep: 4299.80 | backward_inner_microstep: 4260.86 | backward_allreduce_microstep: 38.85 | step_microstep: 5294.00\n",
      "[2022-06-29 03:11:18,506] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1189.98 | backward: 4299.80 | backward_inner: 4260.86 | backward_allreduce: 38.85 | step: 5294.00\n",
      "[2022-06-29 03:11:29,282] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.79 | optimizer_gradients: 182.61 | optimizer_step: 5063.12\n",
      "[2022-06-29 03:11:29,282] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.04 | backward_microstep: 4294.50 | backward_inner_microstep: 4255.55 | backward_allreduce_microstep: 38.86 | step_microstep: 5285.90\n",
      "[2022-06-29 03:11:29,283] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.11 | backward: 4294.50 | backward_inner: 4255.56 | backward_allreduce: 38.86 | step: 5285.91\n",
      "[2022-06-29 03:11:40,093] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 186.19 | optimizer_step: 5088.52\n",
      "[2022-06-29 03:11:40,093] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1194.31 | backward_microstep: 4298.74 | backward_inner_microstep: 4259.73 | backward_allreduce_microstep: 38.92 | step_microstep: 5314.92\n",
      "[2022-06-29 03:11:40,093] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1194.39 | backward: 4298.74 | backward_inner: 4259.74 | backward_allreduce: 38.92 | step: 5314.92\n",
      "[2022-06-29 03:11:50,891] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 182.04 | optimizer_step: 5081.41\n",
      "[2022-06-29 03:11:50,892] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1195.05 | backward_microstep: 4296.88 | backward_inner_microstep: 4257.91 | backward_allreduce_microstep: 38.87 | step_microstep: 5303.62\n",
      "[2022-06-29 03:11:50,892] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1195.13 | backward: 4296.88 | backward_inner: 4257.92 | backward_allreduce: 38.88 | step: 5303.62\n",
      "[2022-06-29 03:12:01,679] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 182.44 | optimizer_step: 5067.22\n",
      "[2022-06-29 03:12:01,680] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.68 | backward_microstep: 4301.49 | backward_inner_microstep: 4262.43 | backward_allreduce_microstep: 38.97 | step_microstep: 5289.88\n",
      "[2022-06-29 03:12:01,680] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.75 | backward: 4301.49 | backward_inner: 4262.43 | backward_allreduce: 38.98 | step: 5289.88\n",
      "[2022-06-29 03:12:12,483] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 184.26 | optimizer_step: 5076.66\n",
      "[2022-06-29 03:12:12,484] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.36 | backward_microstep: 4309.00 | backward_inner_microstep: 4269.74 | backward_allreduce_microstep: 39.17 | step_microstep: 5301.15\n",
      "[2022-06-29 03:12:12,484] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.42 | backward: 4309.00 | backward_inner: 4269.75 | backward_allreduce: 39.17 | step: 5301.15\n",
      "[2022-06-29 03:12:23,261] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 183.60 | optimizer_step: 5061.21\n",
      "[2022-06-29 03:12:23,262] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1189.69 | backward_microstep: 4300.24 | backward_inner_microstep: 4261.22 | backward_allreduce_microstep: 38.93 | step_microstep: 5284.97\n",
      "[2022-06-29 03:12:23,262] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1189.77 | backward: 4300.24 | backward_inner: 4261.23 | backward_allreduce: 38.93 | step: 5284.97\n",
      "[2022-06-29 03:12:34,038] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 183.97 | optimizer_step: 5052.24\n",
      "[2022-06-29 03:12:34,038] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.374999999999999e-07, 9.374999999999999e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:12:34,038] [INFO] [timer.py:181:stop] 0/20, SamplesPerSec=0.3707088317743382\n",
      "[2022-06-29 03:12:34,038] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.94 | backward_microstep: 4306.10 | backward_inner_microstep: 4266.90 | backward_allreduce_microstep: 39.10 | step_microstep: 5276.58\n",
      "[2022-06-29 03:12:34,038] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.02 | backward: 4306.10 | backward_inner: 4266.91 | backward_allreduce: 39.10 | step: 5276.59\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 10789.9 | learning rate: 9.375E-07 | lm loss: 9.697246E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1194.18 | backward: 4302.23 | backward-backward: 4302.19 | backward-allreduce: 0.00 | optimizer: 5292.73 | batch generator: 1.22\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 1.717 B\n",
      "[2022-06-29 03:12:44,901] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 215.20 | optimizer_step: 5115.27\n",
      "[2022-06-29 03:12:44,902] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.32 | backward_microstep: 4295.67 | backward_inner_microstep: 4256.57 | backward_allreduce_microstep: 39.00 | step_microstep: 5370.63\n",
      "[2022-06-29 03:12:44,902] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.40 | backward: 4295.67 | backward_inner: 4256.58 | backward_allreduce: 39.01 | step: 5370.63\n",
      "[2022-06-29 03:12:55,696] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 182.56 | optimizer_step: 5061.92\n",
      "[2022-06-29 03:12:55,696] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.99 | backward_microstep: 4315.64 | backward_inner_microstep: 4276.49 | backward_allreduce_microstep: 39.06 | step_microstep: 5284.66\n",
      "[2022-06-29 03:12:55,696] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.07 | backward: 4315.64 | backward_inner: 4276.49 | backward_allreduce: 39.07 | step: 5284.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 03:13:06,479] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.80 | optimizer_gradients: 182.84 | optimizer_step: 5065.79\n",
      "[2022-06-29 03:13:06,480] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.35 | backward_microstep: 4300.56 | backward_inner_microstep: 4261.50 | backward_allreduce_microstep: 38.98 | step_microstep: 5288.95\n",
      "[2022-06-29 03:13:06,480] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.42 | backward: 4300.56 | backward_inner: 4261.51 | backward_allreduce: 38.98 | step: 5288.95\n",
      "[2022-06-29 03:13:17,264] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.78 | optimizer_gradients: 185.31 | optimizer_step: 5061.32\n",
      "[2022-06-29 03:13:17,265] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.59 | backward_microstep: 4302.34 | backward_inner_microstep: 4263.34 | backward_allreduce_microstep: 38.91 | step_microstep: 5286.78\n",
      "[2022-06-29 03:13:17,265] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.65 | backward: 4302.34 | backward_inner: 4263.35 | backward_allreduce: 38.91 | step: 5286.78\n",
      "[2022-06-29 03:13:28,031] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 197.16 | optimizer_step: 5028.30\n",
      "[2022-06-29 03:13:28,031] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.85 | backward_microstep: 4307.25 | backward_inner_microstep: 4268.23 | backward_allreduce_microstep: 38.93 | step_microstep: 5265.64\n",
      "[2022-06-29 03:13:28,032] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.95 | backward: 4307.25 | backward_inner: 4268.24 | backward_allreduce: 38.94 | step: 5265.64\n",
      "[2022-06-29 03:13:38,787] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 180.28 | optimizer_step: 5045.18\n",
      "[2022-06-29 03:13:38,788] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.63 | backward_microstep: 4295.25 | backward_inner_microstep: 4256.31 | backward_allreduce_microstep: 38.85 | step_microstep: 5265.65\n",
      "[2022-06-29 03:13:38,788] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.70 | backward: 4295.25 | backward_inner: 4256.32 | backward_allreduce: 38.85 | step: 5265.65\n",
      "[2022-06-29 03:13:49,519] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 183.72 | optimizer_step: 5014.19\n",
      "[2022-06-29 03:13:49,520] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.74 | backward_microstep: 4299.33 | backward_inner_microstep: 4260.34 | backward_allreduce_microstep: 38.90 | step_microstep: 5238.03\n",
      "[2022-06-29 03:13:49,520] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.81 | backward: 4299.33 | backward_inner: 4260.35 | backward_allreduce: 38.90 | step: 5238.04\n",
      "[2022-06-29 03:14:00,296] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 181.25 | optimizer_step: 5050.51\n",
      "[2022-06-29 03:14:00,297] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.22 | backward_microstep: 4309.00 | backward_inner_microstep: 4270.04 | backward_allreduce_microstep: 38.87 | step_microstep: 5272.00\n",
      "[2022-06-29 03:14:00,297] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.29 | backward: 4309.00 | backward_inner: 4270.05 | backward_allreduce: 38.88 | step: 5272.01\n",
      "[2022-06-29 03:14:11,046] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 180.36 | optimizer_step: 5031.48\n",
      "[2022-06-29 03:14:11,047] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.97 | backward_microstep: 4302.86 | backward_inner_microstep: 4263.90 | backward_allreduce_microstep: 38.88 | step_microstep: 5252.08\n",
      "[2022-06-29 03:14:11,047] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.04 | backward: 4302.86 | backward_inner: 4263.90 | backward_allreduce: 38.88 | step: 5252.08\n",
      "[2022-06-29 03:14:21,793] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 182.58 | optimizer_step: 5028.54\n",
      "[2022-06-29 03:14:21,793] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.4062499999999999e-06, 1.4062499999999999e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:14:21,794] [INFO] [timer.py:181:stop] 0/30, SamplesPerSec=0.3709171697153578\n",
      "[2022-06-29 03:14:21,794] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.89 | backward_microstep: 4300.90 | backward_inner_microstep: 4261.93 | backward_allreduce_microstep: 38.87 | step_microstep: 5251.56\n",
      "[2022-06-29 03:14:21,794] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.96 | backward: 4300.90 | backward_inner: 4261.94 | backward_allreduce: 38.88 | step: 5251.56\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 10775.5 | learning rate: 1.406E-06 | lm loss: 9.170977E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1194.02 | backward: 4302.98 | backward-backward: 4302.95 | backward-allreduce: 0.00 | optimizer: 5277.89 | batch generator: 1.32\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 1.717 B\n",
      "[2022-06-29 03:14:32,544] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 181.18 | optimizer_step: 5035.23\n",
      "[2022-06-29 03:14:32,545] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.13 | backward_microstep: 4297.68 | backward_inner_microstep: 4258.62 | backward_allreduce_microstep: 38.98 | step_microstep: 5256.56\n",
      "[2022-06-29 03:14:32,545] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.20 | backward: 4297.68 | backward_inner: 4258.63 | backward_allreduce: 38.98 | step: 5256.56\n",
      "[2022-06-29 03:14:43,276] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 181.11 | optimizer_step: 5022.57\n",
      "[2022-06-29 03:14:43,277] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1189.93 | backward_microstep: 4295.36 | backward_inner_microstep: 4256.38 | backward_allreduce_microstep: 38.89 | step_microstep: 5243.78\n",
      "[2022-06-29 03:14:43,277] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.01 | backward: 4295.36 | backward_inner: 4256.39 | backward_allreduce: 38.89 | step: 5243.78\n",
      "[2022-06-29 03:14:54,104] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.78 | optimizer_gradients: 218.26 | optimizer_step: 5066.97\n",
      "[2022-06-29 03:14:54,105] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1189.83 | backward_microstep: 4309.26 | backward_inner_microstep: 4270.00 | backward_allreduce_microstep: 39.18 | step_microstep: 5325.42\n",
      "[2022-06-29 03:14:54,105] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1189.92 | backward: 4309.26 | backward_inner: 4270.00 | backward_allreduce: 39.18 | step: 5325.42\n",
      "[2022-06-29 03:15:04,881] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 182.74 | optimizer_step: 5048.37\n",
      "[2022-06-29 03:15:04,882] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1189.83 | backward_microstep: 4312.95 | backward_inner_microstep: 4271.41 | backward_allreduce_microstep: 41.46 | step_microstep: 5271.27\n",
      "[2022-06-29 03:15:04,882] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1189.90 | backward: 4312.95 | backward_inner: 4271.41 | backward_allreduce: 41.46 | step: 5271.27\n",
      "[2022-06-29 03:15:15,680] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 184.28 | optimizer_step: 5071.45\n",
      "[2022-06-29 03:15:15,680] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.08 | backward_microstep: 4307.38 | backward_inner_microstep: 4268.18 | backward_allreduce_microstep: 39.10 | step_microstep: 5296.00\n",
      "[2022-06-29 03:15:15,680] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.15 | backward: 4307.38 | backward_inner: 4268.20 | backward_allreduce: 39.10 | step: 5296.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 03:15:26,453] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 182.79 | optimizer_step: 5040.84\n",
      "[2022-06-29 03:15:26,453] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.67 | backward_microstep: 4313.33 | backward_inner_microstep: 4274.22 | backward_allreduce_microstep: 39.02 | step_microstep: 5263.86\n",
      "[2022-06-29 03:15:26,453] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.75 | backward: 4313.33 | backward_inner: 4274.23 | backward_allreduce: 39.02 | step: 5263.86\n",
      "[2022-06-29 03:15:37,267] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 183.32 | optimizer_step: 5078.17\n",
      "[2022-06-29 03:15:37,268] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.95 | backward_microstep: 4316.16 | backward_inner_microstep: 4277.07 | backward_allreduce_microstep: 39.01 | step_microstep: 5302.03\n",
      "[2022-06-29 03:15:37,268] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.03 | backward: 4316.17 | backward_inner: 4277.08 | backward_allreduce: 39.01 | step: 5302.03\n",
      "[2022-06-29 03:15:48,177] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 181.80 | optimizer_step: 5181.36\n",
      "[2022-06-29 03:15:48,177] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.60 | backward_microstep: 4310.27 | backward_inner_microstep: 4271.22 | backward_allreduce_microstep: 38.97 | step_microstep: 5403.33\n",
      "[2022-06-29 03:15:48,177] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.67 | backward: 4310.27 | backward_inner: 4271.22 | backward_allreduce: 38.97 | step: 5403.33\n",
      "[2022-06-29 03:15:58,938] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 181.85 | optimizer_step: 5036.11\n",
      "[2022-06-29 03:15:58,938] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.51 | backward_microstep: 4307.33 | backward_inner_microstep: 4267.94 | backward_allreduce_microstep: 39.30 | step_microstep: 5258.21\n",
      "[2022-06-29 03:15:58,938] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.59 | backward: 4307.33 | backward_inner: 4267.94 | backward_allreduce: 39.31 | step: 5258.21\n",
      "[2022-06-29 03:16:09,704] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 182.00 | optimizer_step: 5045.89\n",
      "[2022-06-29 03:16:09,705] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.8749999999999998e-06, 1.8749999999999998e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:16:09,705] [INFO] [timer.py:181:stop] 0/40, SamplesPerSec=0.3708752180795792\n",
      "[2022-06-29 03:16:09,705] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.52 | backward_microstep: 4302.37 | backward_inner_microstep: 4263.35 | backward_allreduce_microstep: 38.93 | step_microstep: 5268.26\n",
      "[2022-06-29 03:16:09,705] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.59 | backward: 4302.37 | backward_inner: 4263.36 | backward_allreduce: 38.94 | step: 5268.26\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 10791.2 | learning rate: 1.875E-06 | lm loss: 8.940726E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1194.04 | backward: 4307.31 | backward-backward: 4307.28 | backward-allreduce: 0.00 | optimizer: 5289.15 | batch generator: 1.38\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 1.717 B\n",
      "[2022-06-29 03:16:20,463] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.78 | optimizer_gradients: 184.85 | optimizer_step: 5030.96\n",
      "[2022-06-29 03:16:20,464] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.38 | backward_microstep: 4305.37 | backward_inner_microstep: 4266.36 | backward_allreduce_microstep: 38.92 | step_microstep: 5256.09\n",
      "[2022-06-29 03:16:20,464] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.45 | backward: 4305.37 | backward_inner: 4266.37 | backward_allreduce: 38.93 | step: 5256.10\n",
      "[2022-06-29 03:16:31,229] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 183.52 | optimizer_step: 5034.71\n",
      "[2022-06-29 03:16:31,230] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.70 | backward_microstep: 4313.08 | backward_inner_microstep: 4273.79 | backward_allreduce_microstep: 39.18 | step_microstep: 5258.36\n",
      "[2022-06-29 03:16:31,230] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.77 | backward: 4313.08 | backward_inner: 4273.81 | backward_allreduce: 39.18 | step: 5258.36\n",
      "[2022-06-29 03:16:42,034] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 221.31 | optimizer_step: 5039.51\n",
      "[2022-06-29 03:16:42,035] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1194.88 | backward_microstep: 4306.60 | backward_inner_microstep: 4267.50 | backward_allreduce_microstep: 39.01 | step_microstep: 5300.94\n",
      "[2022-06-29 03:16:42,035] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1194.96 | backward: 4306.60 | backward_inner: 4267.51 | backward_allreduce: 39.02 | step: 5300.95\n",
      "[2022-06-29 03:16:52,903] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 192.48 | optimizer_step: 5126.71\n",
      "[2022-06-29 03:16:52,904] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.15 | backward_microstep: 4313.23 | backward_inner_microstep: 4274.21 | backward_allreduce_microstep: 38.93 | step_microstep: 5359.26\n",
      "[2022-06-29 03:16:52,904] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.22 | backward: 4313.23 | backward_inner: 4274.22 | backward_allreduce: 38.93 | step: 5359.26\n",
      "[2022-06-29 03:17:03,735] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 182.74 | optimizer_step: 5102.45\n",
      "[2022-06-29 03:17:03,736] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.33 | backward_microstep: 4310.84 | backward_inner_microstep: 4271.59 | backward_allreduce_microstep: 39.17 | step_microstep: 5325.40\n",
      "[2022-06-29 03:17:03,736] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.40 | backward: 4310.84 | backward_inner: 4271.60 | backward_allreduce: 39.17 | step: 5325.40\n",
      "[2022-06-29 03:17:14,504] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 182.61 | optimizer_step: 5041.43\n",
      "[2022-06-29 03:17:14,504] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.21 | backward_microstep: 4309.40 | backward_inner_microstep: 4270.35 | backward_allreduce_microstep: 38.96 | step_microstep: 5264.15\n",
      "[2022-06-29 03:17:14,504] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.28 | backward: 4309.40 | backward_inner: 4270.36 | backward_allreduce: 38.96 | step: 5264.15\n",
      "[2022-06-29 03:17:25,290] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 180.79 | optimizer_step: 5060.99\n",
      "[2022-06-29 03:17:25,291] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.00 | backward_microstep: 4309.55 | backward_inner_microstep: 4270.53 | backward_allreduce_microstep: 38.93 | step_microstep: 5281.87\n",
      "[2022-06-29 03:17:25,291] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.08 | backward: 4309.55 | backward_inner: 4270.54 | backward_allreduce: 38.93 | step: 5281.87\n",
      "[2022-06-29 03:17:36,070] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.78 | optimizer_gradients: 182.83 | optimizer_step: 5056.73\n",
      "[2022-06-29 03:17:36,071] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.23 | backward_microstep: 4304.92 | backward_inner_microstep: 4265.90 | backward_allreduce_microstep: 38.93 | step_microstep: 5279.79\n",
      "[2022-06-29 03:17:36,071] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.30 | backward: 4304.92 | backward_inner: 4265.91 | backward_allreduce: 38.93 | step: 5279.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 03:17:46,878] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 183.46 | optimizer_step: 5077.13\n",
      "[2022-06-29 03:17:46,879] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.11 | backward_microstep: 4312.57 | backward_inner_microstep: 4273.44 | backward_allreduce_microstep: 39.04 | step_microstep: 5300.83\n",
      "[2022-06-29 03:17:46,879] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.18 | backward: 4312.57 | backward_inner: 4273.45 | backward_allreduce: 39.05 | step: 5300.83\n",
      "[2022-06-29 03:17:57,677] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 185.21 | optimizer_step: 5068.99\n",
      "[2022-06-29 03:17:57,678] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.34375e-06, 2.34375e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:17:57,678] [INFO] [timer.py:181:stop] 0/50, SamplesPerSec=0.370806853608525\n",
      "[2022-06-29 03:17:57,678] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.54 | backward_microstep: 4310.76 | backward_inner_microstep: 4271.57 | backward_allreduce_microstep: 39.11 | step_microstep: 5294.49\n",
      "[2022-06-29 03:17:57,678] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.62 | backward: 4310.76 | backward_inner: 4271.58 | backward_allreduce: 39.11 | step: 5294.50\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 10797.3 | learning rate: 2.344E-06 | lm loss: 8.779677E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1194.52 | backward: 4309.73 | backward-backward: 4309.70 | backward-allreduce: 0.00 | optimizer: 5292.39 | batch generator: 1.40\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 1.717 B\n",
      "rank: 0 | time: 2022-06-29 03:17:57 | exiting the program at iteration 50\n",
      "[2022-06-29 03:18:02,542] [INFO] [launch.py:159:main] Process 39437 exits successfully.\n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/sc22-gpt-zero-infinity-cpu.sh 32 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_zero-infinity_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472684.txt && cd -\n",
      "deepspeed --num_nodes 1 --num_gpus 1 pretrain_gpt2.py --model-parallel-size 1 --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json --zero-stage 3 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-06-29 03:18:05,142] [WARNING] [runner.py:132:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2022-06-29 03:18:07,258] [INFO] [runner.py:398:main] cmd = /root/.pyenv/versions/3.9.10/envs/py3.9.10/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 pretrain_gpt2.py --model-parallel-size 1 --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json --zero-stage 3 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda11.4\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda11.4\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2022-06-29 03:18:08,105] [INFO] [launch.py:100:main] dist_world_size=1\n",
      "[2022-06-29 03:18:08,106] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... 4\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... True\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ True\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... mmap\n",
      "  data_path ....................... /home/sys/STRONGHOLD/data/my-gpt2-en_text_document\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... True\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 1000\n",
      "  exit_interval ................... 50\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 2048\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ checkpoints/gpt2_ds\n",
      "  local_rank ...................... 0\n",
      "  log_interval .................... 10\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. 0.00015\n",
      "  lr_decay_iters .................. 320000\n",
      "  lr_decay_style .................. cosine\n",
      "  lr_decay_tokens ................. None\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 1024\n",
      "  memory_centric_tiled_linear ..... False\n",
      "  merge_file ...................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  min_lr .......................... 1e-05\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 16\n",
      "  num_layers ...................... 32\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... True\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  remote_device ................... none\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ checkpoints/gpt2_ds\n",
      "  save_interval ................... 10000\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  scattered_embeddings ............ False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... 1024\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 949,50,1\n",
      "  split_transformers .............. False\n",
      "  synchronize_each_layer .......... True\n",
      "  tensorboard_dir ................. None\n",
      "  tile_factor ..................... 1\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  tokens .......................... 0\n",
      "  train_iters ..................... 50\n",
      "  train_tokens .................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  use_pin_memory .................. False\n",
      "  vocab_file ...................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  warmup .......................... 0.01\n",
      "  warmup_iters .................... None\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 5000000000\n",
      "  zero_contigious_gradients ....... True\n",
      "  zero_reduce_bucket_size ......... 50000000\n",
      "  zero_reduce_scatter ............. True\n",
      "  zero_stage ...................... 3\n",
      "---------------- end of arguments ----------------\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "building GPT2 model ...\n",
      "[2022-06-29 03:18:11,626] [INFO] [utils.py:822:see_memory_usage] Before Building Model\n",
      "[2022-06-29 03:18:11,626] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2022-06-29 03:18:11,627] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.2 GB, percent = 3.5%\n",
      "[2022-06-29 03:18:11,627] [WARNING] [partition_parameters.py:457:__init__] zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f2b55e161f0>\n",
      "train_batch = None, micro_batch=4\n",
      "[2022-06-29 03:18:16,790] [INFO] [utils.py:822:see_memory_usage] After Building Model\n",
      "[2022-06-29 03:18:16,791] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.38 GB         CA 0.39 GB         Max_CA 0 GB \n",
      "[2022-06-29 03:18:16,792] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 9.78 GB, percent = 10.8%\n",
      " > number of parameters on model parallel rank 0            1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "DeepSpeed is enabled.\n",
      "[2022-06-29 03:18:16,795] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.7, git-hash=unknown, git-branch=unknown\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f2b55e161f0>\n",
      "train_batch = None, micro_batch=4\n",
      "[2022-06-29 03:18:16,804] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups using mpu\n",
      "[2022-06-29 03:18:16,804] [INFO] [logging.py:69:log_dist] [Rank 0] Initializing deepspeed groups with model parallel size 1, expert parallel size 1, and data parallel size 1\n",
      "[2022-06-29 03:18:16,805] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
      "[2022-06-29 03:18:16,805] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
      "[2022-06-29 03:18:16,807] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "[2022-06-29 03:18:16,807] [INFO] [engine.py:1084:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2022-06-29 03:18:16,807] [INFO] [engine.py:1090:_configure_optimizer] Using client Optimizer as basic optimizer\n",
      "[2022-06-29 03:18:16,831] [INFO] [engine.py:1106:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW\n",
      "[2022-06-29 03:18:16,831] [INFO] [utils.py:43:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2022-06-29 03:18:16,831] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
      "Initializing ZeRO Stage 3\n",
      "[2022-06-29 03:18:16,835] [INFO] [stage3.py:639:__init__] Reduce bucket size 90000000\n",
      "[2022-06-29 03:18:16,835] [INFO] [stage3.py:640:__init__] Allgather bucket size 50000000.0\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu114/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.2202467918395996 seconds\n",
      "[2022-06-29 03:18:32,396] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 9922.29\n",
      "[2022-06-29 03:18:32,411] [INFO] [stage3.py:811:__init__] optimizer state initialized\n",
      "[2022-06-29 03:18:33,166] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2022-06-29 03:18:33,166] [INFO] [engine.py:798:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2022-06-29 03:18:33,166] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f2b52cc8130>\n",
      "[2022-06-29 03:18:33,166] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:18:33,167] [INFO] [config.py:1044:print] DeepSpeedEngine configuration:\n",
      "[2022-06-29 03:18:33,168] [INFO] [config.py:1048:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2022-06-29 03:18:33,168] [INFO] [config.py:1048:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 16, 'thread_count': 2, 'single_submit': False, 'overlap_events': True}\n",
      "[2022-06-29 03:18:33,168] [INFO] [config.py:1048:print]   allreduce_always_fp32 ........ False\n",
      "[2022-06-29 03:18:33,168] [INFO] [config.py:1048:print]   amp_enabled .................. False\n",
      "[2022-06-29 03:18:33,168] [INFO] [config.py:1048:print]   amp_params ................... False\n",
      "[2022-06-29 03:18:33,168] [INFO] [config.py:1048:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2022-06-29 03:18:33,168] [INFO] [config.py:1048:print]   bfloat16_enabled ............. False\n",
      "[2022-06-29 03:18:33,168] [INFO] [config.py:1048:print]   checkpoint_tag_validation_enabled  True\n",
      "[2022-06-29 03:18:33,168] [INFO] [config.py:1048:print]   checkpoint_tag_validation_fail  False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   curriculum_enabled ........... False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   curriculum_params ............ False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   dataloader_drop_last ......... False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   disable_allgather ............ False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   dump_state ................... False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   dynamic_loss_scale_args ...... None\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   eigenvalue_enabled ........... False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   eigenvalue_layer_num ......... 0\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   eigenvalue_max_iter .......... 100\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   eigenvalue_stability ......... 1e-06\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   eigenvalue_tol ............... 0.01\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   eigenvalue_verbose ........... False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   elasticity_enabled ........... False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   fp16_enabled ................. False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   fp16_master_weights_and_gradients  False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   fp16_mixed_quantize .......... False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   global_rank .................. 0\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   gradient_accumulation_steps .. 1\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   gradient_clipping ............ 1.0\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   gradient_predivide_factor .... 1.0\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   loss_scale ................... 0\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   memory_breakdown ............. False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   optimizer_legacy_fusion ...... False\n",
      "[2022-06-29 03:18:33,169] [INFO] [config.py:1048:print]   optimizer_name ............... None\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   optimizer_params ............. None\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   pld_enabled .................. False\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   pld_params ................... False\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   prescale_gradients ........... False\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   quantize_change_rate ......... 0.001\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   quantize_groups .............. 1\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   quantize_offset .............. 1000\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   quantize_period .............. 1000\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   quantize_rounding ............ 0\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   quantize_start_bits .......... 16\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   quantize_target_bits ......... 8\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   quantize_training_enabled .... False\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   quantize_type ................ 0\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   quantize_verbose ............. False\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   scheduler_name ............... None\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   scheduler_params ............. None\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   sparse_attention ............. None\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   sparse_gradients_enabled ..... False\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   steps_per_print .............. 10\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   tensorboard_enabled .......... False\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   tensorboard_output_path ...... \n",
      "[2022-06-29 03:18:33,170] [INFO] [config.py:1048:print]   train_batch_size ............. 4\n",
      "[2022-06-29 03:18:33,171] [INFO] [config.py:1048:print]   train_micro_batch_size_per_gpu  4\n",
      "[2022-06-29 03:18:33,171] [INFO] [config.py:1048:print]   use_quantizer_kernel ......... False\n",
      "[2022-06-29 03:18:33,171] [INFO] [config.py:1048:print]   wall_clock_breakdown ......... True\n",
      "[2022-06-29 03:18:33,171] [INFO] [config.py:1048:print]   world_size ................... 1\n",
      "[2022-06-29 03:18:33,171] [INFO] [config.py:1048:print]   zero_allow_untested_optimizer  False\n",
      "[2022-06-29 03:18:33,171] [INFO] [config.py:1048:print]   zero_config .................. {\n",
      "    \"stage\": 3, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 9.000000e+07, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": true, \n",
      "    \"offload_param\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 5, \n",
      "        \"buffer_size\": 1.000000e+08, \n",
      "        \"max_in_cpu\": 1, \n",
      "        \"pin_memory\": true\n",
      "    }, \n",
      "    \"offload_optimizer\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 4, \n",
      "        \"pin_memory\": true, \n",
      "        \"pipeline_read\": false, \n",
      "        \"pipeline_write\": false, \n",
      "        \"fast_init\": false, \n",
      "        \"pipeline\": false\n",
      "    }, \n",
      "    \"sub_group_size\": 1.000000e+08, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_fp16_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2022-06-29 03:18:33,171] [INFO] [config.py:1048:print]   zero_enabled ................. True\n",
      "[2022-06-29 03:18:33,171] [INFO] [config.py:1048:print]   zero_optimization_stage ...... 3\n",
      "[2022-06-29 03:18:33,171] [INFO] [config.py:1050:print]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_param_persitence_threshold\": 1.000000e+05, \n",
      "        \"stage3_prefetch_bucket_size\": 5.000000e+07, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_bucket_size\": 9.000000e+07, \n",
      "        \"sub_group_size\": 1.000000e+08, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"buffer_count\": 4, \n",
      "            \"pipeline_read\": false, \n",
      "            \"pipeline_write\": false, \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"max_in_cpu\": 1, \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": true, \n",
      "    \"zero_allow_untested_optimizer\": false, \n",
      "    \"aio\": {\n",
      "        \"block_size\": 1.048576e+06, \n",
      "        \"queue_depth\": 16, \n",
      "        \"single_submit\": false, \n",
      "        \"overlap_events\": true, \n",
      "        \"thread_count\": 2\n",
      "    }\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005476474761962891 seconds\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT2 ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000459 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT2 datasets ...\n",
      "setting training data start iteration to 0\n",
      "setting validation data start iteration to 0\n",
      "done with setups ...\n",
      "time (ms) | model and optimizer: 21588.88 | train/valid/test data iterators: 1228.55\n",
      "training ...\n",
      "[2022-06-29 03:18:56,877] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 15030.66\n",
      "[2022-06-29 03:18:56,878] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2421.48 | backward_microstep: 4981.15 | backward_inner_microstep: 4926.90 | backward_allreduce_microstep: 54.12 | step_microstep: 15061.45\n",
      "[2022-06-29 03:18:56,878] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2421.49 | backward: 4981.15 | backward_inner: 4926.91 | backward_allreduce: 54.14 | step: 15061.45\n",
      "[2022-06-29 03:19:07,955] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4251.80\n",
      "[2022-06-29 03:19:07,956] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1777.07 | backward_microstep: 5015.13 | backward_inner_microstep: 4960.73 | backward_allreduce_microstep: 54.30 | step_microstep: 4282.42\n",
      "[2022-06-29 03:19:07,956] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1777.08 | backward: 5015.13 | backward_inner: 4960.74 | backward_allreduce: 54.31 | step: 4282.43\n",
      "[2022-06-29 03:19:19,111] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4224.17\n",
      "[2022-06-29 03:19:19,112] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1797.46 | backward_microstep: 5100.60 | backward_inner_microstep: 5046.72 | backward_allreduce_microstep: 53.77 | step_microstep: 4253.64\n",
      "[2022-06-29 03:19:19,112] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1797.51 | backward: 5100.60 | backward_inner: 5046.72 | backward_allreduce: 53.78 | step: 4253.64\n",
      "[2022-06-29 03:19:30,269] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4249.93\n",
      "[2022-06-29 03:19:30,270] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1794.84 | backward_microstep: 5079.16 | backward_inner_microstep: 5025.14 | backward_allreduce_microstep: 53.91 | step_microstep: 4279.90\n",
      "[2022-06-29 03:19:30,270] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1794.86 | backward: 5079.16 | backward_inner: 5025.16 | backward_allreduce: 53.92 | step: 4279.90\n",
      "[2022-06-29 03:19:41,417] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4233.29\n",
      "[2022-06-29 03:19:41,418] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1792.71 | backward_microstep: 5087.47 | backward_inner_microstep: 5033.39 | backward_allreduce_microstep: 53.98 | step_microstep: 4263.62\n",
      "[2022-06-29 03:19:41,418] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1792.73 | backward: 5087.47 | backward_inner: 5033.40 | backward_allreduce: 53.99 | step: 4263.62\n",
      "[2022-06-29 03:19:52,615] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4268.29\n",
      "[2022-06-29 03:19:52,616] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1812.22 | backward_microstep: 5084.01 | backward_inner_microstep: 5029.88 | backward_allreduce_microstep: 54.02 | step_microstep: 4297.77\n",
      "[2022-06-29 03:19:52,616] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1812.24 | backward: 5084.01 | backward_inner: 5029.90 | backward_allreduce: 54.03 | step: 4297.77\n",
      "[2022-06-29 03:20:03,811] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4304.48\n",
      "[2022-06-29 03:20:03,811] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1789.85 | backward_microstep: 5067.95 | backward_inner_microstep: 5013.73 | backward_allreduce_microstep: 54.08 | step_microstep: 4333.67\n",
      "[2022-06-29 03:20:03,811] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1789.88 | backward: 5067.94 | backward_inner: 5013.75 | backward_allreduce: 54.11 | step: 4333.67\n",
      "[2022-06-29 03:20:14,967] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4225.71\n",
      "[2022-06-29 03:20:14,968] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1791.33 | backward_microstep: 5105.10 | backward_inner_microstep: 5050.61 | backward_allreduce_microstep: 54.37 | step_microstep: 4256.61\n",
      "[2022-06-29 03:20:14,968] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1791.35 | backward: 5105.10 | backward_inner: 5050.63 | backward_allreduce: 54.39 | step: 4256.62\n",
      "[2022-06-29 03:20:26,127] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4236.71\n",
      "[2022-06-29 03:20:26,128] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1791.06 | backward_microstep: 5097.82 | backward_inner_microstep: 5043.30 | backward_allreduce_microstep: 54.41 | step_microstep: 4266.28\n",
      "[2022-06-29 03:20:26,128] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1791.09 | backward: 5097.83 | backward_inner: 5043.32 | backward_allreduce: 54.43 | step: 4266.29\n",
      "[2022-06-29 03:20:37,288] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4219.14\n",
      "[2022-06-29 03:20:37,289] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.6874999999999996e-07, 4.6874999999999996e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:20:37,289] [INFO] [timer.py:181:stop] 0/10, SamplesPerSec=0.35832065821292713\n",
      "[2022-06-29 03:20:37,289] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1809.78 | backward_microstep: 5098.61 | backward_inner_microstep: 5044.29 | backward_allreduce_microstep: 54.20 | step_microstep: 4249.15\n",
      "[2022-06-29 03:20:37,290] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1809.79 | backward: 5098.61 | backward_inner: 5044.30 | backward_allreduce: 54.22 | step: 4249.15\n",
      " iteration       10/      50 | elapsed time per iteration (ms): 12288.5 | learning rate: 4.687E-07 | lm loss: 1.113701E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "after 10 iterations memory (MB) | allocated: 791.330078125 | max allocated: 4971.755859375 | reserved: 7734.0 | max reserved: 7734.0\n",
      "time (ms) | forward: 1861.24 | backward: 5071.80 | backward-backward: 5071.77 | backward-allreduce: 0.00 | optimizer: 5354.73 | batch generator: 1.82\n",
      "Effective Tera Flops per GPU: 0.46 and total parameters 1.717 B\n",
      "[2022-06-29 03:20:48,455] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4244.87\n",
      "[2022-06-29 03:20:48,456] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1787.28 | backward_microstep: 5099.11 | backward_inner_microstep: 5045.44 | backward_allreduce_microstep: 53.56 | step_microstep: 4273.79\n",
      "[2022-06-29 03:20:48,456] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1787.30 | backward: 5099.11 | backward_inner: 5045.45 | backward_allreduce: 53.57 | step: 4273.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 03:20:59,598] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4241.29\n",
      "[2022-06-29 03:20:59,599] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1786.28 | backward_microstep: 5081.79 | backward_inner_microstep: 5027.72 | backward_allreduce_microstep: 53.93 | step_microstep: 4270.96\n",
      "[2022-06-29 03:20:59,599] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1786.29 | backward: 5081.79 | backward_inner: 5027.74 | backward_allreduce: 53.96 | step: 4270.96\n",
      "[2022-06-29 03:21:10,740] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4232.15\n",
      "[2022-06-29 03:21:10,741] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1787.58 | backward_microstep: 5089.63 | backward_inner_microstep: 5035.47 | backward_allreduce_microstep: 54.00 | step_microstep: 4261.46\n",
      "[2022-06-29 03:21:10,741] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1787.60 | backward: 5089.63 | backward_inner: 5035.49 | backward_allreduce: 54.04 | step: 4261.46\n",
      "[2022-06-29 03:21:21,911] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4272.95\n",
      "[2022-06-29 03:21:21,912] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1784.62 | backward_microstep: 5078.97 | backward_inner_microstep: 5024.64 | backward_allreduce_microstep: 54.23 | step_microstep: 4303.69\n",
      "[2022-06-29 03:21:21,912] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1784.65 | backward: 5078.97 | backward_inner: 5024.65 | backward_allreduce: 54.24 | step: 4303.69\n",
      "[2022-06-29 03:21:33,087] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4244.21\n",
      "[2022-06-29 03:21:33,088] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1805.96 | backward_microstep: 5091.63 | backward_inner_microstep: 5037.69 | backward_allreduce_microstep: 53.83 | step_microstep: 4274.39\n",
      "[2022-06-29 03:21:33,088] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1805.97 | backward: 5091.63 | backward_inner: 5037.71 | backward_allreduce: 53.85 | step: 4274.39\n",
      "[2022-06-29 03:21:44,285] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4293.67\n",
      "[2022-06-29 03:21:44,286] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1793.92 | backward_microstep: 5075.90 | backward_inner_microstep: 5021.83 | backward_allreduce_microstep: 53.93 | step_microstep: 4323.43\n",
      "[2022-06-29 03:21:44,286] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1793.93 | backward: 5075.90 | backward_inner: 5021.84 | backward_allreduce: 53.96 | step: 4323.44\n",
      "[2022-06-29 03:21:55,485] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4263.62\n",
      "[2022-06-29 03:21:55,486] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1799.02 | backward_microstep: 5104.33 | backward_inner_microstep: 5050.12 | backward_allreduce_microstep: 54.08 | step_microstep: 4293.30\n",
      "[2022-06-29 03:21:55,486] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1799.04 | backward: 5104.33 | backward_inner: 5050.14 | backward_allreduce: 54.10 | step: 4293.30\n",
      "[2022-06-29 03:22:06,679] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4259.55\n",
      "[2022-06-29 03:22:06,679] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1789.36 | backward_microstep: 5110.14 | backward_inner_microstep: 5055.15 | backward_allreduce_microstep: 54.86 | step_microstep: 4289.62\n",
      "[2022-06-29 03:22:06,679] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1789.40 | backward: 5110.14 | backward_inner: 5055.17 | backward_allreduce: 54.88 | step: 4289.62\n",
      "[2022-06-29 03:22:17,853] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4242.07\n",
      "[2022-06-29 03:22:17,854] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1804.33 | backward_microstep: 5094.60 | backward_inner_microstep: 5036.86 | backward_allreduce_microstep: 57.58 | step_microstep: 4271.47\n",
      "[2022-06-29 03:22:17,854] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1804.35 | backward: 5094.60 | backward_inner: 5036.90 | backward_allreduce: 57.61 | step: 4271.49\n",
      "[2022-06-29 03:22:29,036] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4257.95\n",
      "[2022-06-29 03:22:29,037] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.374999999999999e-07, 9.374999999999999e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:22:29,037] [INFO] [timer.py:181:stop] 0/20, SamplesPerSec=0.3581780523838733\n",
      "[2022-06-29 03:22:29,037] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1789.16 | backward_microstep: 5101.96 | backward_inner_microstep: 5047.49 | backward_allreduce_microstep: 54.35 | step_microstep: 4288.42\n",
      "[2022-06-29 03:22:29,037] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1789.18 | backward: 5101.96 | backward_inner: 5047.51 | backward_allreduce: 54.36 | step: 4288.42\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 11174.8 | learning rate: 9.375E-07 | lm loss: 1.036611E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1795.57 | backward: 5092.90 | backward-backward: 5092.87 | backward-allreduce: 0.00 | optimizer: 4285.32 | batch generator: 1.26\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 1.717 B\n",
      "[2022-06-29 03:22:40,205] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4231.73\n",
      "[2022-06-29 03:22:40,206] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1791.32 | backward_microstep: 5110.51 | backward_inner_microstep: 5056.11 | backward_allreduce_microstep: 54.25 | step_microstep: 4261.12\n",
      "[2022-06-29 03:22:40,206] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1791.35 | backward: 5110.51 | backward_inner: 5056.12 | backward_allreduce: 54.29 | step: 4261.12\n",
      "[2022-06-29 03:22:51,373] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4245.83\n",
      "[2022-06-29 03:22:51,374] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1791.32 | backward_microstep: 5096.33 | backward_inner_microstep: 5041.67 | backward_allreduce_microstep: 54.52 | step_microstep: 4276.51\n",
      "[2022-06-29 03:22:51,374] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1791.35 | backward: 5096.34 | backward_inner: 5041.71 | backward_allreduce: 54.54 | step: 4276.52\n",
      "[2022-06-29 03:23:02,533] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4240.27\n",
      "[2022-06-29 03:23:02,534] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1802.31 | backward_microstep: 5083.82 | backward_inner_microstep: 5029.36 | backward_allreduce_microstep: 54.30 | step_microstep: 4269.60\n",
      "[2022-06-29 03:23:02,534] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1802.35 | backward: 5083.82 | backward_inner: 5029.39 | backward_allreduce: 54.33 | step: 4269.60\n",
      "[2022-06-29 03:23:13,724] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4262.24\n",
      "[2022-06-29 03:23:13,725] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1795.60 | backward_microstep: 5100.61 | backward_inner_microstep: 5045.92 | backward_allreduce_microstep: 54.57 | step_microstep: 4291.52\n",
      "[2022-06-29 03:23:13,725] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1795.65 | backward: 5100.61 | backward_inner: 5045.93 | backward_allreduce: 54.59 | step: 4291.52\n",
      "[2022-06-29 03:23:24,897] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4250.71\n",
      "[2022-06-29 03:23:24,898] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1786.13 | backward_microstep: 5102.50 | backward_inner_microstep: 5047.75 | backward_allreduce_microstep: 54.64 | step_microstep: 4280.50\n",
      "[2022-06-29 03:23:24,898] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1786.14 | backward: 5102.50 | backward_inner: 5047.76 | backward_allreduce: 54.66 | step: 4280.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 03:23:36,070] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4259.61\n",
      "[2022-06-29 03:23:36,071] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1788.22 | backward_microstep: 5090.67 | backward_inner_microstep: 5036.19 | backward_allreduce_microstep: 54.38 | step_microstep: 4289.57\n",
      "[2022-06-29 03:23:36,071] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1788.24 | backward: 5090.67 | backward_inner: 5036.21 | backward_allreduce: 54.39 | step: 4289.57\n",
      "[2022-06-29 03:23:47,239] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4240.32\n",
      "[2022-06-29 03:23:47,240] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1788.54 | backward_microstep: 5107.14 | backward_inner_microstep: 5053.23 | backward_allreduce_microstep: 53.81 | step_microstep: 4269.60\n",
      "[2022-06-29 03:23:47,240] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1788.55 | backward: 5107.14 | backward_inner: 5053.24 | backward_allreduce: 53.82 | step: 4269.60\n",
      "[2022-06-29 03:23:58,437] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4266.28\n",
      "[2022-06-29 03:23:58,438] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1808.48 | backward_microstep: 5088.96 | backward_inner_microstep: 5034.53 | backward_allreduce_microstep: 54.31 | step_microstep: 4297.07\n",
      "[2022-06-29 03:23:58,438] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1808.51 | backward: 5088.96 | backward_inner: 5034.55 | backward_allreduce: 54.33 | step: 4297.07\n",
      "[2022-06-29 03:24:09,571] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4229.13\n",
      "[2022-06-29 03:24:09,571] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1790.27 | backward_microstep: 5080.23 | backward_inner_microstep: 5025.71 | backward_allreduce_microstep: 54.39 | step_microstep: 4258.53\n",
      "[2022-06-29 03:24:09,572] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1790.29 | backward: 5080.23 | backward_inner: 5025.73 | backward_allreduce: 54.41 | step: 4258.53\n",
      "[2022-06-29 03:24:20,760] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4280.46\n",
      "[2022-06-29 03:24:20,761] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.4062499999999999e-06, 1.4062499999999999e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:24:20,761] [INFO] [timer.py:181:stop] 0/30, SamplesPerSec=0.3581642600833874\n",
      "[2022-06-29 03:24:20,761] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1789.84 | backward_microstep: 5085.45 | backward_inner_microstep: 5031.09 | backward_allreduce_microstep: 54.20 | step_microstep: 4310.25\n",
      "[2022-06-29 03:24:20,761] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1789.85 | backward: 5085.45 | backward_inner: 5031.13 | backward_allreduce: 54.23 | step: 4310.25\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 11172.4 | learning rate: 1.406E-06 | lm loss: 9.611809E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1796.06 | backward: 5094.72 | backward-backward: 5094.69 | backward-allreduce: 0.00 | optimizer: 4280.70 | batch generator: 1.29\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 1.717 B\n",
      "[2022-06-29 03:24:31,903] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4227.35\n",
      "[2022-06-29 03:24:31,904] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1790.85 | backward_microstep: 5088.93 | backward_inner_microstep: 5034.70 | backward_allreduce_microstep: 54.13 | step_microstep: 4256.45\n",
      "[2022-06-29 03:24:31,904] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1790.86 | backward: 5088.93 | backward_inner: 5034.71 | backward_allreduce: 54.15 | step: 4256.45\n",
      "[2022-06-29 03:24:43,131] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4295.08\n",
      "[2022-06-29 03:24:43,131] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1796.35 | backward_microstep: 5102.55 | backward_inner_microstep: 5048.62 | backward_allreduce_microstep: 53.78 | step_microstep: 4324.35\n",
      "[2022-06-29 03:24:43,131] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1796.40 | backward: 5102.55 | backward_inner: 5048.65 | backward_allreduce: 53.81 | step: 4324.35\n",
      "[2022-06-29 03:24:54,338] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4272.55\n",
      "[2022-06-29 03:24:54,339] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1791.55 | backward_microstep: 5110.15 | backward_inner_microstep: 5055.22 | backward_allreduce_microstep: 54.83 | step_microstep: 4302.07\n",
      "[2022-06-29 03:24:54,339] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1791.56 | backward: 5110.15 | backward_inner: 5055.23 | backward_allreduce: 54.84 | step: 4302.07\n",
      "[2022-06-29 03:25:05,509] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4246.76\n",
      "[2022-06-29 03:25:05,510] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1788.03 | backward_microstep: 5102.62 | backward_inner_microstep: 5048.01 | backward_allreduce_microstep: 54.50 | step_microstep: 4276.49\n",
      "[2022-06-29 03:25:05,510] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1788.05 | backward: 5102.62 | backward_inner: 5048.03 | backward_allreduce: 54.51 | step: 4276.49\n",
      "[2022-06-29 03:25:16,691] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4263.58\n",
      "[2022-06-29 03:25:16,692] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1787.89 | backward_microstep: 5096.82 | backward_inner_microstep: 5042.60 | backward_allreduce_microstep: 54.12 | step_microstep: 4292.84\n",
      "[2022-06-29 03:25:16,692] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1787.91 | backward: 5096.82 | backward_inner: 5042.61 | backward_allreduce: 54.13 | step: 4292.84\n",
      "[2022-06-29 03:25:27,876] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4248.69\n",
      "[2022-06-29 03:25:27,877] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1793.14 | backward_microstep: 5109.80 | backward_inner_microstep: 5055.61 | backward_allreduce_microstep: 54.06 | step_microstep: 4278.12\n",
      "[2022-06-29 03:25:27,877] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1793.15 | backward: 5109.79 | backward_inner: 5055.62 | backward_allreduce: 54.08 | step: 4278.12\n",
      "[2022-06-29 03:25:39,079] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4263.66\n",
      "[2022-06-29 03:25:39,080] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1804.49 | backward_microstep: 5101.85 | backward_inner_microstep: 5047.51 | backward_allreduce_microstep: 54.22 | step_microstep: 4293.27\n",
      "[2022-06-29 03:25:39,080] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1804.50 | backward: 5101.85 | backward_inner: 5047.53 | backward_allreduce: 54.24 | step: 4293.27\n",
      "[2022-06-29 03:25:50,275] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4275.47\n",
      "[2022-06-29 03:25:50,276] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1782.67 | backward_microstep: 5104.04 | backward_inner_microstep: 5049.84 | backward_allreduce_microstep: 54.10 | step_microstep: 4305.14\n",
      "[2022-06-29 03:25:50,276] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1782.70 | backward: 5104.04 | backward_inner: 5049.85 | backward_allreduce: 54.10 | step: 4305.15\n",
      "[2022-06-29 03:26:01,432] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4238.04\n",
      "[2022-06-29 03:26:01,433] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1782.41 | backward_microstep: 5103.55 | backward_inner_microstep: 5049.21 | backward_allreduce_microstep: 54.20 | step_microstep: 4267.24\n",
      "[2022-06-29 03:26:01,433] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1782.45 | backward: 5103.55 | backward_inner: 5049.23 | backward_allreduce: 54.23 | step: 4267.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-29 03:26:12,584] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4259.35\n",
      "[2022-06-29 03:26:12,585] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.8749999999999998e-06, 1.8749999999999998e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:26:12,585] [INFO] [timer.py:181:stop] 0/40, SamplesPerSec=0.3580743926343268\n",
      "[2022-06-29 03:26:12,585] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1783.62 | backward_microstep: 5074.89 | backward_inner_microstep: 5020.63 | backward_allreduce_microstep: 54.11 | step_microstep: 4289.17\n",
      "[2022-06-29 03:26:12,585] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1783.65 | backward: 5074.89 | backward_inner: 5020.66 | backward_allreduce: 54.14 | step: 4289.17\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 11182.4 | learning rate: 1.875E-06 | lm loss: 9.275822E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1793.02 | backward: 5099.62 | backward-backward: 5099.58 | backward-allreduce: 0.00 | optimizer: 4288.78 | batch generator: 1.34\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 1.717 B\n",
      "[2022-06-29 03:26:23,731] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4236.51\n",
      "[2022-06-29 03:26:23,732] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1790.03 | backward_microstep: 5086.06 | backward_inner_microstep: 5031.78 | backward_allreduce_microstep: 54.18 | step_microstep: 4265.58\n",
      "[2022-06-29 03:26:23,733] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1790.06 | backward: 5086.06 | backward_inner: 5031.79 | backward_allreduce: 54.19 | step: 4265.59\n",
      "[2022-06-29 03:26:34,917] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4240.27\n",
      "[2022-06-29 03:26:34,918] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1810.10 | backward_microstep: 5101.41 | backward_inner_microstep: 5046.51 | backward_allreduce_microstep: 54.76 | step_microstep: 4269.87\n",
      "[2022-06-29 03:26:34,918] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1810.12 | backward: 5101.41 | backward_inner: 5046.52 | backward_allreduce: 54.79 | step: 4269.87\n",
      "[2022-06-29 03:26:46,080] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4241.91\n",
      "[2022-06-29 03:26:46,081] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1793.59 | backward_microstep: 5093.25 | backward_inner_microstep: 5038.16 | backward_allreduce_microstep: 54.95 | step_microstep: 4271.71\n",
      "[2022-06-29 03:26:46,081] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1793.63 | backward: 5093.25 | backward_inner: 5038.18 | backward_allreduce: 54.98 | step: 4271.72\n",
      "[2022-06-29 03:26:57,268] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4254.11\n",
      "[2022-06-29 03:26:57,269] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1793.53 | backward_microstep: 5107.03 | backward_inner_microstep: 5052.21 | backward_allreduce_microstep: 54.68 | step_microstep: 4283.34\n",
      "[2022-06-29 03:26:57,269] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1793.58 | backward: 5107.03 | backward_inner: 5052.23 | backward_allreduce: 54.71 | step: 4283.35\n",
      "[2022-06-29 03:27:08,456] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4261.26\n",
      "[2022-06-29 03:27:08,456] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1792.79 | backward_microstep: 5100.46 | backward_inner_microstep: 5045.03 | backward_allreduce_microstep: 55.25 | step_microstep: 4290.71\n",
      "[2022-06-29 03:27:08,456] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1792.82 | backward: 5100.46 | backward_inner: 5045.07 | backward_allreduce: 55.29 | step: 4290.72\n",
      "[2022-06-29 03:27:19,640] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4243.16\n",
      "[2022-06-29 03:27:19,641] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1809.35 | backward_microstep: 5099.23 | backward_inner_microstep: 5044.82 | backward_allreduce_microstep: 54.31 | step_microstep: 4272.37\n",
      "[2022-06-29 03:27:19,641] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1809.37 | backward: 5099.23 | backward_inner: 5044.83 | backward_allreduce: 54.32 | step: 4272.37\n",
      "[2022-06-29 03:27:30,815] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4243.48\n",
      "[2022-06-29 03:27:30,816] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1794.47 | backward_microstep: 5103.22 | backward_inner_microstep: 5048.31 | backward_allreduce_microstep: 54.79 | step_microstep: 4272.82\n",
      "[2022-06-29 03:27:30,816] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1794.51 | backward: 5103.22 | backward_inner: 5048.33 | backward_allreduce: 54.80 | step: 4272.83\n",
      "[2022-06-29 03:27:41,985] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4245.95\n",
      "[2022-06-29 03:27:41,986] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1789.07 | backward_microstep: 5102.24 | backward_inner_microstep: 5048.15 | backward_allreduce_microstep: 53.98 | step_microstep: 4275.17\n",
      "[2022-06-29 03:27:41,986] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1789.08 | backward: 5102.24 | backward_inner: 5048.17 | backward_allreduce: 53.99 | step: 4275.17\n",
      "[2022-06-29 03:27:53,143] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4234.16\n",
      "[2022-06-29 03:27:53,144] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1790.95 | backward_microstep: 5099.98 | backward_inner_microstep: 5045.78 | backward_allreduce_microstep: 54.10 | step_microstep: 4263.40\n",
      "[2022-06-29 03:27:53,144] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1790.96 | backward: 5099.98 | backward_inner: 5045.79 | backward_allreduce: 54.11 | step: 4263.40\n",
      "[2022-06-29 03:28:04,328] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4258.76\n",
      "[2022-06-29 03:28:04,329] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.34375e-06, 2.34375e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-06-29 03:28:04,329] [INFO] [timer.py:181:stop] 0/50, SamplesPerSec=0.35807442201378215\n",
      "[2022-06-29 03:28:04,329] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1788.63 | backward_microstep: 5102.80 | backward_inner_microstep: 5048.43 | backward_allreduce_microstep: 54.26 | step_microstep: 4289.63\n",
      "[2022-06-29 03:28:04,329] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1788.66 | backward: 5102.80 | backward_inner: 5048.44 | backward_allreduce: 54.27 | step: 4289.63\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 11174.4 | learning rate: 2.344E-06 | lm loss: 9.047380E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1798.09 | backward: 5099.66 | backward-backward: 5099.63 | backward-allreduce: 0.00 | optimizer: 4275.74 | batch generator: 1.27\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 1.717 B\n",
      "rank: 0 | time: 2022-06-29 03:28:04 | exiting the program at iteration 50\n",
      "[2022-06-29 03:28:10,646] [INFO] [launch.py:159:main] Process 39620 exits successfully.\n",
      "/home/sys/STRONGHOLD\n"
     ]
    }
   ],
   "source": [
    "# Code Here\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "./examples/run.sh -m \"l2l\" -l 32 -h 2048 && \\\n",
    "./examples/run.sh -m \"zero-offload\" -l 32 -h 2048 && \\\n",
    "./examples/run.sh -m \"zero-infinity\" -l 32 -h 2048 && \\\n",
    "\\\n",
    "pyenv deactivate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:  grep -R 'SamplesPerSec' ./results/log_zero-infinity_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472684.txt ./results/log_zero-offload_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472088.txt | awk -v FS='[/,_: ]' '{print $5, $6, $4, $22}' | sort \r\n",
      "\r\n",
      "Running:  grep -R 'SamplesPerSec' ./results/log_l2l_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656470788.txt ./results/log_megatron-lm_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656464109.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-10_2022-06-29.1656478517.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-12_2022-06-29.1656478904.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-14_2022-06-29.1656479286.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-15_2022-06-29.1656468888.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-2_2022-06-29.1656476815.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656477260.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-6_2022-06-29.1656477696.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-8_2022-06-29.1656478111.txt ./results/log_zero-infinity_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472684.txt ./results/log_zero-offload_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656472088.txt --exclude=*zero*.txt | awk -v FS='[/,_: ]' '{print $5, $6, $4, $20, $22}' | sort \r\n",
      "\r\n",
      "l-32 hs-2048 zero-infinity SamplesPerSec=0.3580743926343268\r\n",
      "l-32 hs-2048 zero-infinity SamplesPerSec=0.35807442201378215\r\n",
      "l-32 hs-2048 zero-infinity SamplesPerSec=0.3581642600833874\r\n",
      "l-32 hs-2048 zero-infinity SamplesPerSec=0.3581780523838733\r\n",
      "l-32 hs-2048 zero-infinity SamplesPerSec=0.35832065821292713\r\n",
      "l-32 hs-2048 zero-offload SamplesPerSec=0.37059720779517674\r\n",
      "l-32 hs-2048 zero-offload SamplesPerSec=0.3707088317743382\r\n",
      "l-32 hs-2048 zero-offload SamplesPerSec=0.370806853608525\r\n",
      "l-32 hs-2048 zero-offload SamplesPerSec=0.3708752180795792\r\n",
      "l-32 hs-2048 zero-offload SamplesPerSec=0.3709171697153578\r\n",
      "l-32 hs-2048 l2l SamplesPerSecond 0.13781014511285522\r\n",
      "l-32 hs-2048 l2l SamplesPerSecond 0.16228004126551213\r\n",
      "l-32 hs-2048 l2l SamplesPerSecond 0.1747173025930287\r\n",
      "l-32 hs-2048 l2l SamplesPerSecond 0.17803514419011424\r\n",
      "l-32 hs-2048 l2l SamplesPerSecond 0.17974635511317175\r\n",
      "l-32 hs-2048 megatron-lm SamplesPerSecond 0.7438263111268488\r\n",
      "l-32 hs-2048 megatron-lm SamplesPerSecond 0.7496099192261813\r\n",
      "l-32 hs-2048 megatron-lm SamplesPerSecond 0.7497347875823174\r\n",
      "l-32 hs-2048 megatron-lm SamplesPerSecond 0.7498405609294753\r\n",
      "l-32 hs-2048 megatron-lm SamplesPerSecond 0.7501883830820277\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.4443891688213538\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.4588837381719193\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.48206704653991833\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.4936834767174005\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.500718459212588\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5040417547536666\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5064041904239727\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5084029606168415\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5089427664506396\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5160017424807263\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5166440240183751\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5196351929775191\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5271565503740522\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5272408353652496\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.538802376275119\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5487643996098076\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5491873261544319\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5501055754982426\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5526876802820975\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5580439624480265\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5634126642091529\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5635024715766087\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5671335866691215\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5806314246860206\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5863023174163046\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5892956651298789\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5912245060047543\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.5967951704471945\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6064073519755019\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6064338524896208\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6075235571405322\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6122348693502793\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.613892038809362\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6320721247511137\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6423642682670394\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6464636949129531\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6609623921101718\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6647505018876956\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.6692845753956063\r\n",
      "l-32 hs-2048 stronghold SamplesPerSecond 0.671491268850768\r\n"
     ]
    }
   ],
   "source": [
    "# To print the relevant information from log files\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "./examples/case3.sh && \\\n",
    "\\\n",
    "pyenv deactivate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 CASE - Nearly linear scaling as model size increases (Figure 8b in Section VI.B)\n",
    "\n",
    "In this case, we evaluate the performance (elapsed time per iteration - ms) as the model size increases. Similar to previous cases, the model size changes via increasing/decreasing the number of transformer layers. \n",
    "\n",
    "You would see the `elapsed time per iteration` linearly rise with the number of transformer layers (representing model size), proving STRONGHOLD's scalability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.0  0.0   2520    72 pts/0    Ss+  Jun28   0:00 sleep infinit\r\n",
      "root           7  0.0  0.0   4348   696 pts/1    Ss+  Jun28   0:00 /bin/bash\r\n",
      "root         159  0.0  0.0   4348   804 pts/2    Ss+  Jun28   0:00 /bin/bash\r\n",
      "root       35813  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35825  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35855  0.0  0.0      0     0 ?        Z    01:01   0:01 [python] <def\r\n",
      "root       35856  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35864  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35865  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       39751  0.0  0.0   3976  3092 pts/3    Ss+  03:28   0:00 /bin/bash -c \r\n",
      "root       39909  0.0  0.0   5892  2784 pts/3    R+   03:28   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "# To check if there exists other running processes launched by other reviwers in case of GPU overlead.\n",
    "# Just run it and no need to change anything in this cell.\n",
    "#\n",
    "# `ps aux` in docker container. \n",
    "######\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c 'export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && export PYENV_ROOT=\"/root/.pyenv\" && export PATH=\"$PYENV_ROOT/bin:$PATH\" && eval \"$(pyenv init -)\" && eval \"$(pyenv virtualenv-init -)\" && pyenv activate py3.9.10 && ps aux && pyenv deactivate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following results correspond to Figure 8b in the submitted paper. Please refers to section VI.B on page 9 for more details. Run around 60 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 92 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-92_hs-2048_bs-4_ws-15_2022-06-29.1656473293.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 92 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 92\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 92\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.514 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.697 seconds\n",
      "time to initialize megatron (seconds): 3.698\n",
      "[after megatron is initialized] datetime: 2022-06-29 03:28:20 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 4738084864\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             4.738 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.19\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 03:29:35 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000551 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 03:29:36 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 75267.20 | train/valid/test-data-iterators-setup: 562.59\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 03:29:36 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 24006.4 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.070356E+01 | loss scale: 1.0 | grad norm: 58.297 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.47 and total parameters 4.738 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 24.00635666847229;  SamplesPerSecond: 0.1666225348244212\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 25193.119140625 | reserved: 31118.0 | max reserved: 31118.0\n",
      "time (ms) | e2e-time: 24003.63 | forward-compute: 3364.07 | backward-compute: 20628.33 | backward-embedding-all-reduce: 0.02 | optimizer: 2.15 | batch-generator: 1.95 | offloading-func-call-overhead: 6683.81 | offloading-fwd-overhead: 2252.34 | offloading-bwd-overhead: 343.10 | offloading-fwd-2gpu-overhead: 1081.72 | offloading-fwd-2cpu-overhead: 1168.83 | offloading-bwd-2gpu-overhead: 3.70 | offloading-bwd-2cpu-overhead: 336.67\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 21351.2 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.028280E+01 | loss scale: 1.0 | grad norm: 9441974.784 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.27 and total parameters 4.738 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 21.351154804229736;  SamplesPerSecond: 0.18734349671838763\n",
      "time (ms) | e2e-time: 21351.09 | forward-compute: 3165.73 | backward-compute: 18174.37 | backward-embedding-all-reduce: 0.02 | optimizer: 2.04 | batch-generator: 1.38 | offloading-func-call-overhead: 50.32 | offloading-fwd-overhead: 2863.15 | offloading-bwd-overhead: 67.51 | offloading-fwd-2gpu-overhead: 1428.01 | offloading-fwd-2cpu-overhead: 1433.25 | offloading-bwd-2gpu-overhead: 3.94 | offloading-bwd-2cpu-overhead: 60.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 21290.7 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.016363E+01 | loss scale: 1.0 | grad norm: inf | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.29 and total parameters 4.738 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 21.290717983245848;  SamplesPerSecond: 0.1878752986699505\n",
      "time (ms) | e2e-time: 21290.74 | forward-compute: 3177.43 | backward-compute: 18102.28 | backward-embedding-all-reduce: 0.02 | optimizer: 2.04 | batch-generator: 1.41 | offloading-func-call-overhead: 51.72 | offloading-fwd-overhead: 2881.05 | offloading-bwd-overhead: 93.44 | offloading-fwd-2gpu-overhead: 1373.79 | offloading-fwd-2cpu-overhead: 1505.35 | offloading-bwd-2gpu-overhead: 3.89 | offloading-bwd-2cpu-overhead: 87.10\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 21211.0 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.961718E+00 | loss scale: 1.0 | grad norm: 36329.609 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.32 and total parameters 4.738 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 21.210970544815062;  SamplesPerSecond: 0.18858165832387072\n",
      "time (ms) | e2e-time: 21210.95 | forward-compute: 3100.64 | backward-compute: 18099.30 | backward-embedding-all-reduce: 0.02 | optimizer: 2.04 | batch-generator: 1.39 | offloading-func-call-overhead: 50.76 | offloading-fwd-overhead: 2791.29 | offloading-bwd-overhead: 52.27 | offloading-fwd-2gpu-overhead: 1341.47 | offloading-fwd-2cpu-overhead: 1447.58 | offloading-bwd-2gpu-overhead: 4.19 | offloading-bwd-2cpu-overhead: 45.56\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 21267.1 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.849120E+00 | loss scale: 1.0 | grad norm: 43.413 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.3 and total parameters 4.738 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 21.267106580734254;  SamplesPerSecond: 0.1880838836639666\n",
      "time (ms) | e2e-time: 21267.11 | forward-compute: 3158.16 | backward-compute: 18097.98 | backward-embedding-all-reduce: 0.02 | optimizer: 2.05 | batch-generator: 1.40 | offloading-func-call-overhead: 50.74 | offloading-fwd-overhead: 2840.90 | offloading-bwd-overhead: 343.57 | offloading-fwd-2gpu-overhead: 1444.48 | offloading-fwd-2cpu-overhead: 1394.48 | offloading-bwd-2gpu-overhead: 3.80 | offloading-bwd-2cpu-overhead: 337.35\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 03:47:47 \n",
      "[after training is done] datetime: 2022-06-29 03:47:47 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 64 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-64_hs-2048_bs-4_ws-15_2022-06-29.1656474476.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 64 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 64\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 64\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.156 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.630 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.712 seconds\n",
      "time to initialize megatron (seconds): 3.831\n",
      "[after megatron is initialized] datetime: 2022-06-29 03:48:03 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3328053248\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             3.328 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.24\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 03:48:57 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000606 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 03:48:57 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 53502.85 | train/valid/test-data-iterators-setup: 503.01\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 03:48:57 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 15958.4 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.085885E+01 | loss scale: 1.0 | grad norm: 330172241779.598 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.83 and total parameters 3.328 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 15.958398270606995;  SamplesPerSecond: 0.2506517215682859\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 24649.384765625 | reserved: 31036.0 | max reserved: 31036.0\n",
      "time (ms) | e2e-time: 15958.62 | forward-compute: 2202.18 | backward-compute: 13745.31 | backward-embedding-all-reduce: 0.02 | optimizer: 2.29 | batch-generator: 1.86 | offloading-func-call-overhead: 4300.08 | offloading-fwd-overhead: 1450.51 | offloading-bwd-overhead: 33.80 | offloading-fwd-2gpu-overhead: 693.11 | offloading-fwd-2cpu-overhead: 756.15 | offloading-bwd-2gpu-overhead: 2.31 | offloading-bwd-2cpu-overhead: 29.65\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 14118.4 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.025652E+01 | loss scale: 1.0 | grad norm: 3.226 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.72 and total parameters 3.328 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 14.118449592590332;  SamplesPerSecond: 0.28331722784202074\n",
      "time (ms) | e2e-time: 14118.57 | forward-compute: 2037.61 | backward-compute: 12069.92 | backward-embedding-all-reduce: 0.02 | optimizer: 2.22 | batch-generator: 1.33 | offloading-func-call-overhead: 35.82 | offloading-fwd-overhead: 1822.99 | offloading-bwd-overhead: 40.21 | offloading-fwd-2gpu-overhead: 867.39 | offloading-fwd-2cpu-overhead: 954.22 | offloading-bwd-2gpu-overhead: 2.64 | offloading-bwd-2cpu-overhead: 35.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 14303.4 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.013971E+01 | loss scale: 1.0 | grad norm: 51.268 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.62 and total parameters 3.328 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 14.303354072570801;  SamplesPerSecond: 0.27965468656548914\n",
      "time (ms) | e2e-time: 14303.18 | forward-compute: 2014.60 | backward-compute: 12277.71 | backward-embedding-all-reduce: 0.02 | optimizer: 2.25 | batch-generator: 1.35 | offloading-func-call-overhead: 35.15 | offloading-fwd-overhead: 1800.35 | offloading-bwd-overhead: 104.04 | offloading-fwd-2gpu-overhead: 877.13 | offloading-fwd-2cpu-overhead: 921.88 | offloading-bwd-2gpu-overhead: 2.51 | offloading-bwd-2cpu-overhead: 98.38\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 14053.7 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.906329E+00 | loss scale: 1.0 | grad norm: 3.546 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.76 and total parameters 3.328 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 14.053739166259765;  SamplesPerSecond: 0.284621761701911\n",
      "time (ms) | e2e-time: 14053.76 | forward-compute: 2009.65 | backward-compute: 12033.22 | backward-embedding-all-reduce: 0.02 | optimizer: 2.22 | batch-generator: 1.33 | offloading-func-call-overhead: 36.47 | offloading-fwd-overhead: 1787.34 | offloading-bwd-overhead: 174.10 | offloading-fwd-2gpu-overhead: 848.32 | offloading-fwd-2cpu-overhead: 937.57 | offloading-bwd-2gpu-overhead: 2.47 | offloading-bwd-2cpu-overhead: 169.77\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 14068.9 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.926849E+00 | loss scale: 1.0 | grad norm: 3.588 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.75 and total parameters 3.328 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 14.068874025344849;  SamplesPerSecond: 0.284315574422947\n",
      "time (ms) | e2e-time: 14068.85 | forward-compute: 2047.54 | backward-compute: 12010.48 | backward-embedding-all-reduce: 0.02 | optimizer: 2.22 | batch-generator: 1.31 | offloading-func-call-overhead: 33.54 | offloading-fwd-overhead: 1832.61 | offloading-bwd-overhead: 128.71 | offloading-fwd-2gpu-overhead: 889.64 | offloading-fwd-2cpu-overhead: 941.51 | offloading-bwd-2gpu-overhead: 2.46 | offloading-bwd-2cpu-overhead: 124.04\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 04:01:03 \n",
      "[after training is done] datetime: 2022-06-29 04:01:03 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 56 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-56_hs-2048_bs-4_ws-15_2022-06-29.1656475269.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 56 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 56\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 56\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.528 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.716 seconds\n",
      "time to initialize megatron (seconds): 3.746\n",
      "[after megatron is initialized] datetime: 2022-06-29 04:01:16 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2925187072\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             2.925 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.22\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 04:02:04 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000602 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 04:02:04 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 47545.27 | train/valid/test-data-iterators-setup: 497.77\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 04:02:04 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 13543.4 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.055514E+01 | loss scale: 1.0 | grad norm: 23.933 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.08 and total parameters 2.925 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 13.543380522727967;  SamplesPerSecond: 0.29534723574275695\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 24121.361328125 | reserved: 30350.0 | max reserved: 30350.0\n",
      "time (ms) | e2e-time: 13543.53 | forward-compute: 1828.22 | backward-compute: 11704.20 | backward-embedding-all-reduce: 0.02 | optimizer: 2.34 | batch-generator: 1.88 | offloading-func-call-overhead: 3683.44 | offloading-fwd-overhead: 1204.73 | offloading-bwd-overhead: 131.00 | offloading-fwd-2gpu-overhead: 576.96 | offloading-fwd-2cpu-overhead: 626.57 | offloading-bwd-2gpu-overhead: 1.93 | offloading-bwd-2cpu-overhead: 127.46\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 12109.2 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.001507E+01 | loss scale: 1.0 | grad norm: 8.646 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.92 and total parameters 2.925 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 12.10920615196228;  SamplesPerSecond: 0.33032718658867705\n",
      "time (ms) | e2e-time: 12109.12 | forward-compute: 1704.21 | backward-compute: 10394.19 | backward-embedding-all-reduce: 0.02 | optimizer: 2.28 | batch-generator: 1.32 | offloading-func-call-overhead: 31.09 | offloading-fwd-overhead: 1522.90 | offloading-bwd-overhead: 4.47 | offloading-fwd-2gpu-overhead: 698.93 | offloading-fwd-2cpu-overhead: 822.79 | offloading-bwd-2gpu-overhead: 2.03 | offloading-bwd-2cpu-overhead: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 12019.5 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.894191E+00 | loss scale: 1.0 | grad norm: 728.236 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.97 and total parameters 2.925 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 12.019475555419922;  SamplesPerSecond: 0.3327932222630368\n",
      "time (ms) | e2e-time: 12019.47 | forward-compute: 1702.20 | backward-compute: 10306.51 | backward-embedding-all-reduce: 0.02 | optimizer: 2.27 | batch-generator: 1.35 | offloading-func-call-overhead: 37.43 | offloading-fwd-overhead: 1515.65 | offloading-bwd-overhead: 4.56 | offloading-fwd-2gpu-overhead: 720.90 | offloading-fwd-2cpu-overhead: 793.65 | offloading-bwd-2gpu-overhead: 1.99 | offloading-bwd-2cpu-overhead: 0.86\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 12081.2 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.664505E+00 | loss scale: 1.0 | grad norm: 1373.802 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.93 and total parameters 2.925 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 12.081222772598267;  SamplesPerSecond: 0.33109231369133457\n",
      "time (ms) | e2e-time: 12081.22 | forward-compute: 1688.83 | backward-compute: 10381.63 | backward-embedding-all-reduce: 0.02 | optimizer: 2.29 | batch-generator: 1.28 | offloading-func-call-overhead: 32.81 | offloading-fwd-overhead: 1491.15 | offloading-bwd-overhead: 476.54 | offloading-fwd-2gpu-overhead: 723.50 | offloading-fwd-2cpu-overhead: 766.52 | offloading-bwd-2gpu-overhead: 2.22 | offloading-bwd-2cpu-overhead: 472.73\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 11992.3 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.469240E+00 | loss scale: 1.0 | grad norm: 136.620 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.99 and total parameters 2.925 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 11.992267870903015;  SamplesPerSecond: 0.33354825317947145\n",
      "time (ms) | e2e-time: 11992.26 | forward-compute: 1701.87 | backward-compute: 10279.66 | backward-embedding-all-reduce: 0.02 | optimizer: 2.28 | batch-generator: 1.28 | offloading-func-call-overhead: 29.84 | offloading-fwd-overhead: 1523.39 | offloading-bwd-overhead: 4.51 | offloading-fwd-2gpu-overhead: 730.93 | offloading-fwd-2cpu-overhead: 791.34 | offloading-bwd-2gpu-overhead: 2.12 | offloading-bwd-2cpu-overhead: 0.86\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 04:12:22 \n",
      "[after training is done] datetime: 2022-06-29 04:12:22 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 40 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-40_hs-2048_bs-4_ws-15_2022-06-29.1656475948.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 40 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 40\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 40\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.579 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.703 seconds\n",
      "time to initialize megatron (seconds): 3.779\n",
      "[after megatron is initialized] datetime: 2022-06-29 04:12:35 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2119454720\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             2.119 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.19\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 04:13:10 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000608 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 04:13:11 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 34997.10 | train/valid/test-data-iterators-setup: 438.98\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 04:13:11 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 9029.8 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.097706E+01 | loss scale: 1.0 | grad norm: 1924.498 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.69 and total parameters 2.119 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 9.029772448539735;  SamplesPerSecond: 0.4429790476776485\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 23577.345703125 | reserved: 29884.0 | max reserved: 29884.0\n",
      "time (ms) | e2e-time: 9029.87 | forward-compute: 1160.78 | backward-compute: 7858.07 | backward-embedding-all-reduce: 0.02 | optimizer: 2.42 | batch-generator: 1.82 | offloading-func-call-overhead: 2309.60 | offloading-fwd-overhead: 749.55 | offloading-bwd-overhead: 2.77 | offloading-fwd-2gpu-overhead: 341.34 | offloading-fwd-2cpu-overhead: 406.95 | offloading-bwd-2gpu-overhead: 1.14 | offloading-bwd-2cpu-overhead: 0.58\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 8175.4 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.049935E+01 | loss scale: 1.0 | grad norm: 13.428 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.5 and total parameters 2.119 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.175377941131591;  SamplesPerSecond: 0.4892740162965899\n",
      "time (ms) | e2e-time: 8175.31 | forward-compute: 1069.91 | backward-compute: 7094.65 | backward-embedding-all-reduce: 0.02 | optimizer: 2.40 | batch-generator: 1.31 | offloading-func-call-overhead: 19.70 | offloading-fwd-overhead: 946.56 | offloading-bwd-overhead: 2.85 | offloading-fwd-2gpu-overhead: 404.56 | offloading-fwd-2cpu-overhead: 540.99 | offloading-bwd-2gpu-overhead: 1.21 | offloading-bwd-2cpu-overhead: 0.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 7918.5 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.984436E+00 | loss scale: 1.0 | grad norm: 3.623 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.77 and total parameters 2.119 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.918460369110107;  SamplesPerSecond: 0.5051487048674246\n",
      "time (ms) | e2e-time: 7918.43 | forward-compute: 1099.66 | backward-compute: 6808.02 | backward-embedding-all-reduce: 0.02 | optimizer: 2.41 | batch-generator: 1.34 | offloading-func-call-overhead: 19.99 | offloading-fwd-overhead: 967.45 | offloading-bwd-overhead: 198.33 | offloading-fwd-2gpu-overhead: 432.43 | offloading-fwd-2cpu-overhead: 534.23 | offloading-bwd-2gpu-overhead: 1.20 | offloading-bwd-2cpu-overhead: 195.94\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 8307.6 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.544877E+00 | loss scale: 1.0 | grad norm: 4.066 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.36 and total parameters 2.119 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.307634806632995;  SamplesPerSecond: 0.48148481404193566\n",
      "time (ms) | e2e-time: 8307.64 | forward-compute: 1086.78 | backward-compute: 7210.14 | backward-embedding-all-reduce: 0.02 | optimizer: 2.40 | batch-generator: 1.29 | offloading-func-call-overhead: 31.03 | offloading-fwd-overhead: 951.03 | offloading-bwd-overhead: 2.97 | offloading-fwd-2gpu-overhead: 456.38 | offloading-fwd-2cpu-overhead: 493.78 | offloading-bwd-2gpu-overhead: 1.31 | offloading-bwd-2cpu-overhead: 0.60\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 8031.9 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.645538E+00 | loss scale: 1.0 | grad norm: 3.377 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.65 and total parameters 2.119 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.031850218772888;  SamplesPerSecond: 0.49801725518371565\n",
      "time (ms) | e2e-time: 8031.86 | forward-compute: 1068.08 | backward-compute: 6953.07 | backward-embedding-all-reduce: 0.02 | optimizer: 2.40 | batch-generator: 1.21 | offloading-func-call-overhead: 19.34 | offloading-fwd-overhead: 940.35 | offloading-bwd-overhead: 3.06 | offloading-fwd-2gpu-overhead: 407.20 | offloading-fwd-2cpu-overhead: 532.38 | offloading-bwd-2gpu-overhead: 1.25 | offloading-bwd-2cpu-overhead: 0.61\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 04:20:05 \n",
      "[after training is done] datetime: 2022-06-29 04:20:05 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 24 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-24_hs-2048_bs-4_ws-15_2022-06-29.1656476410.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 24 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 24\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 24\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.579 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.698 seconds\n",
      "time to initialize megatron (seconds): 3.781\n",
      "[after megatron is initialized] datetime: 2022-06-29 04:20:17 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1313722368\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.314 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.20\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 04:20:39 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000556 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 04:20:40 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 22388.46 | train/valid/test-data-iterators-setup: 422.12\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 04:20:40 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 4489.9 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.039566E+01 | loss scale: 1.0 | grad norm: 3.360 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.59 and total parameters 1.314 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 4.489934277534485;  SamplesPerSecond: 0.8908816371798837\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 16782.1015625 | max allocated: 20809.822265625 | reserved: 25428.0 | max reserved: 25428.0\n",
      "time (ms) | e2e-time: 4490.04 | forward-compute: 455.69 | backward-compute: 4023.45 | backward-embedding-all-reduce: 0.02 | optimizer: 2.53 | batch-generator: 1.70 | offloading-func-call-overhead: 913.75 | offloading-fwd-overhead: 264.48 | offloading-bwd-overhead: 1.40 | offloading-fwd-2gpu-overhead: 8.14 | offloading-fwd-2cpu-overhead: 255.10 | offloading-bwd-2gpu-overhead: 0.38 | offloading-bwd-2cpu-overhead: 0.35\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 4123.4 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.058675E+01 | loss scale: 1.0 | grad norm: 135238081.521 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.44 and total parameters 1.314 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 4.123373937606812;  SamplesPerSecond: 0.9700793720206667\n",
      "time (ms) | e2e-time: 4123.21 | forward-compute: 422.10 | backward-compute: 3690.62 | backward-embedding-all-reduce: 0.02 | optimizer: 2.47 | batch-generator: 1.23 | offloading-func-call-overhead: 11.84 | offloading-fwd-overhead: 336.13 | offloading-bwd-overhead: 1.45 | offloading-fwd-2gpu-overhead: 7.82 | offloading-fwd-2cpu-overhead: 327.82 | offloading-bwd-2gpu-overhead: 0.39 | offloading-bwd-2cpu-overhead: 0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 4123.0 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.153189E+01 | loss scale: 1.0 | grad norm: 28.039 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.44 and total parameters 1.314 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 4.122998070716858;  SamplesPerSecond: 0.9701678078409888\n",
      "time (ms) | e2e-time: 4122.98 | forward-compute: 475.72 | backward-compute: 3636.76 | backward-embedding-all-reduce: 0.02 | optimizer: 2.50 | batch-generator: 1.23 | offloading-func-call-overhead: 10.40 | offloading-fwd-overhead: 387.36 | offloading-bwd-overhead: 1.65 | offloading-fwd-2gpu-overhead: 12.51 | offloading-fwd-2cpu-overhead: 374.39 | offloading-bwd-2gpu-overhead: 0.39 | offloading-bwd-2cpu-overhead: 0.48\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 4068.7 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.516344E+00 | loss scale: 1.0 | grad norm: 2480016905540045.500 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.58 and total parameters 1.314 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 4.068661379814148;  SamplesPerSecond: 0.9831243317139151\n",
      "time (ms) | e2e-time: 4068.67 | forward-compute: 423.39 | backward-compute: 3634.79 | backward-embedding-all-reduce: 0.02 | optimizer: 2.47 | batch-generator: 1.17 | offloading-func-call-overhead: 10.15 | offloading-fwd-overhead: 332.37 | offloading-bwd-overhead: 1.50 | offloading-fwd-2gpu-overhead: 5.58 | offloading-fwd-2cpu-overhead: 326.13 | offloading-bwd-2gpu-overhead: 0.39 | offloading-bwd-2cpu-overhead: 0.37\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 4190.5 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.374594E+00 | loss scale: 1.0 | grad norm: 123886504.173 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.27 and total parameters 1.314 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 4.19052414894104;  SamplesPerSecond: 0.9545345302474427\n",
      "time (ms) | e2e-time: 4190.50 | forward-compute: 506.93 | backward-compute: 3673.08 | backward-embedding-all-reduce: 0.02 | optimizer: 2.48 | batch-generator: 1.21 | offloading-func-call-overhead: 9.88 | offloading-fwd-overhead: 425.32 | offloading-bwd-overhead: 1.44 | offloading-fwd-2gpu-overhead: 10.10 | offloading-fwd-2cpu-overhead: 414.77 | offloading-bwd-2gpu-overhead: 0.39 | offloading-bwd-2cpu-overhead: 0.38\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 04:24:10 \n",
      "[after training is done] datetime: 2022-06-29 04:24:10 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 16 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-16_hs-2048_bs-4_ws-15_2022-06-29.1656476653.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 16 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 16\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 16\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.157 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.602 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.706 seconds\n",
      "time to initialize megatron (seconds): 3.809\n",
      "[after megatron is initialized] datetime: 2022-06-29 04:24:20 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 910856192\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             0.911 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.18\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 04:24:36 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000551 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.005 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 04:24:36 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 15825.47 | train/valid/test-data-iterators-setup: 387.75\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 04:24:36 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 2726.6 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.115618E+01 | loss scale: 1.0 | grad norm: inf | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.95 and total parameters 0.911 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 2.7266039848327637;  SamplesPerSecond: 1.4670263896960234\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 13708.4765625 | max allocated: 16917.89404296875 | reserved: 17542.0 | max reserved: 17542.0\n",
      "time (ms) | e2e-time: 2726.64 | forward-compute: 48.13 | backward-compute: 2667.81 | backward-embedding-all-reduce: 0.02 | optimizer: 2.23 | batch-generator: 1.62 | offloading-func-call-overhead: 133.16 | offloading-fwd-overhead: 1.11 | offloading-bwd-overhead: 0.65 | offloading-fwd-2gpu-overhead: 0.02 | offloading-fwd-2cpu-overhead: 0.81 | offloading-bwd-2gpu-overhead: 0.05 | offloading-bwd-2cpu-overhead: 0.22\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 2722.1 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.117472E+01 | loss scale: 1.0 | grad norm: inf | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.96 and total parameters 0.911 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 2.722097611427307;  SamplesPerSecond: 1.46945501998462\n",
      "time (ms) | e2e-time: 2722.03 | forward-compute: 34.59 | backward-compute: 2676.95 | backward-embedding-all-reduce: 0.02 | optimizer: 2.26 | batch-generator: 1.14 | offloading-func-call-overhead: 3.78 | offloading-fwd-overhead: 1.18 | offloading-bwd-overhead: 0.66 | offloading-fwd-2gpu-overhead: 0.01 | offloading-fwd-2cpu-overhead: 0.90 | offloading-bwd-2gpu-overhead: 0.05 | offloading-bwd-2cpu-overhead: 0.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 2730.6 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.134986E+01 | loss scale: 1.0 | grad norm: inf | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.93 and total parameters 0.911 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 2.730643391609192;  SamplesPerSecond: 1.4648562358202202\n",
      "time (ms) | e2e-time: 2730.59 | forward-compute: 35.96 | backward-compute: 2684.20 | backward-embedding-all-reduce: 0.02 | optimizer: 2.25 | batch-generator: 1.16 | offloading-func-call-overhead: 4.04 | offloading-fwd-overhead: 1.04 | offloading-bwd-overhead: 0.71 | offloading-fwd-2gpu-overhead: 0.02 | offloading-fwd-2cpu-overhead: 0.74 | offloading-bwd-2gpu-overhead: 0.05 | offloading-bwd-2cpu-overhead: 0.26\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 2661.4 | learning rate: 1.875E-06 | global batch size:     4 | loss scale: 1.0 | grad norm: nan | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 11.21 and total parameters 0.911 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 2.66138870716095;  SamplesPerSecond: 1.5029747399308013\n",
      "time (ms) | e2e-time: 2661.39 | forward-compute: 34.41 | backward-compute: 2617.81 | backward-embedding-all-reduce: 0.01 | optimizer: 2.03 | batch-generator: 1.10 | offloading-func-call-overhead: 3.70 | offloading-fwd-overhead: 1.55 | offloading-bwd-overhead: 0.65 | offloading-fwd-2gpu-overhead: 0.02 | offloading-fwd-2cpu-overhead: 1.26 | offloading-bwd-2gpu-overhead: 0.05 | offloading-bwd-2cpu-overhead: 0.23\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 2627.6 | learning rate: 2.344E-06 | global batch size:     4 | loss scale: 1.0 | grad norm: nan | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 11.36 and total parameters 0.911 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 2.62761390209198;  SamplesPerSecond: 1.5222936660577842\n",
      "time (ms) | e2e-time: 2627.61 | forward-compute: 34.67 | backward-compute: 2584.30 | backward-embedding-all-reduce: 0.01 | optimizer: 1.95 | batch-generator: 1.11 | offloading-func-call-overhead: 3.86 | offloading-fwd-overhead: 1.32 | offloading-bwd-overhead: 0.68 | offloading-fwd-2gpu-overhead: 0.01 | offloading-fwd-2cpu-overhead: 1.02 | offloading-bwd-2gpu-overhead: 0.05 | offloading-bwd-2cpu-overhead: 0.24\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 04:26:51 \n",
      "[after training is done] datetime: 2022-06-29 04:26:51 \n",
      "/home/sys/STRONGHOLD\n"
     ]
    }
   ],
   "source": [
    "# Code Here\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "./examples/run.sh -m \"stronghold\" -l 92 -h 2048 -w 15 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 64 -h 2048 -w 15 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 56 -h 2048 -w 15 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 40 -h 2048 -w 15 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 24 -h 2048 -w 15 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 16 -h 2048 -w 15 && \\\n",
    "\\\n",
    "pyenv deactivate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:  grep -R 'elapsed time per iteration' ./results/log_stronghold_l-16_hs-2048_bs-4_ws-15_2022-06-29.1656476653.txt ./results/log_stronghold_l-24_hs-2048_bs-4_ws-15_2022-06-29.1656476410.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-15_2022-06-29.1656468888.txt ./results/log_stronghold_l-40_hs-2048_bs-4_ws-15_2022-06-29.1656475948.txt ./results/log_stronghold_l-48_hs-2048_bs-4_ws-15_2022-06-29.1656469235.txt ./results/log_stronghold_l-56_hs-2048_bs-4_ws-15_2022-06-29.1656475269.txt ./results/log_stronghold_l-64_hs-2048_bs-4_ws-15_2022-06-29.1656474476.txt ./results/log_stronghold_l-78_hs-2048_bs-4_ws-15_2022-06-29.1656469802.txt ./results/log_stronghold_l-92_hs-2048_bs-4_ws-15_2022-06-29.1656473293.txt | awk -v FS='[/,_:|]' '{print $5, $6, $8, $4, $12, $13}' | uniq | sort \r\n",
      "\r\n",
      "l-16 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  2627.6 \r\n",
      "l-16 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  2661.4 \r\n",
      "l-16 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  2722.1 \r\n",
      "l-16 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  2726.6 \r\n",
      "l-16 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  2730.6 \r\n",
      "l-24 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  4068.7 \r\n",
      "l-24 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  4123.0 \r\n",
      "l-24 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  4123.4 \r\n",
      "l-24 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  4190.5 \r\n",
      "l-24 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  4489.9 \r\n",
      "l-32 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  5956.9 \r\n",
      "l-32 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  5976.5 \r\n",
      "l-32 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  6017.3 \r\n",
      "l-32 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  6051.8 \r\n",
      "l-32 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  6596.2 \r\n",
      "l-40 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  7918.5 \r\n",
      "l-40 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  8031.9 \r\n",
      "l-40 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  8175.4 \r\n",
      "l-40 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  8307.6 \r\n",
      "l-40 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  9029.8 \r\n",
      "l-48 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  10005.5 \r\n",
      "l-48 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  10053.0 \r\n",
      "l-48 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  11323.9 \r\n",
      "l-48 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  9953.1 \r\n",
      "l-48 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  9984.5 \r\n",
      "l-56 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  11992.3 \r\n",
      "l-56 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  12019.5 \r\n",
      "l-56 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  12081.2 \r\n",
      "l-56 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  12109.2 \r\n",
      "l-56 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  13543.4 \r\n",
      "l-64 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  14053.7 \r\n",
      "l-64 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  14068.9 \r\n",
      "l-64 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  14118.4 \r\n",
      "l-64 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  14303.4 \r\n",
      "l-64 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  15958.4 \r\n",
      "l-78 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  17482.1 \r\n",
      "l-78 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  17553.8 \r\n",
      "l-78 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  17583.2 \r\n",
      "l-78 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  17616.2 \r\n",
      "l-78 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  20163.3 \r\n",
      "l-92 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  21211.0 \r\n",
      "l-92 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  21267.1 \r\n",
      "l-92 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  21290.7 \r\n",
      "l-92 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  21351.2 \r\n",
      "l-92 hs-2048 ws-15 stronghold  elapsed time per iteration (ms)  24006.4 \r\n"
     ]
    }
   ],
   "source": [
    "# To print the relevant information from log files\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "./examples/case4.sh && \\\n",
    "\\\n",
    "pyenv deactivate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 CASE - Impact of working window size (Figure 9 in Section VI.C)\n",
    "\n",
    "Working window size affects the throughput. The larger window can better overlap GPU computation with data transfer, leading to higher training throughput. But, a larger window size means more GPU memory occupancy.\n",
    "\n",
    "This case evaluates the impact of working window size for STRONGHOLD with 1.7B model. You will see that at the first stage, the larger window size can gain more benefits, while at the end of the stage, enlarging window size shows no influence because the current window size can hide the data transformation process.\n",
    "\n",
    "PS: The bandwidth restriction in the virtual machine might slightly hurt the performance of STRONGHOLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.0  0.0   2520    72 pts/0    Ss+  Jun28   0:00 sleep infinit\r\n",
      "root           7  0.0  0.0   4348   696 pts/1    Ss+  Jun28   0:00 /bin/bash\r\n",
      "root         159  0.0  0.0   4348   804 pts/2    Ss+  Jun28   0:00 /bin/bash\r\n",
      "root       35813  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35825  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35855  0.0  0.0      0     0 ?        Z    01:01   0:01 [python] <def\r\n",
      "root       35856  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35864  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       35865  0.0  0.0      0     0 ?        Z    01:01   0:00 [python] <def\r\n",
      "root       42636  0.0  0.0   3976  3192 pts/3    Ss+  04:26   0:00 /bin/bash -c \r\n",
      "root       42794  0.0  0.0   5892  2824 pts/3    R+   04:26   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "# To check if there exists other running processes launched by other reviwers in case of GPU overlead.\n",
    "# Just run it and no need to change anything in this cell.\n",
    "#\n",
    "# `ps aux` in docker container. \n",
    "######\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c 'export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && export PYENV_ROOT=\"/root/.pyenv\" && export PATH=\"$PYENV_ROOT/bin:$PATH\" && eval \"$(pyenv init -)\" && eval \"$(pyenv virtualenv-init -)\" && pyenv activate py3.9.10 && ps aux && pyenv deactivate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following results correspond to Figure 9 in the submitted paper. Please refers to Section VI.C on page 10 for more details. Run around 48 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 2 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-2_2022-06-29.1656476815.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 2 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 2\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 2\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.157 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.604 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.706 seconds\n",
      "time to initialize megatron (seconds): 3.812\n",
      "[after megatron is initialized] datetime: 2022-06-29 04:27:01 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.21\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 04:27:29 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000543 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 04:27:29 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 27526.71 | train/valid/test-data-iterators-setup: 440.25\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 04:27:29 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 9001.1 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.078359E+01 | loss scale: 1.0 | grad norm: 39.110 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.25 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 9.001119470596313;  SamplesPerSecond: 0.4443891688213538\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 4103.3857421875 | max allocated: 7472.5986328125 | reserved: 8602.0 | max reserved: 8602.0\n",
      "time (ms) | e2e-time: 9001.18 | forward-compute: 1255.60 | backward-compute: 7734.62 | backward-embedding-all-reduce: 0.02 | optimizer: 2.52 | batch-generator: 1.77 | offloading-func-call-overhead: 2644.00 | offloading-fwd-overhead: 826.84 | offloading-bwd-overhead: 4304.94 | offloading-fwd-2gpu-overhead: 264.15 | offloading-fwd-2cpu-overhead: 562.06 | offloading-bwd-2gpu-overhead: 295.73 | offloading-bwd-2cpu-overhead: 4008.31\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 7898.8 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.090166E+01 | loss scale: 1.0 | grad norm: 30.404 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.12 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.898828792572021;  SamplesPerSecond: 0.5064041904239727\n",
      "time (ms) | e2e-time: 7898.75 | forward-compute: 1123.59 | backward-compute: 6764.50 | backward-embedding-all-reduce: 0.02 | optimizer: 2.49 | batch-generator: 1.11 | offloading-func-call-overhead: 16.22 | offloading-fwd-overhead: 1037.94 | offloading-bwd-overhead: 5294.42 | offloading-fwd-2gpu-overhead: 353.85 | offloading-fwd-2cpu-overhead: 683.48 | offloading-bwd-2gpu-overhead: 484.69 | offloading-bwd-2cpu-overhead: 4808.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 7859.4 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.046708E+01 | loss scale: 1.0 | grad norm: 3.165 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.16 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.859429907798767;  SamplesPerSecond: 0.5089427664506396\n",
      "time (ms) | e2e-time: 7859.51 | forward-compute: 1146.16 | backward-compute: 6702.60 | backward-embedding-all-reduce: 0.02 | optimizer: 2.48 | batch-generator: 1.15 | offloading-func-call-overhead: 16.28 | offloading-fwd-overhead: 1059.16 | offloading-bwd-overhead: 5453.51 | offloading-fwd-2gpu-overhead: 313.41 | offloading-fwd-2cpu-overhead: 745.14 | offloading-bwd-2gpu-overhead: 436.98 | offloading-bwd-2cpu-overhead: 5015.58\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 7988.5 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.046046E+01 | loss scale: 1.0 | grad norm: 26.184 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.04 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.988521146774292;  SamplesPerSecond: 0.500718459212588\n",
      "time (ms) | e2e-time: 7988.43 | forward-compute: 1141.37 | backward-compute: 6836.42 | backward-embedding-all-reduce: 0.01 | optimizer: 2.49 | batch-generator: 1.13 | offloading-func-call-overhead: 16.16 | offloading-fwd-overhead: 1055.44 | offloading-bwd-overhead: 5279.76 | offloading-fwd-2gpu-overhead: 358.48 | offloading-fwd-2cpu-overhead: 696.35 | offloading-bwd-2gpu-overhead: 373.59 | offloading-bwd-2cpu-overhead: 4905.23\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 7935.9 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.071590E+01 | loss scale: 1.0 | grad norm: 15.630 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.09 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.935850477218628;  SamplesPerSecond: 0.5040417547536666\n",
      "time (ms) | e2e-time: 7935.85 | forward-compute: 1112.51 | backward-compute: 6812.68 | backward-embedding-all-reduce: 0.02 | optimizer: 2.50 | batch-generator: 1.15 | offloading-func-call-overhead: 16.18 | offloading-fwd-overhead: 1026.85 | offloading-bwd-overhead: 5394.62 | offloading-fwd-2gpu-overhead: 304.28 | offloading-fwd-2cpu-overhead: 721.97 | offloading-bwd-2gpu-overhead: 393.04 | offloading-bwd-2cpu-overhead: 5000.64\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 04:34:16 \n",
      "[after training is done] datetime: 2022-06-29 04:34:16 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656477260.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 4 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 4\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 4\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.625 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.697 seconds\n",
      "time to initialize megatron (seconds): 3.816\n",
      "[after megatron is initialized] datetime: 2022-06-29 04:34:28 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.18\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 04:34:55 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000549 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 04:34:56 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 27604.74 | train/valid/test-data-iterators-setup: 427.40\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 04:34:56 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 8716.8 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.041626E+01 | loss scale: 1.0 | grad norm: 13.782 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.45 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.7168048620224;  SamplesPerSecond: 0.4588837381719193\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 6408.6064453125 | max allocated: 9969.9208984375 | reserved: 12026.0 | max reserved: 12026.0\n",
      "time (ms) | e2e-time: 8716.90 | forward-compute: 1181.47 | backward-compute: 7524.43 | backward-embedding-all-reduce: 0.02 | optimizer: 2.52 | batch-generator: 1.79 | offloading-func-call-overhead: 2482.69 | offloading-fwd-overhead: 750.44 | offloading-bwd-overhead: 4278.50 | offloading-fwd-2gpu-overhead: 182.46 | offloading-fwd-2cpu-overhead: 567.35 | offloading-bwd-2gpu-overhead: 201.28 | offloading-bwd-2cpu-overhead: 4076.38\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 7742.3 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 9.987002E+00 | loss scale: 1.0 | grad norm: 2.456 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.27 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.742274785041809;  SamplesPerSecond: 0.5166440240183751\n",
      "time (ms) | e2e-time: 7742.19 | forward-compute: 1045.40 | backward-compute: 6686.09 | backward-embedding-all-reduce: 0.02 | optimizer: 2.47 | batch-generator: 1.18 | offloading-func-call-overhead: 15.88 | offloading-fwd-overhead: 935.34 | offloading-bwd-overhead: 5271.73 | offloading-fwd-2gpu-overhead: 241.09 | offloading-fwd-2cpu-overhead: 693.63 | offloading-bwd-2gpu-overhead: 169.51 | offloading-bwd-2cpu-overhead: 5101.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 7751.9 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.526058E+00 | loss scale: 1.0 | grad norm: 2.373 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.26 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.7519118070602415;  SamplesPerSecond: 0.5160017424807263\n",
      "time (ms) | e2e-time: 7751.91 | forward-compute: 1044.22 | backward-compute: 6697.02 | backward-embedding-all-reduce: 0.02 | optimizer: 2.46 | batch-generator: 1.14 | offloading-func-call-overhead: 15.88 | offloading-fwd-overhead: 935.83 | offloading-bwd-overhead: 5124.38 | offloading-fwd-2gpu-overhead: 171.21 | offloading-fwd-2cpu-overhead: 764.01 | offloading-bwd-2gpu-overhead: 146.77 | offloading-bwd-2cpu-overhead: 4976.73\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 7587.9 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.003788E+01 | loss scale: 1.0 | grad norm: 58.823 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.41 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.587878775596619;  SamplesPerSecond: 0.5271565503740522\n",
      "time (ms) | e2e-time: 7587.97 | forward-compute: 1047.60 | backward-compute: 6529.59 | backward-embedding-all-reduce: 0.02 | optimizer: 2.47 | batch-generator: 1.15 | offloading-func-call-overhead: 15.87 | offloading-fwd-overhead: 935.48 | offloading-bwd-overhead: 5433.27 | offloading-fwd-2gpu-overhead: 246.46 | offloading-fwd-2cpu-overhead: 688.40 | offloading-bwd-2gpu-overhead: 282.56 | offloading-bwd-2cpu-overhead: 5149.80\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 7867.8 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.830299E+00 | loss scale: 1.0 | grad norm: 31.696 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.15 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.867774796485901;  SamplesPerSecond: 0.5084029606168415\n",
      "time (ms) | e2e-time: 7867.69 | forward-compute: 1039.50 | backward-compute: 6817.50 | backward-embedding-all-reduce: 0.02 | optimizer: 2.48 | batch-generator: 1.22 | offloading-func-call-overhead: 15.71 | offloading-fwd-overhead: 930.83 | offloading-bwd-overhead: 5386.07 | offloading-fwd-2gpu-overhead: 199.23 | offloading-fwd-2cpu-overhead: 730.99 | offloading-bwd-2gpu-overhead: 238.55 | offloading-bwd-2cpu-overhead: 5146.62\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 04:41:32 \n",
      "[after training is done] datetime: 2022-06-29 04:41:32 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 6 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-6_2022-06-29.1656477696.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 6 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 6\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 6\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.157 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.588 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.705 seconds\n",
      "time to initialize megatron (seconds): 3.793\n",
      "[after megatron is initialized] datetime: 2022-06-29 04:41:43 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.21\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 04:42:11 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000586 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 04:42:11 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 27479.73 | train/valid/test-data-iterators-setup: 447.61\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 04:42:11 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 8297.6 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.075317E+01 | loss scale: 1.0 | grad norm: 5.945 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.78 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.297600984573364;  SamplesPerSecond: 0.48206704653991833\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 8713.8271484375 | max allocated: 12611.3681640625 | reserved: 16548.0 | max reserved: 16548.0\n",
      "time (ms) | e2e-time: 8297.73 | forward-compute: 1071.78 | backward-compute: 7215.01 | backward-embedding-all-reduce: 0.02 | optimizer: 2.51 | batch-generator: 1.69 | offloading-func-call-overhead: 2301.82 | offloading-fwd-overhead: 673.27 | offloading-bwd-overhead: 4036.51 | offloading-fwd-2gpu-overhead: 132.47 | offloading-fwd-2cpu-overhead: 540.03 | offloading-bwd-2gpu-overhead: 107.73 | offloading-bwd-2cpu-overhead: 3927.88\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 7289.1 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.036319E+01 | loss scale: 1.0 | grad norm: 213.521 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.72 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.289102578163147;  SamplesPerSecond: 0.5487643996098076\n",
      "time (ms) | e2e-time: 7288.94 | forward-compute: 962.04 | backward-compute: 6316.30 | backward-embedding-all-reduce: 0.02 | optimizer: 2.47 | batch-generator: 1.17 | offloading-func-call-overhead: 17.89 | offloading-fwd-overhead: 853.22 | offloading-bwd-overhead: 5101.53 | offloading-fwd-2gpu-overhead: 224.24 | offloading-fwd-2cpu-overhead: 628.32 | offloading-bwd-2gpu-overhead: 204.42 | offloading-bwd-2cpu-overhead: 4896.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 7237.4 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.025148E+01 | loss scale: 1.0 | grad norm: 6.158 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.77 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.237360525131225;  SamplesPerSecond: 0.5526876802820975\n",
      "time (ms) | e2e-time: 7237.37 | forward-compute: 953.77 | backward-compute: 6272.98 | backward-embedding-all-reduce: 0.02 | optimizer: 2.48 | batch-generator: 1.15 | offloading-func-call-overhead: 16.57 | offloading-fwd-overhead: 849.37 | offloading-bwd-overhead: 4840.10 | offloading-fwd-2gpu-overhead: 163.49 | offloading-fwd-2cpu-overhead: 685.11 | offloading-bwd-2gpu-overhead: 253.02 | offloading-bwd-2cpu-overhead: 4586.05\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 7271.3 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.813936E+00 | loss scale: 1.0 | grad norm: 404.164 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.74 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.27133150100708;  SamplesPerSecond: 0.5501055754982426\n",
      "time (ms) | e2e-time: 7271.33 | forward-compute: 949.16 | backward-compute: 6311.53 | backward-embedding-all-reduce: 0.02 | optimizer: 2.46 | batch-generator: 1.15 | offloading-func-call-overhead: 16.27 | offloading-fwd-overhead: 844.92 | offloading-bwd-overhead: 4713.26 | offloading-fwd-2gpu-overhead: 205.65 | offloading-fwd-2cpu-overhead: 638.63 | offloading-bwd-2gpu-overhead: 90.84 | offloading-bwd-2cpu-overhead: 4621.50\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 7423.9 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.524220E+00 | loss scale: 1.0 | grad norm: 3.963 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.58 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.423872232437134;  SamplesPerSecond: 0.538802376275119\n",
      "time (ms) | e2e-time: 7423.87 | forward-compute: 959.77 | backward-compute: 6453.48 | backward-embedding-all-reduce: 0.02 | optimizer: 2.47 | batch-generator: 1.16 | offloading-func-call-overhead: 16.28 | offloading-fwd-overhead: 855.65 | offloading-bwd-overhead: 5156.00 | offloading-fwd-2gpu-overhead: 148.46 | offloading-fwd-2cpu-overhead: 706.48 | offloading-bwd-2gpu-overhead: 176.59 | offloading-bwd-2cpu-overhead: 4978.50\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 04:48:27 \n",
      "[after training is done] datetime: 2022-06-29 04:48:27 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 8 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-8_2022-06-29.1656478111.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 8 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 8\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 8\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.599 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.704 seconds\n",
      "time to initialize megatron (seconds): 3.807\n",
      "[after megatron is initialized] datetime: 2022-06-29 04:48:38 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.21\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 04:49:06 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000598 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 04:49:06 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 28123.70 | train/valid/test-data-iterators-setup: 422.19\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 04:49:06 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 8102.4 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.065660E+01 | loss scale: 1.0 | grad norm: 52.027 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.94 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.102357459068298;  SamplesPerSecond: 0.4936834767174005\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 11019.0478515625 | max allocated: 15092.6982421875 | reserved: 18752.0 | max reserved: 18752.0\n",
      "time (ms) | e2e-time: 8102.38 | forward-compute: 988.56 | backward-compute: 7102.95 | backward-embedding-all-reduce: 0.02 | optimizer: 2.52 | batch-generator: 1.80 | offloading-func-call-overhead: 2140.29 | offloading-fwd-overhead: 599.89 | offloading-bwd-overhead: 2448.71 | offloading-fwd-2gpu-overhead: 132.94 | offloading-fwd-2cpu-overhead: 466.34 | offloading-bwd-2gpu-overhead: 28.76 | offloading-bwd-2cpu-overhead: 2419.10\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 7283.5 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.003618E+01 | loss scale: 1.0 | grad norm: 5.261 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.72 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.2834892749786375;  SamplesPerSecond: 0.5491873261544319\n",
      "time (ms) | e2e-time: 7283.42 | forward-compute: 869.05 | backward-compute: 6403.80 | backward-embedding-all-reduce: 0.02 | optimizer: 2.46 | batch-generator: 1.21 | offloading-func-call-overhead: 16.58 | offloading-fwd-overhead: 752.02 | offloading-bwd-overhead: 2906.08 | offloading-fwd-2gpu-overhead: 218.43 | offloading-fwd-2cpu-overhead: 532.97 | offloading-bwd-2gpu-overhead: 9.56 | offloading-bwd-2cpu-overhead: 2895.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 7098.5 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.751514E+00 | loss scale: 1.0 | grad norm: 11.207 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.92 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.09846043586731;  SamplesPerSecond: 0.5635024715766087\n",
      "time (ms) | e2e-time: 7098.47 | forward-compute: 906.85 | backward-compute: 6181.02 | backward-embedding-all-reduce: 0.02 | optimizer: 2.45 | batch-generator: 1.13 | offloading-func-call-overhead: 17.24 | offloading-fwd-overhead: 798.60 | offloading-bwd-overhead: 4122.61 | offloading-fwd-2gpu-overhead: 154.88 | offloading-fwd-2cpu-overhead: 643.09 | offloading-bwd-2gpu-overhead: 28.08 | offloading-bwd-2cpu-overhead: 4093.53\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 7053.0 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.805072E+00 | loss scale: 1.0 | grad norm: 9.412 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.98 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.053012013435364;  SamplesPerSecond: 0.5671335866691215\n",
      "time (ms) | e2e-time: 7053.00 | forward-compute: 885.65 | backward-compute: 6156.76 | backward-embedding-all-reduce: 0.02 | optimizer: 2.44 | batch-generator: 1.15 | offloading-func-call-overhead: 16.57 | offloading-fwd-overhead: 783.07 | offloading-bwd-overhead: 3074.53 | offloading-fwd-2gpu-overhead: 221.82 | offloading-fwd-2cpu-overhead: 560.64 | offloading-bwd-2gpu-overhead: 30.02 | offloading-bwd-2cpu-overhead: 3043.62\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 7099.6 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.900784E+00 | loss scale: 1.0 | grad norm: 16.499 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.92 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.09959192276001;  SamplesPerSecond: 0.5634126642091529\n",
      "time (ms) | e2e-time: 7099.76 | forward-compute: 867.92 | backward-compute: 6221.06 | backward-embedding-all-reduce: 0.02 | optimizer: 2.45 | batch-generator: 1.20 | offloading-func-call-overhead: 16.26 | offloading-fwd-overhead: 763.78 | offloading-bwd-overhead: 2846.82 | offloading-fwd-2gpu-overhead: 156.65 | offloading-fwd-2cpu-overhead: 606.51 | offloading-bwd-2gpu-overhead: 19.28 | offloading-bwd-2cpu-overhead: 2826.57\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 04:55:13 \n",
      "[after training is done] datetime: 2022-06-29 04:55:13 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 10 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-10_2022-06-29.1656478517.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 10 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 10\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 10\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.157 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.586 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.700 seconds\n",
      "time to initialize megatron (seconds): 3.781\n",
      "[after megatron is initialized] datetime: 2022-06-29 04:55:24 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.22\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 04:55:52 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000643 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 04:55:53 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 28123.17 | train/valid/test-data-iterators-setup: 444.25\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 04:55:53 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 7697.7 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.090260E+01 | loss scale: 1.0 | grad norm: 3.460 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.31 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.697708034515381;  SamplesPerSecond: 0.5196351929775191\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 13324.2685546875 | max allocated: 17446.0830078125 | reserved: 21834.0 | max reserved: 21834.0\n",
      "time (ms) | e2e-time: 7697.78 | forward-compute: 950.65 | backward-compute: 6736.28 | backward-embedding-all-reduce: 0.02 | optimizer: 2.50 | batch-generator: 1.82 | offloading-func-call-overhead: 2088.72 | offloading-fwd-overhead: 606.56 | offloading-bwd-overhead: 1713.48 | offloading-fwd-2gpu-overhead: 139.26 | offloading-fwd-2cpu-overhead: 466.54 | offloading-bwd-2gpu-overhead: 3.99 | offloading-bwd-2cpu-overhead: 1708.61\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 6533.4 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.064505E+01 | loss scale: 1.0 | grad norm: 3.591 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.61 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.533440351486206;  SamplesPerSecond: 0.6122348693502793\n",
      "time (ms) | e2e-time: 6533.37 | forward-compute: 869.04 | backward-compute: 5653.76 | backward-embedding-all-reduce: 0.02 | optimizer: 2.44 | batch-generator: 1.23 | offloading-func-call-overhead: 16.07 | offloading-fwd-overhead: 765.22 | offloading-bwd-overhead: 729.78 | offloading-fwd-2gpu-overhead: 254.24 | offloading-fwd-2cpu-overhead: 510.20 | offloading-bwd-2gpu-overhead: 10.92 | offloading-bwd-2cpu-overhead: 717.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 6889.1 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.021579E+01 | loss scale: 1.0 | grad norm: 36.416 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.17 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.889051866531372;  SamplesPerSecond: 0.5806314246860206\n",
      "time (ms) | e2e-time: 6889.04 | forward-compute: 828.02 | backward-compute: 6050.47 | backward-embedding-all-reduce: 0.02 | optimizer: 2.45 | batch-generator: 1.21 | offloading-func-call-overhead: 18.32 | offloading-fwd-overhead: 715.47 | offloading-bwd-overhead: 1466.04 | offloading-fwd-2gpu-overhead: 133.64 | offloading-fwd-2cpu-overhead: 581.08 | offloading-bwd-2gpu-overhead: 1.11 | offloading-bwd-2cpu-overhead: 1463.91\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 6822.4 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.964854E+00 | loss scale: 1.0 | grad norm: 4.869 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.24 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.822418880462647;  SamplesPerSecond: 0.5863023174163046\n",
      "time (ms) | e2e-time: 6822.42 | forward-compute: 849.57 | backward-compute: 5962.30 | backward-embedding-all-reduce: 0.02 | optimizer: 2.44 | batch-generator: 1.18 | offloading-func-call-overhead: 16.92 | offloading-fwd-overhead: 741.79 | offloading-bwd-overhead: 2074.78 | offloading-fwd-2gpu-overhead: 205.82 | offloading-fwd-2cpu-overhead: 535.31 | offloading-bwd-2gpu-overhead: 30.45 | offloading-bwd-2cpu-overhead: 2043.42\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 6787.8 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.612624E+00 | loss scale: 1.0 | grad norm: 5.554 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.29 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.787764167785644;  SamplesPerSecond: 0.5892956651298789\n",
      "time (ms) | e2e-time: 6787.82 | forward-compute: 832.43 | backward-compute: 5944.77 | backward-embedding-all-reduce: 0.02 | optimizer: 2.44 | batch-generator: 1.21 | offloading-func-call-overhead: 16.16 | offloading-fwd-overhead: 729.92 | offloading-bwd-overhead: 1879.45 | offloading-fwd-2gpu-overhead: 165.68 | offloading-fwd-2cpu-overhead: 563.59 | offloading-bwd-2gpu-overhead: 1.10 | offloading-bwd-2cpu-overhead: 1877.45\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 05:01:40 \n",
      "[after training is done] datetime: 2022-06-29 05:01:40 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 12 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-12_2022-06-29.1656478904.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 12 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 12\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 12\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.161 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.568 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.709 seconds\n",
      "time to initialize megatron (seconds): 3.783\n",
      "[after megatron is initialized] datetime: 2022-06-29 05:01:51 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.20\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 05:02:19 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000596 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 05:02:20 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 28395.82 | train/valid/test-data-iterators-setup: 466.20\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 05:02:20 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 7586.7 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.074984E+01 | loss scale: 1.0 | grad norm: 7.697 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.41 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.586665773391724;  SamplesPerSecond: 0.5272408353652496\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 15629.4892578125 | max allocated: 20184.4990234375 | reserved: 26160.0 | max reserved: 26160.0\n",
      "time (ms) | e2e-time: 7586.71 | forward-compute: 940.50 | backward-compute: 6635.30 | backward-embedding-all-reduce: 0.02 | optimizer: 2.47 | batch-generator: 1.77 | offloading-func-call-overhead: 1925.47 | offloading-fwd-overhead: 601.36 | offloading-bwd-overhead: 961.77 | offloading-fwd-2gpu-overhead: 113.73 | offloading-fwd-2cpu-overhead: 486.93 | offloading-bwd-2gpu-overhead: 0.90 | offloading-bwd-2cpu-overhead: 959.92\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 6702.5 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.063771E+01 | loss scale: 1.0 | grad norm: 3107831.059 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.39 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.7024671077728275;  SamplesPerSecond: 0.5967951704471945\n",
      "time (ms) | e2e-time: 6702.38 | forward-compute: 816.79 | backward-compute: 5874.92 | backward-embedding-all-reduce: 0.02 | optimizer: 2.46 | batch-generator: 1.30 | offloading-func-call-overhead: 15.85 | offloading-fwd-overhead: 717.47 | offloading-bwd-overhead: 1265.83 | offloading-fwd-2gpu-overhead: 209.15 | offloading-fwd-2cpu-overhead: 507.57 | offloading-bwd-2gpu-overhead: 0.97 | offloading-bwd-2cpu-overhead: 1264.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 6584.1 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.049995E+01 | loss scale: 1.0 | grad norm: 22.707 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.54 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.584106826782227;  SamplesPerSecond: 0.6075235571405322\n",
      "time (ms) | e2e-time: 6584.12 | forward-compute: 825.66 | backward-compute: 5747.81 | backward-embedding-all-reduce: 0.02 | optimizer: 2.46 | batch-generator: 1.23 | offloading-func-call-overhead: 16.53 | offloading-fwd-overhead: 710.61 | offloading-bwd-overhead: 1067.19 | offloading-fwd-2gpu-overhead: 185.33 | offloading-fwd-2cpu-overhead: 524.64 | offloading-bwd-2gpu-overhead: 0.94 | offloading-bwd-2cpu-overhead: 1065.31\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 6765.6 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.011405E+01 | loss scale: 1.0 | grad norm: 4672.528 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.31 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.765619421005249;  SamplesPerSecond: 0.5912245060047543\n",
      "time (ms) | e2e-time: 6765.72 | forward-compute: 879.16 | backward-compute: 5875.81 | backward-embedding-all-reduce: 0.02 | optimizer: 2.47 | batch-generator: 1.19 | offloading-func-call-overhead: 15.75 | offloading-fwd-overhead: 773.25 | offloading-bwd-overhead: 1025.97 | offloading-fwd-2gpu-overhead: 191.28 | offloading-fwd-2cpu-overhead: 581.32 | offloading-bwd-2gpu-overhead: 9.86 | offloading-bwd-2cpu-overhead: 1014.87\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 6595.9 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.037519E+01 | loss scale: 1.0 | grad norm: 22.472 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.53 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.5959378480911255;  SamplesPerSecond: 0.6064338524896208\n",
      "time (ms) | e2e-time: 6595.85 | forward-compute: 801.96 | backward-compute: 5783.20 | backward-embedding-all-reduce: 0.02 | optimizer: 2.43 | batch-generator: 1.24 | offloading-func-call-overhead: 16.73 | offloading-fwd-overhead: 698.13 | offloading-bwd-overhead: 718.25 | offloading-fwd-2gpu-overhead: 115.40 | offloading-fwd-2cpu-overhead: 582.07 | offloading-bwd-2gpu-overhead: 0.98 | offloading-bwd-2cpu-overhead: 716.28\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 05:08:02 \n",
      "[after training is done] datetime: 2022-06-29 05:08:02 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 14 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-14_2022-06-29.1656479286.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 14 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 14\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 14\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.160 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.616 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.712 seconds\n",
      "time to initialize megatron (seconds): 3.829\n",
      "[after megatron is initialized] datetime: 2022-06-29 05:08:13 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.20\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-06-29 05:08:43 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000588 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-06-29 05:08:43 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 29242.30 | train/valid/test-data-iterators-setup: 444.11\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-06-29 05:08:43 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 7167.9 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.103886E+01 | loss scale: 1.0 | grad norm: 6938.920 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.85 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.167894053459167;  SamplesPerSecond: 0.5580439624480265\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 17934.7099609375 | max allocated: 22457.8056640625 | reserved: 27550.0 | max reserved: 27550.0\n",
      "time (ms) | e2e-time: 7167.92 | forward-compute: 1026.05 | backward-compute: 6130.90 | backward-embedding-all-reduce: 0.02 | optimizer: 2.51 | batch-generator: 1.82 | offloading-func-call-overhead: 1812.32 | offloading-fwd-overhead: 709.95 | offloading-bwd-overhead: 395.61 | offloading-fwd-2gpu-overhead: 176.76 | offloading-fwd-2cpu-overhead: 532.57 | offloading-bwd-2gpu-overhead: 0.86 | offloading-bwd-2cpu-overhead: 393.78\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 6227.0 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.061177E+01 | loss scale: 1.0 | grad norm: 1937.207 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.03 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.226996421813965;  SamplesPerSecond: 0.6423642682670394\n",
      "time (ms) | e2e-time: 6226.91 | forward-compute: 784.93 | backward-compute: 5431.31 | backward-embedding-all-reduce: 0.02 | optimizer: 2.44 | batch-generator: 1.25 | offloading-func-call-overhead: 15.09 | offloading-fwd-overhead: 678.37 | offloading-bwd-overhead: 464.42 | offloading-fwd-2gpu-overhead: 178.38 | offloading-fwd-2cpu-overhead: 499.32 | offloading-bwd-2gpu-overhead: 0.89 | offloading-bwd-2cpu-overhead: 462.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 6187.5 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.023255E+01 | loss scale: 1.0 | grad norm: 33.184 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.09 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.1875091075897215;  SamplesPerSecond: 0.6464636949129531\n",
      "time (ms) | e2e-time: 6187.61 | forward-compute: 784.88 | backward-compute: 5391.95 | backward-embedding-all-reduce: 0.02 | optimizer: 2.42 | batch-generator: 1.32 | offloading-func-call-overhead: 14.30 | offloading-fwd-overhead: 692.10 | offloading-bwd-overhead: 319.71 | offloading-fwd-2gpu-overhead: 155.65 | offloading-fwd-2cpu-overhead: 535.76 | offloading-bwd-2gpu-overhead: 0.95 | offloading-bwd-2cpu-overhead: 317.85\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 6515.8 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.982765E+00 | loss scale: 1.0 | grad norm: 13.519 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.63 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.515803670883178;  SamplesPerSecond: 0.613892038809362\n",
      "time (ms) | e2e-time: 6515.71 | forward-compute: 941.31 | backward-compute: 5563.68 | backward-embedding-all-reduce: 0.02 | optimizer: 2.45 | batch-generator: 1.28 | offloading-func-call-overhead: 16.09 | offloading-fwd-overhead: 831.21 | offloading-bwd-overhead: 356.66 | offloading-fwd-2gpu-overhead: 244.51 | offloading-fwd-2cpu-overhead: 586.10 | offloading-bwd-2gpu-overhead: 0.89 | offloading-bwd-2cpu-overhead: 354.88\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 6328.4 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.676443E+00 | loss scale: 1.0 | grad norm: 6.211 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.89 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.328391718864441;  SamplesPerSecond: 0.6320721247511137\n",
      "time (ms) | e2e-time: 6328.41 | forward-compute: 812.15 | backward-compute: 5505.53 | backward-embedding-all-reduce: 0.02 | optimizer: 2.46 | batch-generator: 1.44 | offloading-func-call-overhead: 17.42 | offloading-fwd-overhead: 711.11 | offloading-bwd-overhead: 690.83 | offloading-fwd-2gpu-overhead: 135.73 | offloading-fwd-2cpu-overhead: 574.77 | offloading-bwd-2gpu-overhead: 0.88 | offloading-bwd-2cpu-overhead: 688.72\n",
      "[exiting program at iteration 50] datetime: 2022-06-29 05:14:07 \n",
      "[after training is done] datetime: 2022-06-29 05:14:07 \n",
      "/home/sys/STRONGHOLD\n"
     ]
    }
   ],
   "source": [
    "# Code Here\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 2 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 4 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 6 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 8 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 10 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 12 && \\\n",
    "./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 14 && \\\n",
    "\\\n",
    "pyenv deactivate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:  grep -R 'elapsed time per iteration' ./results/log_stronghold_l-32_hs-2048_bs-4_ws-10_2022-06-29.1656478517.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-12_2022-06-29.1656478904.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-14_2022-06-29.1656479286.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-15_2022-06-29.1656468888.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-2_2022-06-29.1656476815.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-4_2022-06-29.1656477260.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-6_2022-06-29.1656477696.txt ./results/log_stronghold_l-32_hs-2048_bs-4_ws-8_2022-06-29.1656478111.txt | awk -v FS='[/,_:|]' '{print $8, $4, $12, $13}' | uniq | sort \r\n",
      "\r\n",
      "ws-10 stronghold  elapsed time per iteration (ms)  6533.4 \r\n",
      "ws-10 stronghold  elapsed time per iteration (ms)  6787.8 \r\n",
      "ws-10 stronghold  elapsed time per iteration (ms)  6822.4 \r\n",
      "ws-10 stronghold  elapsed time per iteration (ms)  6889.1 \r\n",
      "ws-10 stronghold  elapsed time per iteration (ms)  7697.7 \r\n",
      "ws-12 stronghold  elapsed time per iteration (ms)  6584.1 \r\n",
      "ws-12 stronghold  elapsed time per iteration (ms)  6595.9 \r\n",
      "ws-12 stronghold  elapsed time per iteration (ms)  6702.5 \r\n",
      "ws-12 stronghold  elapsed time per iteration (ms)  6765.6 \r\n",
      "ws-12 stronghold  elapsed time per iteration (ms)  7586.7 \r\n",
      "ws-14 stronghold  elapsed time per iteration (ms)  6187.5 \r\n",
      "ws-14 stronghold  elapsed time per iteration (ms)  6227.0 \r\n",
      "ws-14 stronghold  elapsed time per iteration (ms)  6328.4 \r\n",
      "ws-14 stronghold  elapsed time per iteration (ms)  6515.8 \r\n",
      "ws-14 stronghold  elapsed time per iteration (ms)  7167.9 \r\n",
      "ws-15 stronghold  elapsed time per iteration (ms)  5956.9 \r\n",
      "ws-15 stronghold  elapsed time per iteration (ms)  5976.5 \r\n",
      "ws-15 stronghold  elapsed time per iteration (ms)  6017.3 \r\n",
      "ws-15 stronghold  elapsed time per iteration (ms)  6051.8 \r\n",
      "ws-15 stronghold  elapsed time per iteration (ms)  6596.2 \r\n",
      "ws-2 stronghold  elapsed time per iteration (ms)  7859.4 \r\n",
      "ws-2 stronghold  elapsed time per iteration (ms)  7898.8 \r\n",
      "ws-2 stronghold  elapsed time per iteration (ms)  7935.9 \r\n",
      "ws-2 stronghold  elapsed time per iteration (ms)  7988.5 \r\n",
      "ws-2 stronghold  elapsed time per iteration (ms)  9001.1 \r\n",
      "ws-4 stronghold  elapsed time per iteration (ms)  7587.9 \r\n",
      "ws-4 stronghold  elapsed time per iteration (ms)  7742.3 \r\n",
      "ws-4 stronghold  elapsed time per iteration (ms)  7751.9 \r\n",
      "ws-4 stronghold  elapsed time per iteration (ms)  7867.8 \r\n",
      "ws-4 stronghold  elapsed time per iteration (ms)  8716.8 \r\n",
      "ws-6 stronghold  elapsed time per iteration (ms)  7237.4 \r\n",
      "ws-6 stronghold  elapsed time per iteration (ms)  7271.3 \r\n",
      "ws-6 stronghold  elapsed time per iteration (ms)  7289.1 \r\n",
      "ws-6 stronghold  elapsed time per iteration (ms)  7423.9 \r\n",
      "ws-6 stronghold  elapsed time per iteration (ms)  8297.6 \r\n",
      "ws-8 stronghold  elapsed time per iteration (ms)  7053.0 \r\n",
      "ws-8 stronghold  elapsed time per iteration (ms)  7098.5 \r\n",
      "ws-8 stronghold  elapsed time per iteration (ms)  7099.6 \r\n",
      "ws-8 stronghold  elapsed time per iteration (ms)  7283.5 \r\n",
      "ws-8 stronghold  elapsed time per iteration (ms)  8102.4 \r\n"
     ]
    }
   ],
   "source": [
    "# To print the relevant information from log files\n",
    "!docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "./examples/case5.sh && \\\n",
    "\\\n",
    "pyenv deactivate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# The end of this Artifact Evaluation\n",
    "-----\n",
    "\n",
    "#### Many thanks for your review, time and efforts on this artifact evaluation.  <br> Many thanks for your understanding and bearing with some inconveniences on this notebook. \n",
    "\n",
    "The repository will be open as soon as possible. Users can reproduce other experiment figures using the released docker image or source code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
